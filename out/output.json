{
    "2601.14724": {
        "authors": [
            "Haowei Zhang",
            "Shudong Yang",
            "Jinlan Fu",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
        "abstract": "arXiv:2601.14724v1 Announce Type: new  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
        "arxiv_id": "2601.14724",
        "ARXIVID": "2601.14724",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on streaming video understanding and efficient real-time processing, which is a novel methodology in video-based tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.15284": {
        "authors": [
            "Anurag Bagchi",
            "Zhipeng Bao",
            "Homanga Bharadhwaj",
            "Yu-Xiong Wang",
            "Pavel Tokmakov",
            "Martial Hebert"
        ],
        "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
        "abstract": "arXiv:2601.15284v1 Announce Type: new  Abstract: What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.",
        "arxiv_id": "2601.15284",
        "ARXIVID": "2601.15284",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for action-conditioned world modeling in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.15016": {
        "authors": [
            "Xiaodong Wang",
            "Langling Huang",
            "Zhirong Wu",
            "Xu Zhao",
            "Teng Xu",
            "Xuhong Xia",
            "Peixi Peng"
        ],
        "title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding",
        "abstract": "arXiv:2601.15016v1 Announce Type: new  Abstract: The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.",
        "arxiv_id": "2601.15016",
        "ARXIVID": "2601.15016",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for multimodal large language models in interactive livestream video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.15275": {
        "authors": [
            "Yu Wu",
            "Minsik Jeon",
            "Jen-Hao Rick Chang",
            "Oncel Tuzel",
            "Shubham Tulsiani"
        ],
        "title": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention",
        "abstract": "arXiv:2601.15275v1 Announce Type: new  Abstract: We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.",
        "arxiv_id": "2601.15275",
        "ARXIVID": "2601.15275",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on novel positional encoding for multi-view transformers, which is relevant for spatial reasoning in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14514": {
        "authors": [
            "Tony Chen",
            "Sam Cheyette",
            "Kelsey Allen",
            "Joshua Tenenbaum",
            "Kevin Smith"
        ],
        "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
        "abstract": "arXiv:2601.14514v1 Announce Type: new  Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.",
        "arxiv_id": "2601.14514",
        "ARXIVID": "2601.14514",
        "COMMENT": "Matches criterion 1 as it discusses spatial intelligence and reasoning in human planning and simulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.15250": {
        "authors": [
            "Zichen Xi",
            "Hao-Xiang Chen",
            "Nan Xue",
            "Hongyu Yan",
            "Qi-Yuan Feng",
            "Levent Burak Kara",
            "Joaquim Jorge",
            "Qun-Ce Xu"
        ],
        "title": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion",
        "abstract": "arXiv:2601.15250v1 Announce Type: new  Abstract: Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.",
        "arxiv_id": "2601.15250",
        "ARXIVID": "2601.15250",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks and introduces a novel generative framework for semantic scene completion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14671": {
        "authors": [
            "Yonghao Yu",
            "Lang Huang",
            "Zerun Wang",
            "Runyi Li",
            "Toshihiko Yamasaki"
        ],
        "title": "Mirai: Autoregressive Visual Generation Needs Foresight",
        "abstract": "arXiv:2601.14671v1 Announce Type: new  Abstract: Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning \"future\" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.",
        "arxiv_id": "2601.14671",
        "ARXIVID": "2601.14671",
        "COMMENT": "Matches criterion 2 as it explores autoregressive visual generation and training strategies for vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14827": {
        "authors": [
            "Ben Schaper",
            "Maxime Di Folco",
            "Bernhard Kainz",
            "Julia A. Schnabel",
            "Cosmin I. Bercea"
        ],
        "title": "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies",
        "abstract": "arXiv:2601.14827v1 Announce Type: new  Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.",
        "arxiv_id": "2601.14827",
        "ARXIVID": "2601.14827",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its exploration of vision-language models and their alignment with medical taxonomies, which is relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.15221": {
        "authors": [
            "Hanlei Guo",
            "Jiahao Shao",
            "Xinya Chen",
            "Xiyang Tan",
            "Sheng Miao",
            "Yujun Shen",
            "Yiyi Liao"
        ],
        "title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation",
        "abstract": "arXiv:2601.15221v1 Announce Type: new  Abstract: Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.",
        "arxiv_id": "2601.15221",
        "ARXIVID": "2601.15221",
        "COMMENT": "Matches criterion 6 as it focuses on urban scene generation using 3D-to-2D diffusion cascades.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.15075": {
        "authors": [
            "Chen Qian",
            "Peng Wang",
            "Dongrui Liu",
            "Junyao Yang",
            "Dadi Guo",
            "Ling Tang",
            "Jilin Mei",
            "Qihan Ren",
            "Shuai Shao",
            "Yong Liu",
            "Jie Fu",
            "Jing Shao",
            "Xia Hu"
        ],
        "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution",
        "abstract": "arXiv:2601.15075v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.",
        "arxiv_id": "2601.15075",
        "ARXIVID": "2601.15075",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and accountability in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.14875": {
        "authors": [
            "Zhe Chang",
            "Haodong Jin",
            "Ying Sun",
            "Yan Song",
            "Hui Yu"
        ],
        "title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars",
        "abstract": "arXiv:2601.14875v1 Announce Type: new  Abstract: High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.",
        "arxiv_id": "2601.14875",
        "ARXIVID": "2601.14875",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling and high-fidelity 4D facial avatars.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.14791": {
        "authors": [
            "Ziyao Ling",
            "Silvia Mirri",
            "Paola Salomoni",
            "Giovanni Delnevo"
        ],
        "title": "Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach",
        "abstract": "arXiv:2601.14791v1 Announce Type: new  Abstract: The scarcity of training data presents a fundamental challenge in applying deep learning to archaeological artifact classification, particularly for the rare types of Chinese porcelain. This study investigates whether synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) can effectively augment limited real datasets for multi-task CNN-based porcelain classification. Using MobileNetV3 with transfer learning, we conducted controlled experiments comparing models trained on pure real data against those trained on mixed real-synthetic datasets (95:5 and 90:10 ratios) across four classification tasks: dynasty, glaze, kiln and type identification. Results demonstrate task-specific benefits: type classification showed the most substantial improvement (5.5\\% F1-macro increase with 90:10 ratio), while dynasty and kiln tasks exhibited modest gains (3-4\\%), suggesting that synthetic augmentation effectiveness depends on the alignment between generated features and task-relevant visual signatures. Our work contributes practical guidelines for deploying generative AI in archaeological research, demonstrating both the potential and limitations of synthetic data when archaeological authenticity must be balanced with data diversity.",
        "arxiv_id": "2601.14791",
        "ARXIVID": "2601.14791",
        "COMMENT": "Does not match any specific criteria but is tangentially related to generative modeling and synthetic data augmentation, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.15283": {
        "authors": [
            "Ruofan Liang",
            "Norman M\\\"uller",
            "Ethan Weber",
            "Duncan Zauss",
            "Nandita Vijaykumar",
            "Peter Kontschieder",
            "Christian Richardt"
        ],
        "title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes",
        "abstract": "arXiv:2601.15283v1 Announce Type: new  Abstract: We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.",
        "arxiv_id": "2601.15283",
        "ARXIVID": "2601.15283",
        "COMMENT": "Does not match any specific criterion but is related to lighting decomposition and relighting in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.14651": {
        "authors": [
            "Chenglizhao Chen",
            "Boze Li",
            "Mengke Song",
            "Dehao Feng",
            "Xinyu Liu",
            "Shanchen Pang",
            "Jufeng Yang",
            "Hui Yu"
        ],
        "title": "READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection",
        "abstract": "arXiv:2601.14651v1 Announce Type: new  Abstract: Depression is a severe global mental health issue that impairs daily functioning and overall quality of life. Although recent audio-visual approaches have improved automatic depression detection, methods that ignore emotional cues often fail to capture subtle depressive signals hidden within emotional expressions. Conversely, those incorporating emotions frequently confuse transient emotional expressions with stable depressive symptoms in feature representations, a phenomenon termed \\emph{Emotional Ambiguity}, thereby leading to detection errors. To address this critical issue, we propose READ-Net, the first audio-visual depression detection framework explicitly designed to resolve Emotional Ambiguity through Adaptive Feature Recalibration (AFR). The core insight of AFR is to dynamically adjust the weights of emotional features to enhance depression-related signals. Rather than merely overlooking or naively combining emotional information, READ-Net innovatively identifies and preserves depressive-relevant cues within emotional features, while adaptively filtering out irrelevant emotional noise. This recalibration strategy significantly clarifies feature representations, and effectively mitigates the persistent challenge of emotional interference. Additionally, READ-Net can be easily integrated into existing frameworks for improved performance. Extensive evaluations on three publicly available datasets show that READ-Net outperforms state-of-the-art methods, with average gains of 4.55\\% in accuracy and 1.26\\% in F1-score, demonstrating its robustness to emotional disturbances and improving audio-visual depression detection.",
        "arxiv_id": "2601.14651",
        "ARXIVID": "2601.14651",
        "COMMENT": "Does not match any specific criterion but is related to multimodal learning in audio-visual depression detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
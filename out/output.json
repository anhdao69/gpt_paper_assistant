{
    "2511.20886": {
        "authors": [
            "Jiancheng Pan",
            "Runze Wang",
            "Tianwen Qian",
            "Mohammad Mahdi",
            "Yanwei Fu",
            "Xiangyang Xue",
            "Xiaomeng Huang",
            "Luc Van Gool",
            "Danda Pani Paudel",
            "Yuqian Fu"
        ],
        "title": "V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence",
        "abstract": "arXiv:2511.20886v1 Announce Type: new  Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).",
        "arxiv_id": "2511.20886",
        "ARXIVID": "2511.20886",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework (V^2-SAM) for spatial reasoning and cross-view object correspondence, which is relevant for embodied agents and robotic AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.20937": {
        "authors": [
            "Qineng Wang",
            "Wenlong Huang",
            "Yu Zhou",
            "Hang Yin",
            "Tianwei Bao",
            "Jianwen Lyu",
            "Weiyu Liu",
            "Ruohan Zhang",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Manling Li"
        ],
        "title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction",
        "abstract": "arXiv:2511.20937v1 Announce Type: new  Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.",
        "arxiv_id": "2511.20937",
        "ARXIVID": "2511.20937",
        "COMMENT": "Matches criterion 3 as it introduces a benchmark (ENACT) for evaluating embodied cognition in vision-language models, focusing on egocentric interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.21631": {
        "authors": [
            "Shuai Bai",
            "Yuxuan Cai",
            "Ruizhe Chen",
            "Keqin Chen",
            "Xionghui Chen",
            "Zesen Cheng",
            "Lianghao Deng",
            "Wei Ding",
            "Chang Gao",
            "Chunjiang Ge",
            "Wenbin Ge",
            "Zhifang Guo",
            "Qidong Huang",
            "Jie Huang",
            "Fei Huang",
            "Binyuan Hui",
            "Shutong Jiang",
            "Zhaohai Li",
            "Mingsheng Li",
            "Mei Li",
            "Kaixin Li",
            "Zicheng Lin",
            "Junyang Lin",
            "Xuejing Liu",
            "Jiawei Liu",
            "Chenglong Liu",
            "Yang Liu",
            "Dayiheng Liu",
            "Shixuan Liu",
            "Dunjie Lu",
            "Ruilin Luo",
            "Chenxu Lv",
            "Rui Men",
            "Lingchen Meng",
            "Xuancheng Ren",
            "Xingzhang Ren",
            "Sibo Song",
            "Yuchong Sun",
            "Jun Tang",
            "Jianhong Tu",
            "Jianqiang Wan",
            "Peng Wang",
            "Pengfei Wang",
            "Qiuyue Wang",
            "Yuxuan Wang",
            "Tianbao Xie",
            "Yiheng Xu",
            "Haiyang Xu",
            "Jin Xu",
            "Zhibo Yang",
            "Mingkun Yang",
            "Jianxin Yang",
            "An Yang",
            "Bowen Yu",
            "Fei Zhang",
            "Hang Zhang",
            "Xi Zhang",
            "Bo Zheng",
            "Humen Zhong",
            "Jingren Zhou",
            "Fan Zhou",
            "Jing Zhou",
            "Yuanzhi Zhu",
            "Ke Zhu"
        ],
        "title": "Qwen3-VL Technical Report",
        "abstract": "arXiv:2511.21631v1 Announce Type: new  Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
        "arxiv_id": "2511.21631",
        "ARXIVID": "2511.21631",
        "COMMENT": "Matches criterion 2 as it introduces Qwen3-VL, a vision-language model with advanced multimodal reasoning capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.21339": {
        "authors": [
            "Tae-Min Choi",
            "Tae Kyeong Jeong",
            "Garam Kim",
            "Jaemin Lee",
            "Yeongyoon Koh",
            "In Cheul Choi",
            "Jae-Ho Chung",
            "Jong Woong Park",
            "Juyoun Park"
        ],
        "title": "SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding",
        "abstract": "arXiv:2511.21339v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.",
        "arxiv_id": "2511.21339",
        "ARXIVID": "2511.21339",
        "COMMENT": "Matches criteria 3 and 6 as it introduces SurgMLLMBench, a new benchmark dataset for multimodal large language models in surgical scene understanding, which involves video-based tasks and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.20766": {
        "authors": [
            "Karen Ullrich",
            "Jingtong Su",
            "Claudia Shi",
            "Arjun Subramonian",
            "Amir Bar",
            "Ivan Evtimov",
            "Nikolaos Tsilivis",
            "Randall Balestriero",
            "Julia Kempe",
            "Mark Ibrahim"
        ],
        "title": "OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability",
        "abstract": "arXiv:2511.20766v1 Announce Type: new  Abstract: Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\\%$ to just $4\\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/",
        "arxiv_id": "2511.20766",
        "ARXIVID": "2511.20766",
        "COMMENT": "Matches criteria 3 as it introduces OpenApps, a new benchmark for evaluating UI-agent reliability under environment variations, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2511.21106": {
        "authors": [
            "Ze Feng",
            "Sen Yang",
            "Boqiang Duan",
            "Wankou Yang",
            "Jingdong Wang"
        ],
        "title": "EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens",
        "abstract": "arXiv:2511.21106v1 Announce Type: new  Abstract: Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.",
        "arxiv_id": "2511.21106",
        "ARXIVID": "2511.21106",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on improving Multimodal Large Language Models (MLLMs) with a novel knowledge distillation approach, addressing vision-language integration challenges.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.21663": {
        "authors": [
            "Naifu Zhang",
            "Wei Tao",
            "Xi Xiao",
            "Qianpu Sun",
            "Yuxin Zheng",
            "Wentao Mo",
            "Peiqiang Wang",
            "Nan Zhang"
        ],
        "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models",
        "abstract": "arXiv:2511.21663v1 Announce Type: new  Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.",
        "arxiv_id": "2511.21663",
        "ARXIVID": "2511.21663",
        "COMMENT": "Matches criteria 3 as it proposes a novel adversarial attack method (ADVLA) targeting Vision-Language-Action models in embodied intelligence, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.21272": {
        "authors": [
            "Qingyun Li",
            "Shuran Ma",
            "Junwei Luo",
            "Yi Yu",
            "Yue Zhou",
            "Fengxiang Wang",
            "Xudong Lu",
            "Xiaoxing Wang",
            "Xin He",
            "Yushi Chen",
            "Xue Yang",
            "Junchi Yan"
        ],
        "title": "Co-Training Vision Language Models for Remote Sensing Multi-task Learning",
        "abstract": "arXiv:2511.21272v1 Announce Type: new  Abstract: With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.",
        "arxiv_id": "2511.21272",
        "ARXIVID": "2511.21272",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) for remote sensing multi-task learning, including novel strategies for dynamic resolution and object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.21145": {
        "authors": [
            "Jiaming He",
            "Guanyu Hou",
            "Hongwei Li",
            "Zhicong Huang",
            "Kangjie Chen",
            "Yi Yu",
            "Wenbo Jiang",
            "Guowen Xu",
            "Tianwei Zhang"
        ],
        "title": "TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models",
        "abstract": "arXiv:2511.21145v1 Announce Type: new  Abstract: Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.",
        "arxiv_id": "2511.21145",
        "ARXIVID": "2511.21145",
        "COMMENT": "Matches criterion 6 as it introduces a framework for evaluating safety risks in text-to-video models, focusing on temporal dynamics in video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.21592": {
        "authors": [
            "Haotian Xue",
            "Qi Chen",
            "Zhonghao Wang",
            "Xun Huang",
            "Eli Shechtman",
            "Jinrong Xie",
            "Yongxin Chen"
        ],
        "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training",
        "abstract": "arXiv:2511.21592v1 Announce Type: new  Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.",
        "arxiv_id": "2511.21592",
        "ARXIVID": "2511.21592",
        "COMMENT": "Matches criterion 6 as it focuses on improving motion quality in video diffusion models, directly addressing video understanding challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.20994": {
        "authors": [
            "Yuxiao Xiang",
            "Junchi Chen",
            "Zhenchao Jin",
            "Changtao Miao",
            "Haojie Yuan",
            "Qi Chu",
            "Tao Gong",
            "Nenghai Yu"
        ],
        "title": "GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision",
        "abstract": "arXiv:2511.20994v1 Announce Type: new  Abstract: Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.",
        "arxiv_id": "2511.20994",
        "ARXIVID": "2511.20994",
        "COMMENT": "Matches criterion 2 as it explores multimodal large reasoning models (MLRMs) and introduces a novel safety auditor for vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.21662": {
        "authors": [
            "Tianyi Xiong",
            "Yi Ge",
            "Ming Li",
            "Zuolong Zhang",
            "Pranav Kulkarni",
            "Kaishen Wang",
            "Qi He",
            "Zeying Zhu",
            "Chenxi Liu",
            "Ruibo Chen",
            "Tong Zheng",
            "Yanshuo Chen",
            "Xiyao Wang",
            "Renrui Zhang",
            "Wenhu Chen",
            "Heng Huang"
        ],
        "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following",
        "abstract": "arXiv:2511.21662v1 Announce Type: new  Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.",
        "arxiv_id": "2511.21662",
        "ARXIVID": "2511.21662",
        "COMMENT": "Matches criterion 2 as it benchmarks multimodal large language models (LMMs) on pluralistic criteria-following, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.20809": {
        "authors": [
            "Ozgur Kara",
            "Yujia Chen",
            "Ming-Hsuan Yang",
            "James M. Rehg",
            "Wen-Sheng Chu",
            "Du Tran"
        ],
        "title": "Layer-Aware Video Composition via Split-then-Merge",
        "abstract": "arXiv:2511.20809v1 Announce Type: new  Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io",
        "arxiv_id": "2511.20809",
        "ARXIVID": "2511.20809",
        "COMMENT": "Matches criterion 6 as it introduces a novel framework for video composition, addressing video-based tasks with innovative methodologies.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.21475": {
        "authors": [
            "Shuai Zhang",
            "Bao Tang",
            "Siyuan Yu",
            "Yueting Zhu",
            "Jingfeng Yao",
            "Ya Zou",
            "Shanglin Yuan",
            "Li Yu",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices",
        "abstract": "arXiv:2511.21475v1 Announce Type: new  Abstract: Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.",
        "arxiv_id": "2511.21475",
        "ARXIVID": "2511.21475",
        "COMMENT": "Matches criterion 6 as it focuses on video generation (image-to-video synthesis) with novel methodologies for mobile devices.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.21565": {
        "authors": [
            "Kang Du",
            "Xue Liao",
            "Junpeng Xia",
            "Chaozheng Guo",
            "Yi Gu",
            "Yirui Guan",
            "Duotun Wang",
            "ShengHuang",
            "Zeyu Wang"
        ],
        "title": "UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes",
        "abstract": "arXiv:2511.21565v1 Announce Type: new  Abstract: Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.",
        "arxiv_id": "2511.21565",
        "ARXIVID": "2511.21565",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UAVLight) for illumination-robust 3D reconstruction, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.21025": {
        "authors": [
            "Shijia Yang",
            "Yunong Liu",
            "Bohan Zhai",
            "Ximeng Sun",
            "Zicheng Liu",
            "Emad Barsoum",
            "Manling Li",
            "Chenfeng Xu"
        ],
        "title": "CaptionQA: Is Your Caption as Useful as the Image Itself?",
        "abstract": "arXiv:2511.21025v1 Announce Type: new  Abstract: Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.",
        "arxiv_id": "2511.21025",
        "ARXIVID": "2511.21025",
        "COMMENT": "Matches criterion 5 as it evaluates the utility of image captions in multimodal systems, combining image understanding and language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.21098": {
        "authors": [
            "Gayoung Lee",
            "Junho Kim",
            "Jin-Hwa Kim",
            "Junmo Kim"
        ],
        "title": "Pygmalion Effect in Vision: Image-to-Clay Translation for Reflective Geometry Reconstruction",
        "abstract": "arXiv:2511.21098v1 Announce Type: new  Abstract: Understanding reflection remains a long-standing challenge in 3D reconstruction due to the entanglement of appearance and geometry under view-dependent reflections. In this work, we present the Pygmalion Effect in Vision, a novel framework that metaphorically \"sculpts\" reflective objects into clay-like forms through image-to-clay translation. Inspired by the myth of Pygmalion, our method learns to suppress specular cues while preserving intrinsic geometric consistency, enabling robust reconstruction from multi-view images containing complex reflections. Specifically, we introduce a dual-branch network in which a BRDF-based reflective branch is complemented by a clay-guided branch that stabilizes geometry and refines surface normals. The two branches are trained jointly using the synthesized clay-like images, which provide a neutral, reflection-free supervision signal that complements the reflective views. Experiments on both synthetic and real datasets demonstrate substantial improvement in normal accuracy and mesh completeness over existing reflection-handling methods. Beyond technical gains, our framework reveals that seeing by unshining, translating radiance into neutrality, can serve as a powerful inductive bias for reflective object geometry learning.",
        "arxiv_id": "2511.21098",
        "ARXIVID": "2511.21098",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on reflective geometry reconstruction from multi-view images.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.21113": {
        "authors": [
            "YuAn Wang",
            "Xiaofan Li",
            "Chi Huang",
            "Wenhao Zhang",
            "Hao Li",
            "Bosheng Wang",
            "Xun Sun",
            "Jun Wang"
        ],
        "title": "FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain",
        "abstract": "arXiv:2511.21113v1 Announce Type: new  Abstract: In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \\textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.",
        "arxiv_id": "2511.21113",
        "ARXIVID": "2511.21113",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates 3D scene generation with appearance-driven diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.20928": {
        "authors": [
            "Gil Goldman",
            "Raja Giryes",
            "Mahadev Satyanarayanan"
        ],
        "title": "Smooth regularization for efficient video recognition",
        "abstract": "arXiv:2511.20928v1 Announce Type: new  Abstract: We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/gilgoldm/grw-smoothing.",
        "arxiv_id": "2511.20928",
        "ARXIVID": "2511.20928",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a smooth regularization technique for video recognition models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.21477": {
        "authors": [
            "Dong-Jae Lee",
            "Jiwan Hur",
            "Jaehyun Choi",
            "Jaemyung Yu",
            "Junmo Kim"
        ],
        "title": "Frequency-Aware Token Reduction for Efficient Vision Transformer",
        "abstract": "arXiv:2511.21477v1 Announce Type: new  Abstract: Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.",
        "arxiv_id": "2511.21477",
        "ARXIVID": "2511.21477",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a token reduction strategy for Vision Transformers.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20693": {
        "authors": [
            "Mingming Zhao",
            "Xiaokang Wei",
            "Yuanqi Shao",
            "Kaiwen Zhou",
            "Lin Yang",
            "Siwei Rao",
            "Junhui Zhan",
            "Zhitang Chen"
        ],
        "title": "$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators",
        "abstract": "arXiv:2511.20693v1 Announce Type: new  Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\\% and 19.3\\% average performance improvement and reduces resource usage by 37\\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW",
        "arxiv_id": "2511.20693",
        "ARXIVID": "2511.20693",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a framework for automating agentic workflows.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20770": {
        "authors": [
            "Raghuveer Thirukovalluru",
            "Xiaochuang Han",
            "Bhuwan Dhingra",
            "Emily Dinan",
            "Maha Elbayad"
        ],
        "title": "Text-Guided Semantic Image Encoder",
        "abstract": "arXiv:2511.20770v1 Announce Type: new  Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.",
        "arxiv_id": "2511.20770",
        "ARXIVID": "2511.20770",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a text-guided semantic image encoder for vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20716": {
        "authors": [
            "Kun Guo",
            "Yun Shen",
            "Xijun Wang",
            "Chaoqun You",
            "Yun Rui",
            "Tony Q. S. Quek"
        ],
        "title": "Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?",
        "abstract": "arXiv:2511.20716v1 Announce Type: new  Abstract: Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.",
        "arxiv_id": "2511.20716",
        "ARXIVID": "2511.20716",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video object recognition and optimization strategies for video analytics.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.20991": {
        "authors": [
            "Zhiwen Zheng",
            "Yiwei Ouyang",
            "Zhao Huang",
            "Tao Zhang",
            "Xiaoshuai Zhang",
            "Huiyu Zhou",
            "Wenwen Tang",
            "Shaowei Jiang",
            "Jin Liu",
            "Xingru Huang"
        ],
        "title": "Wavefront-Constrained Passive Obscured Object Detection",
        "abstract": "arXiv:2511.20991v1 Announce Type: new  Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.",
        "arxiv_id": "2511.20991",
        "ARXIVID": "2511.20991",
        "COMMENT": "Does not match any specific criterion but involves physics-driven methods for object detection, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.21574": {
        "authors": [
            "Xiang Gu",
            "Liming Lu",
            "Xu Zheng",
            "Anan Du",
            "Yongbin Zhou",
            "Shuchao Pang"
        ],
        "title": "Multimodal Robust Prompt Distillation for 3D Point Cloud Models",
        "abstract": "arXiv:2511.21574v1 Announce Type: new  Abstract: Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.",
        "arxiv_id": "2511.21574",
        "ARXIVID": "2511.21574",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multimodal learning and robust model training.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.21523": {
        "authors": [
            "Pierre Adorni",
            "Minh-Tan Pham",
            "St\\'ephane May",
            "S\\'ebastien Lef\\`evre"
        ],
        "title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?",
        "abstract": "arXiv:2511.21523v1 Announce Type: new  Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.",
        "arxiv_id": "2511.21523",
        "ARXIVID": "2511.21523",
        "COMMENT": "Does not match any specific criterion but is generally relevant to foundation models and efficient training strategies.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.21591": {
        "authors": [
            "Charles Schepanowski",
            "Charles Ling"
        ],
        "title": "On the Limits of Innate Planning in Large Language Models",
        "abstract": "arXiv:2511.21591v1 Announce Type: new  Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.",
        "arxiv_id": "2511.21591",
        "ARXIVID": "2511.21591",
        "COMMENT": "Does not match any specific criterion but involves planning and reasoning in LLMs, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.20679": {
        "authors": [
            "Melika Ayoughi",
            "Pascal Mettes",
            "Paul Groth"
        ],
        "title": "Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring",
        "abstract": "arXiv:2511.20679v1 Announce Type: new  Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.",
        "arxiv_id": "2511.20679",
        "ARXIVID": "2511.20679",
        "COMMENT": "Does not match any specific criterion but involves LLMs and hierarchical data, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.21625": {
        "authors": [
            "Hichem Sahbi"
        ],
        "title": "Active Learning for GCN-based Action Recognition",
        "abstract": "arXiv:2511.21625v1 Announce Type: new  Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.",
        "arxiv_id": "2511.21625",
        "ARXIVID": "2511.21625",
        "COMMENT": "Does not match any specific criteria. Focuses on active learning for GCN-based action recognition, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.21444": {
        "authors": [
            "Zhe Jiang",
            "Jiong Wang",
            "Xiaoyu Yue",
            "Zijie Guo",
            "Wenlong Zhang",
            "Fenghua Ling",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "title": "EWE: An Agentic Framework for Extreme Weather Analysis",
        "abstract": "arXiv:2511.21444v1 Announce Type: new  Abstract: Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.",
        "arxiv_id": "2511.21444",
        "ARXIVID": "2511.21444",
        "COMMENT": "Does not match any specific criterion but is generally relevant to AI applications in Earth Science and extreme weather analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
{
    "2602.04304": {
        "authors": [
            "Zipeng Zhu",
            "Zhanghao Hu",
            "Qinglin Zhu",
            "Yuxi Hong",
            "Yijun Liu",
            "Jingyong Su",
            "Yulan He",
            "Lin Gui"
        ],
        "title": "Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement",
        "abstract": "arXiv:2602.04304v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static \"magic layer\" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.",
        "arxiv_id": "2602.04304",
        "ARXIVID": "2602.04304",
        "COMMENT": "This paper matches criterion 2 as it proposes a novel method for improving visual grounding and reasoning in large vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.03892": {
        "authors": [
            "Jinxing Zhou",
            "Yanghao Zhou",
            "Yaoting Wang",
            "Zongyan Han",
            "Jiaqi Ma",
            "Henghui Ding",
            "Rao Muhammad Anwer",
            "Hisham Cholakkal"
        ],
        "title": "Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation",
        "abstract": "arXiv:2602.03892v1 Announce Type: new  Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.",
        "arxiv_id": "2602.03892",
        "ARXIVID": "2602.03892",
        "COMMENT": "This paper matches criterion 5 as it integrates image/video understanding tasks with multimodal large language models for segmentation quality assessment.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.03890": {
        "authors": [
            "Xindan Zhang",
            "Weilong Yan",
            "Yufei Shi",
            "Xuerui Qiu",
            "Tao He",
            "Ying Li",
            "Ming Li",
            "Hehe Fan"
        ],
        "title": "4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping",
        "abstract": "arXiv:2602.03890v1 Announce Type: new  Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.",
        "arxiv_id": "2602.03890",
        "ARXIVID": "2602.03890",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model tailored for dynamic point cloud understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.04094": {
        "authors": [
            "Junbo Zou",
            "Ziheng Huang",
            "Shengjie Zhang",
            "Liwen Zhang",
            "Weining Shen"
        ],
        "title": "VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding",
        "abstract": "arXiv:2602.04094v1 Announce Type: new  Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.",
        "arxiv_id": "2602.04094",
        "ARXIVID": "2602.04094",
        "COMMENT": "This paper matches criterion 6 as it focuses on video understanding with a novel adaptive frame sampling method for long video analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04188": {
        "authors": [
            "Ning Zhang",
            "Zhengyu Li",
            "Kwong Weng Loh",
            "Mingxi Xu",
            "Qi Wang",
            "Zhengyu Wen",
            "Xiaoyu He",
            "Wei Zhao",
            "Kehong Gong",
            "Mingyuan Zhang"
        ],
        "title": "DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding",
        "abstract": "arXiv:2602.04188v1 Announce Type: new  Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.",
        "arxiv_id": "2602.04188",
        "ARXIVID": "2602.04188",
        "COMMENT": "This paper matches criterion 2 as it explores a novel framework for bidirectional text-motion understanding and generation, which is a multimodal integration task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04672": {
        "authors": [
            "Jin-Chuan Shi",
            "Binhong Ye",
            "Tao Liu",
            "Junzhe He",
            "Yangjinhui Xu",
            "Xiaoyang Liu",
            "Zeju Li",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation",
        "abstract": "arXiv:2602.04672v1 Announce Type: new  Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.",
        "arxiv_id": "2602.04672",
        "ARXIVID": "2602.04672",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a robust framework for hand-object interaction reconstruction, which is relevant for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04789": {
        "authors": [
            "Chengtao Lv",
            "Yumeng Shi",
            "Yushi Huang",
            "Ruihao Gong",
            "Shen Ren",
            "Wenya Wang"
        ],
        "title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
        "abstract": "arXiv:2602.04789v1 Announce Type: new  Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.",
        "arxiv_id": "2602.04789",
        "ARXIVID": "2602.04789",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel sparse attention mechanism for autoregressive video generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04441": {
        "authors": [
            "Weiguang Zhao",
            "Haoran Xu",
            "Xingyu Miao",
            "Qin Zhao",
            "Rui Zhang",
            "Kaizhu Huang",
            "Ning Gao",
            "Peizhou Cao",
            "Mingze Sun",
            "Mulin Yu",
            "Tao Lu",
            "Linning Xu",
            "Junting Dong",
            "Jiangmiao Pang"
        ],
        "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
        "abstract": "arXiv:2602.04441v1 Announce Type: new  Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.",
        "arxiv_id": "2602.04441",
        "ARXIVID": "2602.04441",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new synthetic dataset and benchmark for point tracking, which is relevant for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.04712": {
        "authors": [
            "David F. Ramirez",
            "Tim Overman",
            "Kristen Jaskie",
            "Joe Marvin",
            "Andreas Spanias"
        ],
        "title": "SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation",
        "abstract": "arXiv:2602.04712v1 Announce Type: new  Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.",
        "arxiv_id": "2602.04712",
        "ARXIVID": "2602.04712",
        "COMMENT": "Matches criterion 2 as it combines multimodal large language models with image retrieval for SAR-based visual question answering.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2602.04517": {
        "authors": [
            "Leonid Antsfeld",
            "Boris Chidlovskii",
            "Yohann Cabon",
            "Vincent Leroy",
            "Jerome Revaud"
        ],
        "title": "S-MUSt3R: Sliding Multi-view 3D Reconstruction",
        "abstract": "arXiv:2602.04517v1 Announce Type: new  Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.",
        "arxiv_id": "2602.04517",
        "ARXIVID": "2602.04517",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for scalable monocular 3D reconstruction in embodied AI settings.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2602.04657": {
        "authors": [
            "Haokui Zhang",
            "Congyang Ou",
            "Dawei Yan",
            "Peng Wang",
            "Qingsen Yan",
            "Ying Li",
            "Rong Xiao",
            "Chunhua Shen"
        ],
        "title": "PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective",
        "abstract": "arXiv:2602.04657v1 Announce Type: new  Abstract: Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.",
        "arxiv_id": "2602.04657",
        "ARXIVID": "2602.04657",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on accelerating vision-language models through token reduction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.04167": {
        "authors": [
            "Yu Zhou",
            "Xiaoyan Yang",
            "Bojia Zi",
            "Lihan Zhang",
            "Ruijie Sun",
            "Weishi Zheng",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Point2Insert: Video Object Insertion via Sparse Point Guidance",
        "abstract": "arXiv:2602.04167v1 Announce Type: new  Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\\times$10 more parameters.",
        "arxiv_id": "2602.04167",
        "ARXIVID": "2602.04167",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for video object insertion with sparse point guidance.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.04356": {
        "authors": [
            "Jaehyun Kwak",
            "Nam Cao",
            "Boryeong Cho",
            "Segyu Lee",
            "Sumyeong Ahn",
            "Se-Young Yun"
        ],
        "title": "When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models",
        "abstract": "arXiv:2602.04356v1 Announce Type: new  Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.",
        "arxiv_id": "2602.04356",
        "ARXIVID": "2602.04356",
        "COMMENT": "Matches criterion 2 as it explores adversarial attacks on large vision-language models, which is relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.04473": {
        "authors": [
            "Junjie Li",
            "Congyang Ou",
            "Haokui Zhang",
            "Guoting Wei",
            "Shengqin Jiang",
            "Ying Li",
            "Chunhua Shen"
        ],
        "title": "SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening",
        "abstract": "arXiv:2602.04473v1 Announce Type: new  Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.",
        "arxiv_id": "2602.04473",
        "ARXIVID": "2602.04473",
        "COMMENT": "This paper does not directly match any of the criteria but discusses diffusion models for pan-sharpening, which is tangentially related to computer vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.04089": {
        "authors": [
            "Xiaofeng Lin",
            "Sirou Zhu",
            "Yilei Chen",
            "Mingyu Chen",
            "Hejian Sang",
            "Ioannis Paschalidis",
            "Zhipeng Wang",
            "Aldo Pacchiano",
            "Xuezhou Zhang"
        ],
        "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
        "abstract": "arXiv:2602.04089v1 Announce Type: new  Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.",
        "arxiv_id": "2602.04089",
        "ARXIVID": "2602.04089",
        "COMMENT": "Does not match any specific criteria. Focuses on meta-reinforcement learning for LLMs, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.04837": {
        "authors": [
            "Zhaotian Weng",
            "Antonis Antoniades",
            "Deepak Nathani",
            "Zhen Zhang",
            "Xiao Pu",
            "Xin Eric Wang"
        ],
        "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
        "abstract": "arXiv:2602.04837v1 Announce Type: new  Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
        "arxiv_id": "2602.04837",
        "ARXIVID": "2602.04837",
        "COMMENT": "Does not match any specific criteria. Focuses on open-ended self-improvement in agents, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.04496": {
        "authors": [
            "Zhentao Tang",
            "Yuqi Cui",
            "Shixiong Kai",
            "Wenqian Zhao",
            "Ke Ye",
            "Xing Li",
            "Anxin Tian",
            "Zehua Pei",
            "Hui-Ling Zhen",
            "Shoubo Hu",
            "Xiaoguang Li",
            "Yunhe Wang",
            "Mingxuan Yuan"
        ],
        "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control",
        "abstract": "arXiv:2602.04496v1 Announce Type: new  Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.",
        "arxiv_id": "2602.04496",
        "ARXIVID": "2602.04496",
        "COMMENT": "Does not closely match any specific criterion but focuses on reasoning and tool orchestration, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.03978": {
        "authors": [
            "Zidi Xiong",
            "Shan Chen",
            "Himabindu Lakkaraju"
        ],
        "title": "Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning",
        "abstract": "arXiv:2602.03978v1 Announce Type: new  Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a \"free gift\" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.",
        "arxiv_id": "2602.03978",
        "ARXIVID": "2602.03978",
        "COMMENT": "This paper does not directly match any of the criteria but discusses monitorability in reasoning models, which might be tangentially interesting for understanding transparency in embodied agents or reasoning systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.04257": {
        "authors": [
            "Jiaxin Cen",
            "Xudong Mao",
            "Guanghui Yue",
            "Wei Zhou",
            "Ruomei Wang",
            "Fan Zhou",
            "Baoquan Zhao"
        ],
        "title": "Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery",
        "abstract": "arXiv:2602.04257v1 Announce Type: new  Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.",
        "arxiv_id": "2602.04257",
        "ARXIVID": "2602.04257",
        "COMMENT": "Does not match any specific criteria. Focuses on monocular video human mesh recovery, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.04204": {
        "authors": [
            "Chao Li",
            "Rui Zhang",
            "Siyuan Huang",
            "Xian Zhong",
            "Hongbo Jiang"
        ],
        "title": "AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting",
        "abstract": "arXiv:2602.04204v1 Announce Type: new  Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.",
        "arxiv_id": "2602.04204",
        "ARXIVID": "2602.04204",
        "COMMENT": "Does not match any specific criteria. Focuses on human trajectory forecasting, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.04583": {
        "authors": [
            "Gabriele Magrini",
            "Federico Becattini",
            "Niccol\\`o Biondi",
            "Pietro Pala"
        ],
        "title": "PEPR: Privileged Event-based Predictive Regularization for Domain Generalization",
        "abstract": "arXiv:2602.04583v1 Announce Type: new  Abstract: Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.",
        "arxiv_id": "2602.04583",
        "ARXIVID": "2602.04583",
        "COMMENT": "Does not closely match any specific criterion but discusses domain generalization using privileged information, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.03918": {
        "authors": [
            "Peihao Xiang",
            "Kaida Wu",
            "Ou Bai"
        ],
        "title": "Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers",
        "abstract": "arXiv:2602.03918v1 Announce Type: new  Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.",
        "arxiv_id": "2602.03918",
        "ARXIVID": "2602.03918",
        "COMMENT": "Does not closely match any specific criterion but discusses pruning in vision transformers, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.04575": {
        "authors": [
            "Jiaheng Liu",
            "Yuanxing Zhang",
            "Shihao Li",
            "Xinping Lei"
        ],
        "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
        "abstract": "arXiv:2602.04575v1 Announce Type: new  Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.   Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.",
        "arxiv_id": "2602.04575",
        "ARXIVID": "2602.04575",
        "COMMENT": "Does not closely match any specific criterion but discusses a new paradigm for content generation, which is tangentially related to multimodal systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
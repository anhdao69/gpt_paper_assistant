{
    "2511.12267": {
        "authors": [
            "Ruixun Liu",
            "Bowen Fu",
            "Jiayi Song",
            "Kaiyu Li",
            "Wanchen Li",
            "Lanxuan Xue",
            "Hui Qiao",
            "Weizhan Zhang",
            "Deyu Meng",
            "Xiangyong Cao"
        ],
        "title": "ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks",
        "abstract": "arXiv:2511.12267v1 Announce Type: new  Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.",
        "arxiv_id": "2511.12267",
        "ARXIVID": "2511.12267",
        "COMMENT": "Matches criteria 2 and 6 as it introduces a novel active perception framework for ultra-high-resolution geospatial vision-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.13647": {
        "authors": [
            "Chunshi Wang",
            "Junliang Ye",
            "Yunhan Yang",
            "Yang Li",
            "Zizhuo Lin",
            "Jun Zhu",
            "Zhuo Chen",
            "Yawei Luo",
            "Chunchao Guo"
        ],
        "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
        "abstract": "arXiv:2511.13647v1 Announce Type: new  Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
        "arxiv_id": "2511.13647",
        "ARXIVID": "2511.13647",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a novel 3D multimodal large language model (MLLM) for vision-language integration and part-based generation/editing.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.13259": {
        "authors": [
            "Yushuo Zheng",
            "Jiangyong Ying",
            "Huiyu Duan",
            "Chunyi Li",
            "Zicheng Zhang",
            "Jing Liu",
            "Xiaohong Liu",
            "Guangtao Zhai"
        ],
        "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
        "abstract": "arXiv:2511.13259v1 Announce Type: new  Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
        "arxiv_id": "2511.13259",
        "ARXIVID": "2511.13259",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) and 2 (Visual and Multimodal Large Language Models) as it introduces a new benchmark (GeoX-Bench) for cross-view geo-localization and pose estimation, and evaluates LMMs on these tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.13269": {
        "authors": [
            "Lingfeng Zhang",
            "Yuchen Zhang",
            "Hongsheng Li",
            "Haoxiang Fu",
            "Yingbo Tang",
            "Hangjun Ye",
            "Long Chen",
            "Xiaojun Liang",
            "Xiaoshuai Hao",
            "Wenbo Ding"
        ],
        "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
        "abstract": "arXiv:2511.13269v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
        "arxiv_id": "2511.13269",
        "ARXIVID": "2511.13269",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of a spatial intelligence benchmark for UAV navigation.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.13283": {
        "authors": [
            "Jongha Kim",
            "Minseong Bae",
            "Sanghyeok Lee",
            "Jinsung Yoon",
            "Hyunwoo J. Kim"
        ],
        "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing",
        "abstract": "arXiv:2511.13283v1 Announce Type: new  Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.",
        "arxiv_id": "2511.13283",
        "ARXIVID": "2511.13283",
        "COMMENT": "Matches criteria 2 as it introduces a novel multimodal large language model (MLLM) for table understanding with efficient token focusing strategies.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.11323": {
        "authors": [
            "Yitian Kou",
            "Yihe Gu",
            "Chen Zhou",
            "DanDan Zhu",
            "Shuguang Kuai"
        ],
        "title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms",
        "abstract": "arXiv:2511.11323v1 Announce Type: new  Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.",
        "arxiv_id": "2511.11323",
        "ARXIVID": "2511.11323",
        "COMMENT": "Matches criteria 3 as it introduces a novel hybrid reinforcement learning framework for socially-aware navigation in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.12452": {
        "authors": [
            "Xiaoyu Lin",
            "Aniket Ghorpade",
            "Hansheng Zhu",
            "Justin Qiu",
            "Dea Rrozhani",
            "Monica Lama",
            "Mick Yang",
            "Zixuan Bian",
            "Ruohan Ren",
            "Alan B. Hong",
            "Jiatao Gu",
            "Chris Callison-Burch"
        ],
        "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
        "abstract": "arXiv:2511.12452v1 Announce Type: new  Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
        "arxiv_id": "2511.12452",
        "ARXIVID": "2511.12452",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) due to the focus on dense annotations for multimodal datasets and their integration with vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.13655": {
        "authors": [
            "Henry Herzog",
            "Favyen Bastani",
            "Yawen Zhang",
            "Gabriel Tseng",
            "Joseph Redmon",
            "Hadrien Sablon",
            "Ryan Park",
            "Jacob Morrison",
            "Alexandra Buraczynski",
            "Karen Farley",
            "Joshua Hansen",
            "Andrew Howe",
            "Patrick Alan Johnson",
            "Mark Otterlee",
            "Ted Schmitt",
            "Hunter Pitelka",
            "Stephen Daspit",
            "Rachel Ratner",
            "Christopher Wilhelm",
            "Sebastian Wood",
            "Mike Jacobi",
            "Hannah Kerner",
            "Evan Shelhamer",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Patrick Beukema"
        ],
        "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
        "abstract": "arXiv:2511.13655v1 Announce Type: new  Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
        "arxiv_id": "2511.13655",
        "ARXIVID": "2511.13655",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) and 2 (Visual and Multimodal Large Language Models) due to its focus on a spatio-temporal foundation model for multimodal Earth observation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.13282": {
        "authors": [
            "Kaiwen Wang",
            "Kaili Zheng",
            "Yiming Shi",
            "Chenyi Guo",
            "Ji Wu"
        ],
        "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space",
        "abstract": "arXiv:2511.13282v1 Announce Type: new  Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.",
        "arxiv_id": "2511.13282",
        "ARXIVID": "2511.13282",
        "COMMENT": "Matches criteria 4 as it introduces a novel dataset and method for multi-person mesh recovery with scene-consistent optimization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.11357": {
        "authors": [
            "Haixin Li",
            "Yanke Li",
            "Diego Paez-Granados"
        ],
        "title": "KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics",
        "abstract": "arXiv:2511.11357v1 Announce Type: new  Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.",
        "arxiv_id": "2511.11357",
        "ARXIVID": "2511.11357",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of a simulation platform for multivariate time series with causal dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.12977": {
        "authors": [
            "Yixuan Yang",
            "Luyang Xie",
            "Zhen Luo",
            "Zixiang Zhao",
            "Mingqi Gao",
            "Feng Zheng"
        ],
        "title": "ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes",
        "abstract": "arXiv:2511.12977v1 Announce Type: new  Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.",
        "arxiv_id": "2511.12977",
        "ARXIVID": "2511.12977",
        "COMMENT": "Matches criterion 3 as it introduces a new pipeline for creating articulated 3D objects for simulation and robotics.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.12738": {
        "authors": [
            "Parsa Esmaeilkhani",
            "Longin Jan Latecki"
        ],
        "title": "Direct Visual Grounding by Directing Attention of Visual Tokens",
        "abstract": "arXiv:2511.12738v1 Announce Type: new  Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.",
        "arxiv_id": "2511.12738",
        "ARXIVID": "2511.12738",
        "COMMENT": "Matches criterion 2 as it proposes improvements in Vision Language Models (VLMs) for visual grounding tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.12575": {
        "authors": [
            "Jiayi Zhu",
            "Yihao Huang",
            "Yue Cao",
            "Xiaojun Jia",
            "Qing Guo",
            "Felix Juefei-Xu",
            "Geguang Pu",
            "Bin Wang"
        ],
        "title": "Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection",
        "abstract": "arXiv:2511.12575v1 Announce Type: new  Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.",
        "arxiv_id": "2511.12575",
        "ARXIVID": "2511.12575",
        "COMMENT": "Matches criterion 2 as it discusses Large Visual Language Models (LVLMs) and their vulnerabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.12054": {
        "authors": [
            "Cuiqun Chen",
            "Qi Chen",
            "Bin Yang",
            "Xingyi Zhang"
        ],
        "title": "UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization",
        "abstract": "arXiv:2511.12054v1 Announce Type: new  Abstract: Cross-view geo-localization (CVGL) matches query images ($\\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\\rightarrow$ Drone AP by +10.63\\% on University-1652 and +16.73\\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG",
        "arxiv_id": "2511.12054",
        "ARXIVID": "2511.12054",
        "COMMENT": "Does not match any specific criteria but focuses on unsupervised cross-view geo-localization, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.12024": {
        "authors": [
            "Jose Reinaldo Cunha Santos A V Silva Neto",
            "Hodaka Kawachi",
            "Yasushi Yagi",
            "Tomoya Nakamura"
        ],
        "title": "Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging",
        "abstract": "arXiv:2511.12024v1 Announce Type: new  Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.",
        "arxiv_id": "2511.12024",
        "ARXIVID": "2511.12024",
        "COMMENT": "Does not match any specific criteria but explores photorealistic imaging, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.13488": {
        "authors": [
            "Lipeng Wang",
            "Hongxing Fan",
            "Haohua Chen",
            "Zehuan Huang",
            "Lu Sheng"
        ],
        "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE",
        "abstract": "arXiv:2511.13488v1 Announce Type: new  Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.",
        "arxiv_id": "2511.13488",
        "ARXIVID": "2511.13488",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and robotics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.12992": {
        "authors": [
            "Lintong Zhang",
            "Kang Yin",
            "Seong-Whan Lee"
        ],
        "title": "Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection",
        "abstract": "arXiv:2511.12992v1 Announce Type: new  Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.",
        "arxiv_id": "2511.12992",
        "ARXIVID": "2511.12992",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to visual counterfactual explanations, which may align with your friend's general interest in computer vision and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12301": {
        "authors": [
            "Chi Liu",
            "Jincheng Liu",
            "Congcong Zhu",
            "Minghao Wang",
            "Sheng Shen",
            "Jia Gu",
            "Tianqing Zhu",
            "Wanlei Zhou"
        ],
        "title": "Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method",
        "abstract": "arXiv:2511.12301v1 Announce Type: new  Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.",
        "arxiv_id": "2511.12301",
        "ARXIVID": "2511.12301",
        "COMMENT": "Does not match any specific criteria but discusses generative data augmentation in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12525": {
        "authors": [
            "Jing Li",
            "Yifan Wang",
            "Jiafeng Yan",
            "Renlong Zhang",
            "Bin Yang"
        ],
        "title": "MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics",
        "abstract": "arXiv:2511.12525v1 Announce Type: new  Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.",
        "arxiv_id": "2511.12525",
        "ARXIVID": "2511.12525",
        "COMMENT": "Does not match any specific criteria but involves multimodal fusion in degraded scenarios.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12151": {
        "authors": [
            "Kaixiang Yang",
            "Boyang Shen",
            "Xin Li",
            "Yuchen Dai",
            "Yuxuan Luo",
            "Yueran Ma",
            "Wei Fang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing",
        "abstract": "arXiv:2511.12151v1 Announce Type: new  Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.",
        "arxiv_id": "2511.12151",
        "ARXIVID": "2511.12151",
        "COMMENT": "Does not match any specific criteria but involves text-guided image editing, which is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12956": {
        "authors": [
            "Chen Ma",
            "Ningfei Wang",
            "Junhao Zheng",
            "Qing Guo",
            "Qian Wang",
            "Qi Alfred Chen",
            "Chao Shen"
        ],
        "title": "T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving",
        "abstract": "arXiv:2511.12956v1 Announce Type: new  Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.   In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.",
        "arxiv_id": "2511.12956",
        "ARXIVID": "2511.12956",
        "COMMENT": "Does not match any specific criteria but is related to adversarial attacks in vision systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12098": {
        "authors": [
            "Xianhao Zhou",
            "Jianghao Wu",
            "Ku Zhao",
            "Jinlong He",
            "Huangxuan Zhao",
            "Lei Chen",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "title": "DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT",
        "abstract": "arXiv:2511.12098v1 Announce Type: new  Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\\rightarrow$CT and CBCT$\\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.",
        "arxiv_id": "2511.12098",
        "ARXIVID": "2511.12098",
        "COMMENT": "Does not match any specific criteria but involves vision and multimodal integration in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13145": {
        "authors": [
            "Cesar Portocarrero Rodriguez",
            "Laura Vandeweyen",
            "Yosuke Yamamoto"
        ],
        "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks",
        "abstract": "arXiv:2511.13145v1 Announce Type: new  Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.",
        "arxiv_id": "2511.13145",
        "ARXIVID": "2511.13145",
        "COMMENT": "Does not match any specific criteria but focuses on road distress detection using vision transformers and GANs, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13065": {
        "authors": [
            "Reeshoon Sayera",
            "Akash Kumar",
            "Sirshapan Mitra",
            "Prudvi Kamtam",
            "Yogesh S Rawat"
        ],
        "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
        "abstract": "arXiv:2511.13065v1 Announce Type: new  Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.",
        "arxiv_id": "2511.13065",
        "ARXIVID": "2511.13065",
        "COMMENT": "Does not match any specific criteria but focuses on robustness in gait recognition, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12740": {
        "authors": [
            "Amirhossein Hassanzadeh",
            "Bartosz Krawczyk",
            "Michael Saunders",
            "Rob Wible",
            "Keith Krause",
            "Dimah Dera",
            "Jan van Aardt"
        ],
        "title": "Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests",
        "abstract": "arXiv:2511.12740v1 Announce Type: new  Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.",
        "arxiv_id": "2511.12740",
        "ARXIVID": "2511.12740",
        "COMMENT": "Does not match any specific criteria but is related to 3D point cloud processing, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13399": {
        "authors": [
            "Yuchen Bao",
            "Yiting Wang",
            "Wenjian Huang",
            "Haowei Wang",
            "Shen Chen",
            "Taiping Yao",
            "Shouhong Ding",
            "Jianguo Zhang"
        ],
        "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
        "abstract": "arXiv:2511.13399v1 Announce Type: new  Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
        "arxiv_id": "2511.13399",
        "ARXIVID": "2511.13399",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13102": {
        "authors": [
            "Yu Zhu",
            "Dan Zeng",
            "Shuiwang Li",
            "Qijun Zhao",
            "Qiaomu Shen",
            "Bo Tang"
        ],
        "title": "CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation",
        "abstract": "arXiv:2511.13102v1 Announce Type: new  Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.",
        "arxiv_id": "2511.13102",
        "ARXIVID": "2511.13102",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and pose estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.13108": {
        "authors": [
            "Jiazhen Yan",
            "Ziqiang Li",
            "Fan Wang",
            "Boyu Wang",
            "Zhangjie Fu"
        ],
        "title": "DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection",
        "abstract": "arXiv:2511.13108v1 Announce Type: new  Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.",
        "arxiv_id": "2511.13108",
        "ARXIVID": "2511.13108",
        "COMMENT": "Does not match any specific criteria but is related to multimodal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12662": {
        "authors": [
            "Hongbin Huang",
            "Junwei Li",
            "Tianxin Xie",
            "Zhuang Li",
            "Cekai Weng",
            "Yaodong Yang",
            "Yue Luo",
            "Li Liu",
            "Jing Tang",
            "Zhijing Shao",
            "Zeyu Wang"
        ],
        "title": "Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans",
        "abstract": "arXiv:2511.12662v1 Announce Type: new  Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.",
        "arxiv_id": "2511.12662",
        "ARXIVID": "2511.12662",
        "COMMENT": "Does not match any specific criteria but is related to multimodal systems and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12939": {
        "authors": [
            "Wei Jiang",
            "Jiahao Cui",
            "Yizheng Wu",
            "Zhan Peng",
            "Zhiyu Pan",
            "Zhiguo Cao"
        ],
        "title": "Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking",
        "abstract": "arXiv:2511.12939v1 Announce Type: new  Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.",
        "arxiv_id": "2511.12939",
        "ARXIVID": "2511.12939",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.12382": {
        "authors": [
            "Ansh Makwe",
            "Akansh Agrawal",
            "Prateek Jain",
            "Akshan Agrawal",
            "Priyanka Bagade"
        ],
        "title": "AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification",
        "abstract": "arXiv:2511.12382v1 Announce Type: new  Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.",
        "arxiv_id": "2511.12382",
        "ARXIVID": "2511.12382",
        "COMMENT": "Does not match any specific criteria but is related to medical image classification.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.13222": {
        "authors": [
            "Qida Tan",
            "Hongyu Yang",
            "Wenchao Du"
        ],
        "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation",
        "abstract": "arXiv:2511.13222v1 Announce Type: new  Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.",
        "arxiv_id": "2511.13222",
        "ARXIVID": "2511.13222",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.12107": {
        "authors": [
            "Tianxiang Zhang",
            "Peipeng Yu",
            "Zhihua Xia",
            "Longchen Dai",
            "Xiaoyu Zhou",
            "Hui Gao"
        ],
        "title": "Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection",
        "abstract": "arXiv:2511.12107v1 Announce Type: new  Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.",
        "arxiv_id": "2511.12107",
        "ARXIVID": "2511.12107",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
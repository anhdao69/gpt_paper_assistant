{
    "2512.20557": {
        "authors": [
            "Shengchao Zhou",
            "Yuxin Chen",
            "Yuying Ge",
            "Wei Huang",
            "Jiehong Lin",
            "Ying Shan",
            "Xiaojuan Qi"
        ],
        "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
        "abstract": "arXiv:2512.20557v1 Announce Type: new  Abstract: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
        "arxiv_id": "2512.20557",
        "ARXIVID": "2512.20557",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 6 (Video Understanding) as it focuses on dynamic spatial reasoning in 4D for vision-language models, addressing object geometry and relationships over time.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2512.20617": {
        "authors": [
            "Yuxi Xiao",
            "Longfei Li",
            "Shen Yan",
            "Xinhang Liu",
            "Sida Peng",
            "Yunchao Wei",
            "Xiaowei Zhou",
            "Bingyi Kang"
        ],
        "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "abstract": "arXiv:2512.20617v1 Announce Type: new  Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
        "arxiv_id": "2512.20617",
        "ARXIVID": "2512.20617",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) as it evaluates spatial abilities in MLLMs and proposes a hierarchical benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.20387": {
        "authors": [
            "YuChe Hsu",
            "AnJui Wang",
            "TsaiChing Ni",
            "YuanFu Yang"
        ],
        "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
        "abstract": "arXiv:2512.20387v1 Announce Type: new  Abstract: We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",
        "arxiv_id": "2512.20387",
        "ARXIVID": "2512.20387",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a Vision-Language Simulation Model and a new dataset for generative digital twins.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.20561": {
        "authors": [
            "Kaitong Cai",
            "Jusheng Zhang",
            "Jing Yang",
            "Yijia Fan",
            "Pengtao Xie",
            "Jian Wang",
            "Keze Wang"
        ],
        "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
        "abstract": "arXiv:2512.20561v1 Announce Type: new  Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.   We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.   Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
        "arxiv_id": "2512.20561",
        "ARXIVID": "2512.20561",
        "COMMENT": "Matches criteria 2 as it proposes a novel text-guided visual token selection framework for large multimodal models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.20619": {
        "authors": [
            "Jianhong Bai",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Fu Xiao",
            "Yuanxing Zhang",
            "Qinghe Wang",
            "Xiaoyu Shi",
            "Menghan Xia",
            "Zuozhu Liu",
            "Haoji Hu",
            "Pengfei Wan",
            "Kun Gai"
        ],
        "title": "SemanticGen: Video Generation in Semantic Space",
        "abstract": "arXiv:2512.20619v1 Announce Type: new  Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
        "arxiv_id": "2512.20619",
        "ARXIVID": "2512.20619",
        "COMMENT": "Matches criteria 6 as it introduces a novel method for video generation in semantic space, pushing boundaries in video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.20206": {
        "authors": [
            "Zhe Sun",
            "Kunlun Wu",
            "Chuanjian Fu",
            "Zeming Song",
            "Langyong Shi",
            "Zihe Xue",
            "Bohan Jing",
            "Ying Yang",
            "Xiaomeng Gao",
            "Aijia Li",
            "Tianyu Guo",
            "Huiying Li",
            "Xueyuan Yang",
            "Rongkai Liu",
            "Xinyi He",
            "Yuxi Wang",
            "Yue Li",
            "Mingyuan Liu",
            "Yujie Lu",
            "Hongzhao Xie",
            "Shiyun Zhao",
            "Bo Dai",
            "Wei Wang",
            "Tao Yuan",
            "Song-Chun Zhu",
            "Yujia Peng",
            "Zhenliang Zhang"
        ],
        "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
        "abstract": "arXiv:2512.20206v1 Announce Type: new  Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",
        "arxiv_id": "2512.20206",
        "ARXIVID": "2512.20206",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces TongSIM, a general-purpose simulation platform for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2512.20013": {
        "authors": [
            "Zepeng Xin",
            "Kaiyu Li",
            "Luodi Chen",
            "Wanchen Li",
            "Yuchen Xiao",
            "Hui Qiao",
            "Weizhan Zhang",
            "Deyu Meng",
            "Xiangyong Cao"
        ],
        "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
        "abstract": "arXiv:2512.20013v1 Announce Type: new  Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",
        "arxiv_id": "2512.20013",
        "ARXIVID": "2512.20013",
        "COMMENT": "Matches criteria 2 as it proposes a multi-modal large language model (SegEarth-R2) for language-guided segmentation in remote sensing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.20606": {
        "authors": [
            "Soowon Son",
            "Honggyu An",
            "Chaehyun Kim",
            "Hyunah Ko",
            "Jisu Nam",
            "Dahyun Chung",
            "Siyoon Jin",
            "Jung Yi",
            "Jaewon Min",
            "Junhwa Hur",
            "Seungryong Kim"
        ],
        "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
        "abstract": "arXiv:2512.20606v1 Announce Type: new  Abstract: Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.",
        "arxiv_id": "2512.20606",
        "ARXIVID": "2512.20606",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for point tracking in embodied AI using video Diffusion Transformers.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.20296": {
        "authors": [
            "Ji-Hoon Kim",
            "Junseok Ahn",
            "Doyeop Kwak",
            "Joon Son Chung",
            "Shinji Watanabe"
        ],
        "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
        "abstract": "arXiv:2512.20296v1 Announce Type: new  Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.",
        "arxiv_id": "2512.20296",
        "ARXIVID": "2512.20296",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a framework for generating interactive videos and conversational speech.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.20056": {
        "authors": [
            "Hao Li",
            "Fabian Deuser",
            "Wenping Yin",
            "Steffen Knoblauch",
            "Wufan Zhao",
            "Filip Biljecki",
            "Yong Xue",
            "Wei Huang"
        ],
        "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
        "abstract": "arXiv:2512.20056v1 Announce Type: new  Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC",
        "arxiv_id": "2512.20056",
        "ARXIVID": "2512.20056",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a probabilistic geolocalization approach for disaster response.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.20148": {
        "authors": [
            "Robert van de Ven",
            "Trim Bresilla",
            "Bram Nelissen",
            "Ard Nieuwenhuizen",
            "Eldert J. van Henten",
            "Gert Kootstra"
        ],
        "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
        "abstract": "arXiv:2512.20148v1 Announce Type: new  Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.",
        "arxiv_id": "2512.20148",
        "ARXIVID": "2512.20148",
        "COMMENT": "Matches criteria 3 as it introduces a novel pipeline for improving annotations in robotic AI tasks like apple pose estimation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.20556": {
        "authors": [
            "Mingwei Tang",
            "Jiahao Nie",
            "Guang Yang",
            "Ziqing Cui",
            "Jie Li"
        ],
        "title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
        "abstract": "arXiv:2512.20556v1 Announce Type: new  Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.",
        "arxiv_id": "2512.20556",
        "ARXIVID": "2512.20556",
        "COMMENT": "Matches criteria 5 as it explores techniques combining image understanding tasks with textual guidance using multi-grained text descriptions.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.19949": {
        "authors": [
            "Zixuan Huang",
            "Xiang Li",
            "Zhaoyang Lv",
            "James M. Rehg"
        ],
        "title": "How Much 3D Do Video Foundation Models Encode?",
        "abstract": "arXiv:2512.19949v1 Announce Type: new  Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
        "arxiv_id": "2512.19949",
        "ARXIVID": "2512.19949",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it evaluates the 3D understanding of video foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.19817": {
        "authors": [
            "SaiKiran Tedla",
            "Kelly Zhu",
            "Trevor Canham",
            "Felix Taubner",
            "Michael S. Brown",
            "Kiriakos N. Kutulakos",
            "David B. Lindell"
        ],
        "title": "Generating the Past, Present and Future from a Motion-Blurred Image",
        "abstract": "arXiv:2512.19817v1 Announce Type: new  Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io",
        "arxiv_id": "2512.19817",
        "ARXIVID": "2512.19817",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on recovering videos from motion-blurred images and explores scene dynamics.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.20157": {
        "authors": [
            "Sofian Chaybouti",
            "Sanath Narayan",
            "Yasser Dahou",
            "Ph\\'uc H. L\\^e Khac",
            "Ankit Singh",
            "Ngoc Dung Huynh",
            "Wamiq Reyaz Para",
            "Hilde Kuehne",
            "Hakim Hacid"
        ],
        "title": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
        "abstract": "arXiv:2512.20157v1 Announce Type: new  Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.",
        "arxiv_id": "2512.20157",
        "ARXIVID": "2512.20157",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a new vision foundation model (AMoE) and explores multi-teacher distillation for unified visual representations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.20107": {
        "authors": [
            "Thanh-Tung Le",
            "Tuan Pham",
            "Tung Nguyen",
            "Deying Kong",
            "Xiaohui Xie",
            "Stephan Mandt"
        ],
        "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
        "abstract": "arXiv:2512.20107v1 Announce Type: new  Abstract: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.",
        "arxiv_id": "2512.20107",
        "ARXIVID": "2512.20107",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on novel view synthesis, which involves rendering photorealistic, 3D-consistent images from sparse views, a task closely related to video-based understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.20032": {
        "authors": [
            "Chang Sun",
            "Dongliang Xie",
            "Bo Qin",
            "Hong Yang"
        ],
        "title": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
        "abstract": "arXiv:2512.20032v1 Announce Type: new  Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.",
        "arxiv_id": "2512.20032",
        "ARXIVID": "2512.20032",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of vision-language integration.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.20194": {
        "authors": [
            "Zhaoyang Jia",
            "Jiahao Li",
            "Bin Li",
            "Houqiang Li",
            "Yan Lu"
        ],
        "title": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
        "abstract": "arXiv:2512.20194v1 Announce Type: new  Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.",
        "arxiv_id": "2512.20194",
        "ARXIVID": "2512.20194",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.20469": {
        "authors": [
            "Linfeng Zhang",
            "Siheng Chen",
            "Yuzhu Cai",
            "Jingyi Chai",
            "Junhan Chang",
            "Kun Chen",
            "Zhi X. Chen",
            "Zhaohan Ding",
            "Yuwen Du",
            "Yuanpeng Gao",
            "Yuan Gao",
            "Jing Gao",
            "Zhifeng Gao",
            "Qiangqiang Gu",
            "Yanhui Hong",
            "Yuan Huang",
            "Xi Fang",
            "Xiaohong Ji",
            "Guolin Ke",
            "Zixing Lei",
            "Xinyu Li",
            "Yongge Li",
            "Ruoxue Liao",
            "Hang Lin",
            "Xiaolu Lin",
            "Yuxiang Liu",
            "Xinzijian Liu",
            "Zexi Liu",
            "Jintan Lu",
            "Tingjia Miao",
            "Haohui Que",
            "Weijie Sun",
            "Yanfeng Wang",
            "Bingyang Wu",
            "Tianju Xue",
            "Rui Ye",
            "Jinzhe Zeng",
            "Duo Zhang",
            "Jiahui Zhang",
            "Linfeng Zhang",
            "Tianhan Zhang",
            "Wenchang Zhang",
            "Yuzhi Zhang",
            "Zezhong Zhang",
            "Hang Zheng",
            "Hui Zhou",
            "Tong Zhu",
            "Xinyu Zhu",
            "Qingguo Zhou",
            "Weinan E"
        ],
        "title": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale",
        "abstract": "arXiv:2512.20469v1 Announce Type: new  Abstract: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \\emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.   However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.   We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \\emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.   We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.",
        "arxiv_id": "2512.20469",
        "ARXIVID": "2512.20469",
        "COMMENT": "Does not closely match any specific criteria but is relevant to general interest in AI infrastructure and agentic science.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.19934": {
        "authors": [
            "Wentao Wu",
            "Xiao Wang",
            "Chenglong Li",
            "Jin Tang",
            "Bin Luo"
        ],
        "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
        "abstract": "arXiv:2512.19934v1 Announce Type: new  Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",
        "arxiv_id": "2512.19934",
        "ARXIVID": "2512.19934",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and multimodal learning due to its focus on vehicle-centric perception and multimodal structured pre-training.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.20140": {
        "authors": [
            "Xingyou Yin",
            "Ceyao Zhang",
            "Min Hu",
            "Kai Chen"
        ],
        "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection",
        "abstract": "arXiv:2512.20140v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.",
        "arxiv_id": "2512.20140",
        "ARXIVID": "2512.20140",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.20074": {
        "authors": [
            "H M Quamran Hasan",
            "Housam Khalifa Bashier",
            "Jiayi Dai",
            "Mi-Young Kim",
            "Randy Goebel"
        ],
        "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
        "abstract": "arXiv:2512.20074v1 Announce Type: new  Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",
        "arxiv_id": "2512.20074",
        "ARXIVID": "2512.20074",
        "COMMENT": "Does not closely match any specific criteria but is relevant to general interest in explainable AI and multi-task learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.20432": {
        "authors": [
            "Ji Song",
            "Xing Wang",
            "Jianguo Wu",
            "Xiaowei Yue"
        ],
        "title": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
        "abstract": "arXiv:2512.20432v1 Announce Type: new  Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.",
        "arxiv_id": "2512.20432",
        "ARXIVID": "2512.20432",
        "COMMENT": "Does not closely match any specific criteria but is relevant to general interest in computer vision and anomaly detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.20251": {
        "authors": [
            "Binfeng Wang",
            "Di Wang",
            "Haonan Guo",
            "Ying Fu",
            "Jing Zhang"
        ],
        "title": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
        "abstract": "arXiv:2512.20251v1 Announce Type: new  Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.",
        "arxiv_id": "2512.20251",
        "ARXIVID": "2512.20251",
        "COMMENT": "Does not closely match any specific criteria but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
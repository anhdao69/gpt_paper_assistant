{
    "2601.10781": {
        "authors": [
            "Kanchana Ranasinghe",
            "Honglu Zhou",
            "Yu Fang",
            "Luyu Yang",
            "Le Xue",
            "Ran Xu",
            "Caiming Xiong",
            "Silvio Savarese",
            "Michael S Ryoo",
            "Juan Carlos Niebles"
        ],
        "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
        "abstract": "arXiv:2601.10781v1 Announce Type: new  Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.",
        "arxiv_id": "2601.10781",
        "ARXIVID": "2601.10781",
        "COMMENT": "Matches criterion 5. Proposes a language-conditioned optical flow forecasting model for control and video generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.10744": {
        "authors": [
            "Sen Wang",
            "Bangwei Liu",
            "Zhenkun Gao",
            "Lizhuang Ma",
            "Xuhong Wang",
            "Yuan Xie",
            "Xin Tan"
        ],
        "title": "Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration",
        "abstract": "arXiv:2601.10744v1 Announce Type: new  Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.",
        "arxiv_id": "2601.10744",
        "ARXIVID": "2601.10744",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark and method for embodied exploration with long-term memory.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.11514": {
        "authors": [
            "Yawar Siddiqui",
            "Duncan Frost",
            "Samir Aroudj",
            "Armen Avetisyan",
            "Henry Howard-Jenkins",
            "Daniel DeTone",
            "Pierre Moulon",
            "Qirui Wu",
            "Zhengqin Li",
            "Julian Straub",
            "Richard Newcombe",
            "Jakob Engel"
        ],
        "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
        "abstract": "arXiv:2601.11514v1 Announce Type: new  Abstract: Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.",
        "arxiv_id": "2601.11514",
        "ARXIVID": "2601.11514",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark for 3D shape generation in real-world scenarios and proposes novel methods to handle challenges in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11254": {
        "authors": [
            "Cheng-Zhuang Liu",
            "Si-Bao Chen",
            "Qing-Ling Shu",
            "Chris Ding",
            "Jin Tang",
            "Bin Luo"
        ],
        "title": "FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection",
        "abstract": "arXiv:2601.11254v1 Announce Type: new  Abstract: Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.",
        "arxiv_id": "2601.11254",
        "ARXIVID": "2601.11254",
        "COMMENT": "Matches criterion 6. Proposes a novel method for UAV video anomaly detection with a new dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11522": {
        "authors": [
            "Ruiheng Zhang",
            "Jingfeng Yao",
            "Huangxuan Zhao",
            "Hao Yan",
            "Xiao He",
            "Lei Chen",
            "Zhou Wei",
            "Yong Luo",
            "Zengmao Wang",
            "Lefei Zhang",
            "Dacheng Tao",
            "Bo Du"
        ],
        "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
        "abstract": "arXiv:2601.11522v1 Announce Type: new  Abstract: Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
        "arxiv_id": "2601.11522",
        "ARXIVID": "2601.11522",
        "COMMENT": "Matches criterion 5. Combines image understanding and generation tasks with a unified model for chest X-ray analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11322": {
        "authors": [
            "Pavana Pradeep",
            "Krishna Kant",
            "Suya Yu"
        ],
        "title": "Enhancing Vision Language Models with Logic Reasoning for Situational Awareness",
        "abstract": "arXiv:2601.11322v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.",
        "arxiv_id": "2601.11322",
        "ARXIVID": "2601.11322",
        "COMMENT": "Matches criterion 2. Enhances vision-language models with logic reasoning for situational awareness.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.11475": {
        "authors": [
            "Rajeev Yasarla",
            "Deepti Hegde",
            "Shizhong Han",
            "Hsin-Pai Cheng",
            "Yunxiao Shi",
            "Meysam Sadeghigooghari",
            "Shweta Mahajan",
            "Apratim Bhattacharyya",
            "Litian Liu",
            "Risheek Garrepalli",
            "Thomas Svantesson",
            "Fatih Porikli",
            "Hong Cai"
        ],
        "title": "Generative Scenario Rollouts for End-to-End Autonomous Driving",
        "abstract": "arXiv:2601.11475v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.",
        "arxiv_id": "2601.11475",
        "ARXIVID": "2601.11475",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language-Action models for autonomous driving, integrating vision and language for planning and reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.11194": {
        "authors": [
            "Boyi Pang",
            "Savva Ignatyev",
            "Vladimir Ippolitov",
            "Ramil Khafizov",
            "Yurii Melnik",
            "Oleg Voynov",
            "Maksim Nakhodnov",
            "Aibek Alanov",
            "Xiaopeng Fan",
            "Peter Wonka",
            "Evgeny Burnaev"
        ],
        "title": "ATATA: One Algorithm to Align Them All",
        "abstract": "arXiv:2601.11194v1 Announce Type: new  Abstract: We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.",
        "arxiv_id": "2601.11194",
        "ARXIVID": "2601.11194",
        "COMMENT": "Matches criterion 5 as it introduces a multi-modal algorithm for joint inference across image, video, and 3D shape generation, integrating structured latent spaces.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.10922": {
        "authors": [
            "Yosub Shin",
            "Michael Buriek",
            "Boris Sobolev",
            "Pavel Bushuyeu",
            "Vikas Kumar",
            "Haoyang Xu",
            "Samuel Watson",
            "Igor Molybog"
        ],
        "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
        "abstract": "arXiv:2601.10922v1 Announce Type: new  Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.",
        "arxiv_id": "2601.10922",
        "ARXIVID": "2601.10922",
        "COMMENT": "Matches criterion 2 as it focuses on data curation for multimodal reasoning, specifically vision-language reasoning, which aligns with VLLMs and MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.11393": {
        "authors": [
            "Haomiao Tang",
            "Jinpeng Wang",
            "Minyi Zhao",
            "Guanghao Meng",
            "Ruisheng Luo",
            "Long Chen",
            "Shu-Tao Xia"
        ],
        "title": "Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning",
        "abstract": "arXiv:2601.11393v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.",
        "arxiv_id": "2601.11393",
        "ARXIVID": "2601.11393",
        "COMMENT": "Matches criterion 5 as it introduces a probabilistic learning framework for composed image retrieval, integrating multi-modal queries and uni-modal targets.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.11354": {
        "authors": [
            "Weiyi Wang",
            "Xinchi Chen",
            "Jingjing Gong",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
        "abstract": "arXiv:2601.11354v1 Announce Type: new  Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
        "arxiv_id": "2601.11354",
        "ARXIVID": "2601.11354",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (AstroReason-Bench) for evaluating agentic planning in space planning problems, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.10819": {
        "authors": [
            "Yizhou Wang",
            "Sameer Pusegaonkar",
            "Yuxing Wang",
            "Anqi Li",
            "Vishal Kumar",
            "Chetan Sethi",
            "Ganapathy Aiyer",
            "Yun He",
            "Kartikay Thakkar",
            "Swapnil Rathi",
            "Bhushan Rupde",
            "Zheng Tang",
            "Sujit Biswas"
        ],
        "title": "A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems",
        "abstract": "arXiv:2601.10819v1 Announce Type: new  Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning \"inside-out\" autonomous driving models to \"outside-in\" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.",
        "arxiv_id": "2601.10819",
        "ARXIVID": "2601.10819",
        "COMMENT": "Matches criterion 3 as it introduces a unified 3D object perception framework optimized for multi-camera systems, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.11037": {
        "authors": [
            "Shiyu Liu",
            "Yongjing Yin",
            "Jianhao Yan",
            "Yunbo Tang",
            "Qinggang Zhang",
            "Bei Li",
            "Xin Chen",
            "Jingang Wang",
            "Xunliang Cai",
            "Jinsong Su"
        ],
        "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
        "abstract": "arXiv:2601.11037v1 Announce Type: new  Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.",
        "arxiv_id": "2601.11037",
        "ARXIVID": "2601.11037",
        "COMMENT": "Matches criterion 3 as it introduces a novel RL framework (BAPO) for improving reliability in agentic search, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.10836": {
        "authors": [
            "Gerhard Krumpl",
            "Henning Avenhaus",
            "Horst Possegger"
        ],
        "title": "One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection",
        "abstract": "arXiv:2601.10836v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.",
        "arxiv_id": "2601.10836",
        "ARXIVID": "2601.10836",
        "COMMENT": "This paper does not match any specific criterion but is generally relevant to your friend's interest in machine learning and empirical insights, particularly in OOD detection.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.11396": {
        "authors": [
            "Hanlin Wu",
            "Pengfei Lin",
            "Ehsan Javanmardi",
            "Nanren Bao",
            "Bo Qian",
            "Hao Si",
            "Manabu Tsukada"
        ],
        "title": "SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction",
        "abstract": "arXiv:2601.11396v1 Announce Type: new  Abstract: As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\\% gain in efficiency.",
        "arxiv_id": "2601.11396",
        "ARXIVID": "2601.11396",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D occupancy prediction for autonomous driving.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.11248": {
        "authors": [
            "Fangke Chen",
            "Tianhao Dong",
            "Sirry Chen",
            "Guobin Zhang",
            "Yishu Zhang",
            "Yining Chen"
        ],
        "title": "Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval",
        "abstract": "arXiv:2601.11248v1 Announce Type: new  Abstract: Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.",
        "arxiv_id": "2601.11248",
        "ARXIVID": "2601.11248",
        "COMMENT": "Does not match any specific criteria. Focuses on cross-script handwriting retrieval.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11492": {
        "authors": [
            "Kaiwen Wang",
            "Kaili Zheng",
            "Rongrong Deng",
            "Qingmin Fan",
            "Milin Zhang",
            "Zongrui Li",
            "Xuesi Zhou",
            "Bo Han",
            "Liren Chen",
            "Chenyi Guo",
            "Ji Wu"
        ],
        "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics",
        "abstract": "arXiv:2601.11492v1 Announce Type: new  Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.",
        "arxiv_id": "2601.11492",
        "ARXIVID": "2601.11492",
        "COMMENT": "Does not match any specific criteria but presents an AI system for tactical analysis in boxing, which is an application of computer vision and decision-making.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10802": {
        "authors": [
            "Gerhard Krumpl",
            "Henning Avenhaus",
            "Horst Possegger"
        ],
        "title": "ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research",
        "abstract": "arXiv:2601.10802v1 Announce Type: new  Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.",
        "arxiv_id": "2601.10802",
        "ARXIVID": "2601.10802",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset for OOD detection, which is tangentially related to computer vision benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11030": {
        "authors": [
            "Xianliang Huang",
            "Jiajie Gou",
            "Shuhang Chen",
            "Zhizhou Zhong",
            "Jihong Guan",
            "Shuigeng Zhou"
        ],
        "title": "IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field",
        "abstract": "arXiv:2601.11030v1 Announce Type: new  Abstract: This paper presents the first unified distractor removal method, named IDDR-NGP, which directly operates on Instant-NPG. The method is able to remove a wide range of distractors in 3D scenes, such as snowflakes, confetti, defoliation and petals, whereas existing methods usually focus on a specific type of distractors. By incorporating implicit 3D representations with 2D detectors, we demonstrate that it is possible to efficiently restore 3D scenes from multiple corrupted images. We design the learned perceptual image patch similarity~( LPIPS) loss and the multi-view compensation loss (MVCL) to jointly optimize the rendering results of IDDR-NGP, which could aggregate information from multi-view corrupted images. All of them can be trained in an end-to-end manner to synthesize high-quality 3D scenes. To support the research on distractors removal in implicit 3D representations, we build a new benchmark dataset that consists of both synthetic and real-world distractors. To validate the effectiveness and robustness of IDDR-NGP, we provide a wide range of distractors with corresponding annotated labels added to both realistic and synthetic scenes. Extensive experimental results demonstrate the effectiveness and robustness of IDDR-NGP in removing multiple types of distractors. In addition, our approach achieves results comparable with the existing SOTA desnow methods and is capable of accurately removing both realistic and synthetic distractors.",
        "arxiv_id": "2601.11030",
        "ARXIVID": "2601.11030",
        "COMMENT": "Does not match any specific criteria but involves 3D scene restoration and distractor removal, which may be tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
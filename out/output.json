{
    "2512.15940": {
        "authors": [
            "Tin Stribor Sohn",
            "Maximilian Dillitzer",
            "Jason J. Corso",
            "Eric Sax"
        ],
        "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
        "abstract": "arXiv:2512.15940v1 Announce Type: new  Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.",
        "arxiv_id": "2512.15940",
        "ARXIVID": "2512.15940",
        "COMMENT": "Matches criteria 1 and 3 closely as it introduces a novel framework for 4D spatio-temporal reasoning in embodied agents, enabling structured memory and reasoning in dynamic environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.16561": {
        "authors": [
            "Yuxin Wang",
            "Lei Ke",
            "Boqiang Zhang",
            "Tianyuan Qu",
            "Hanxun Yu",
            "Zhenpeng Huang",
            "Meng Yu",
            "Dan Xu",
            "Dong Yu"
        ],
        "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
        "abstract": "arXiv:2512.16561v1 Announce Type: new  Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
        "arxiv_id": "2512.16561",
        "ARXIVID": "2512.16561",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) due to its focus on 3D spatial reasoning and vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.16776": {
        "authors": [
            "Kling Team",
            "Jialu Chen",
            "Yuanzheng Ci",
            "Xiangyu Du",
            "Zipeng Feng",
            "Kun Gai",
            "Sainan Guo",
            "Feng Han",
            "Jingbin He",
            "Kang He",
            "Xiao Hu",
            "Xiaohua Hu",
            "Boyuan Jiang",
            "Fangyuan Kong",
            "Hang Li",
            "Jie Li",
            "Qingyu Li",
            "Shen Li",
            "Xiaohan Li",
            "Yan Li",
            "Jiajun Liang",
            "Borui Liao",
            "Yiqiao Liao",
            "Weihong Lin",
            "Quande Liu",
            "Xiaokun Liu",
            "Yilun Liu",
            "Yuliang Liu",
            "Shun Lu",
            "Hangyu Mao",
            "Yunyao Mao",
            "Haodong Ouyang",
            "Wenyu Qin",
            "Wanqi Shi",
            "Xiaoyu Shi",
            "Lianghao Su",
            "Haozhi Sun",
            "Peiqin Sun",
            "Pengfei Wan",
            "Chao Wang",
            "Chenyu Wang",
            "Meng Wang",
            "Qiulin Wang",
            "Runqi Wang",
            "Xintao Wang",
            "Xuebo Wang",
            "Zekun Wang",
            "Min Wei",
            "Tiancheng Wen",
            "Guohao Wu",
            "Xiaoshi Wu",
            "Zhenhua Wu",
            "Da Xie",
            "Yingtong Xiong",
            "Yulong Xu",
            "Sile Yang",
            "Zikang Yang",
            "Weicai Ye",
            "Ziyang Yuan",
            "Shenglong Zhang",
            "Shuaiyu Zhang",
            "Yuanxing Zhang",
            "Yufan Zhang",
            "Wenzheng Zhao",
            "Ruiliang Zhou",
            "Yan Zhou",
            "Guosheng Zhu",
            "Yongjie Zhu"
        ],
        "title": "Kling-Omni Technical Report",
        "abstract": "arXiv:2512.16776v1 Announce Type: new  Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
        "arxiv_id": "2512.16776",
        "ARXIVID": "2512.16776",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) and criterion 6 (Video Understanding) due to its focus on multimodal video generation and reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.16909": {
        "authors": [
            "Yuanchen Ju",
            "Yongyuan Liang",
            "Yen-Jen Wang",
            "Nandiraju Gireesh",
            "Yuanliang Ju",
            "Seungjae Lee",
            "Qiao Gu",
            "Elvis Hsieh",
            "Furong Huang",
            "Koushil Sreenath"
        ],
        "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
        "abstract": "arXiv:2512.16909v1 Announce Type: new  Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
        "arxiv_id": "2512.16909",
        "ARXIVID": "2512.16909",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a novel scene graph representation (MomaGraph) for embodied agents and evaluates it with a new benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.16461": {
        "authors": [
            "Tin Stribor Sohn",
            "Maximilian Dillitzer",
            "Jason J. Corso",
            "Eric Sax"
        ],
        "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
        "abstract": "arXiv:2512.16461v1 Announce Type: new  Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
        "arxiv_id": "2512.16461",
        "ARXIVID": "2512.16461",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a framework for 4D scene understanding in robotics.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.16918": {
        "authors": [
            "Chaoyang Wang",
            "Kaituo Feng",
            "Dongyang Chen",
            "Zhongyu Wang",
            "Zhixun Li",
            "Sicheng Gao",
            "Meng Meng",
            "Xu Zhou",
            "Manyuan Zhang",
            "Yuzhang Shang",
            "Xiangyu Yue"
        ],
        "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
        "abstract": "arXiv:2512.16918v1 Announce Type: new  Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
        "arxiv_id": "2512.16918",
        "ARXIVID": "2512.16918",
        "COMMENT": "Matches criteria 2 and 5 closely as it explores a multimodal large language model (MLLM) with adaptive tool-use for images and videos, and integrates image/video understanding with reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.16615": {
        "authors": [
            "Yifan Zhou",
            "Zeqi Xiao",
            "Tianyi Wei",
            "Shuai Yang",
            "Xingang Pan"
        ],
        "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
        "abstract": "arXiv:2512.16615v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA",
        "arxiv_id": "2512.16615",
        "ARXIVID": "2512.16615",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) due to its focus on efficient sparse attention mechanisms for diffusion transformers.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.16584": {
        "authors": [
            "Jintao Tong",
            "Jiaqi Gu",
            "Yujing Lou",
            "Lubin Fan",
            "Yixiong Zou",
            "Yue Wu",
            "Jieping Ye",
            "Ruixuan Li"
        ],
        "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs",
        "abstract": "arXiv:2512.16584v1 Announce Type: new  Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.",
        "arxiv_id": "2512.16584",
        "ARXIVID": "2512.16584",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel paradigm for unified multimodal reasoning in MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.16567": {
        "authors": [
            "Yin Zhang",
            "Yongqiang Zhang",
            "Yaoyue Zheng",
            "Bogdan Raducanu",
            "Dan Liu"
        ],
        "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation",
        "abstract": "arXiv:2512.16567v1 Announce Type: new  Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.",
        "arxiv_id": "2512.16567",
        "ARXIVID": "2512.16567",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) due to its focus on fine-tuning vision foundation models for domain generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16908": {
        "authors": [
            "Yuqun Wu",
            "Chih-hao Lin",
            "Henry Che",
            "Aditi Tiwari",
            "Chuhang Zou",
            "Shenlong Wang",
            "Derek Hoiem"
        ],
        "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection",
        "abstract": "arXiv:2512.16908v1 Announce Type: new  Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.",
        "arxiv_id": "2512.16908",
        "ARXIVID": "2512.16908",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its introduction of a multiview object change detection benchmark.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16077": {
        "authors": [
            "Haomeng Zhang",
            "Kuan-Chuan Peng",
            "Suhas Lohit",
            "Raymond A. Yeh"
        ],
        "title": "Auto-Vocabulary 3D Object Detection",
        "abstract": "arXiv:2512.16077v1 Announce Type: new  Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
        "arxiv_id": "2512.16077",
        "ARXIVID": "2512.16077",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its novel framework for open-vocabulary 3D object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16564": {
        "authors": [
            "Kirill Mazur",
            "Marwan Taher",
            "Andrew J. Davison"
        ],
        "title": "4D Primitive-M\\^ach\\'e: Glueing Primitives for Persistent 4D Scene Reconstruction",
        "abstract": "arXiv:2512.16564v1 Announce Type: new  Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.   Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.   The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
        "arxiv_id": "2512.16564",
        "ARXIVID": "2512.16564",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on a novel 4D reconstruction system for dynamic scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.15957": {
        "authors": [
            "Utsav Panchal",
            "Yuchen Liu",
            "Luigi Palmieri",
            "Ilche Georgievski",
            "Marco Aiello"
        ],
        "title": "Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models",
        "abstract": "arXiv:2512.15957v1 Announce Type: new  Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.",
        "arxiv_id": "2512.15957",
        "ARXIVID": "2512.15957",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it focuses on multi-human behavior prediction for robots in human-populated environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16727": {
        "authors": [
            "Haochen Chang",
            "Pengfei Ren",
            "Buyuan Zhang",
            "Da Li",
            "Tianhao Han",
            "Haoyang Zhang",
            "Liang Xie",
            "Hongbo Chen",
            "Erwei Yin"
        ],
        "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition",
        "abstract": "arXiv:2512.16727v1 Announce Type: new  Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/",
        "arxiv_id": "2512.16727",
        "ARXIVID": "2512.16727",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (OMG-Bench) for embodied AI and proposes a novel method (HMATr) for gesture recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16913": {
        "authors": [
            "Xin Lin",
            "Meixi Song",
            "Dizhe Zhang",
            "Wenxuan Lu",
            "Haodong Li",
            "Bo Du",
            "Ming-Hsuan Yang",
            "Truong Nguyen",
            "Lu Qi"
        ],
        "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
        "abstract": "arXiv:2512.16913v1 Announce Type: new  Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
        "arxiv_id": "2512.16913",
        "ARXIVID": "2512.16913",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a foundation model for panoramic depth estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16907": {
        "authors": [
            "Mingfei Chen",
            "Yifan Wang",
            "Zhengqin Li",
            "Homanga Bharadhwaj",
            "Yujin Chen",
            "Chuan Qin",
            "Ziyi Kou",
            "Yuan Tian",
            "Eric Whitmire",
            "Rajinder Sodhi",
            "Hrvoje Benko",
            "Eli Shlizerman",
            "Yue Liu"
        ],
        "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
        "abstract": "arXiv:2512.16907v1 Announce Type: new  Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
        "arxiv_id": "2512.16907",
        "ARXIVID": "2512.16907",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a dataset and model for 3D hand trajectory prediction in egocentric videos.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16922": {
        "authors": [
            "Sihan Xu",
            "Ziqiao Ma",
            "Wenhao Chai",
            "Xuweiyi Chen",
            "Weiyang Jin",
            "Joyce Chai",
            "Saining Xie",
            "Stella X. Yu"
        ],
        "title": "Next-Embedding Prediction Makes Strong Vision Learners",
        "abstract": "arXiv:2512.16922v1 Announce Type: new  Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
        "arxiv_id": "2512.16922",
        "ARXIVID": "2512.16922",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on a novel generative pretraining method for vision models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.16921": {
        "authors": [
            "Qihao Liu",
            "Chengzhi Mao",
            "Yaojie Liu",
            "Alan Yuille",
            "Wen-Sheng Chu"
        ],
        "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
        "abstract": "arXiv:2512.16921v1 Announce Type: new  Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
        "arxiv_id": "2512.16921",
        "ARXIVID": "2512.16921",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on auditing multimodal LLMs for capability gaps.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.16413": {
        "authors": [
            "Liyuan Deng",
            "Hao Guo",
            "Yunpeng Bai",
            "Yongkang Dai",
            "Huaxi Huang",
            "Yilei Shi"
        ],
        "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
        "abstract": "arXiv:2512.16413v1 Announce Type: new  Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
        "arxiv_id": "2512.16413",
        "ARXIVID": "2512.16413",
        "COMMENT": "Matches criterion 5 as it integrates 3D geometry understanding with large language models, bridging image and language modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.16853": {
        "authors": [
            "Amita Kamath",
            "Kai-Wei Chang",
            "Ranjay Krishna",
            "Luke Zettlemoyer",
            "Yushi Hu",
            "Marjan Ghazvininejad"
        ],
        "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
        "abstract": "arXiv:2512.16853v1 Announce Type: new  Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
        "arxiv_id": "2512.16853",
        "ARXIVID": "2512.16853",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark (GenEval 2) for text-to-image evaluation, addressing benchmark drift and improving alignment with human judgment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.16501": {
        "authors": [
            "Beitong Zhou",
            "Zhexiao Huang",
            "Yuan Guo",
            "Zhangxuan Gu",
            "Tianyu Xia",
            "Zichen Luo",
            "Fei Tang",
            "Dehan Kong",
            "Yanyi Shang",
            "Suling Ou",
            "Zhenlin Guo",
            "Changhua Meng",
            "Shuheng Shen"
        ],
        "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
        "abstract": "arXiv:2512.16501v1 Announce Type: new  Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
        "arxiv_id": "2512.16501",
        "ARXIVID": "2512.16501",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its introduction of a new GUI grounding benchmark.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.16740": {
        "authors": [
            "Yunkai Yang",
            "Yudong Zhang",
            "Kunquan Zhang",
            "Jinxiao Zhang",
            "Xinying Chen",
            "Haohuan Fu",
            "Runmin Dong"
        ],
        "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
        "abstract": "arXiv:2512.16740v1 Announce Type: new  Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
        "arxiv_id": "2512.16740",
        "ARXIVID": "2512.16740",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on task-oriented data synthesis and control-rectify sampling for semantic segmentation, which involves spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.16670": {
        "authors": [
            "Ole Beisswenger",
            "Jan-Niklas Dihlmann",
            "Hendrik P. A. Lensch"
        ],
        "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
        "abstract": "arXiv:2512.16670v1 Announce Type: new  Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.",
        "arxiv_id": "2512.16670",
        "ARXIVID": "2512.16670",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks with a novel methodology for temporally consistent neural rendering.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.16219": {
        "authors": [
            "Zhihao Zhang",
            "Xuejun Yang",
            "Weihua Liu",
            "Mouquan Shen"
        ],
        "title": "Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models",
        "abstract": "arXiv:2512.16219v1 Announce Type: new  Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.",
        "arxiv_id": "2512.16219",
        "ARXIVID": "2512.16219",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on improving single-view synthesis with diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.15388": {
        "authors": [
            "Reinhard Moratz",
            "Niklas Daute",
            "James Ondieki",
            "Markus Kattenbeck",
            "Mario Krajina",
            "Ioannis Giannopoulos"
        ],
        "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations",
        "abstract": "arXiv:2512.15388v1 Announce Type: new  Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.",
        "arxiv_id": "2512.15388",
        "ARXIVID": "2512.15388",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on improving spatial reasoning for route instructions.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.16313": {
        "authors": [
            "Haiyu Zhao",
            "Yiwen Shan",
            "Yuanbiao Gou",
            "Xi Peng"
        ],
        "title": "LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation",
        "abstract": "arXiv:2512.16313v1 Announce Type: new  Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.",
        "arxiv_id": "2512.16313",
        "ARXIVID": "2512.16313",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel lightweight model for video restoration tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.16483": {
        "authors": [
            "Senmao Li",
            "Kai Wang",
            "Salman Khan",
            "Fahad Shahbaz Khan",
            "Jian Yang",
            "Yaxing Wang"
        ],
        "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
        "abstract": "arXiv:2512.16483v1 Announce Type: new  Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.",
        "arxiv_id": "2512.16483",
        "ARXIVID": "2512.16483",
        "COMMENT": "Matches criterion 4 as it focuses on improving efficiency in visual autoregressive models, which are foundational in computer vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.15567": {
        "authors": [
            "Zhangde Song",
            "Jieyu Lu",
            "Yuanqi Du",
            "Botao Yu",
            "Thomas M. Pruyn",
            "Yue Huang",
            "Kehan Guo",
            "Xiuzhe Luo",
            "Yuanhao Qu",
            "Yi Qu",
            "Yinkai Wang",
            "Haorui Wang",
            "Jeff Guo",
            "Jingru Gan",
            "Parshin Shojaee",
            "Di Luo",
            "Andres M Bran",
            "Gen Li",
            "Qiyuan Zhao",
            "Shao-Xiong Lennon Luo",
            "Yuxuan Zhang",
            "Xiang Zou",
            "Wanru Zhao",
            "Yifan F. Zhang",
            "Wucheng Zhang",
            "Shunan Zheng",
            "Saiyang Zhang",
            "Sartaaj Takrim Khan",
            "Mahyar Rajabi-Kochi",
            "Samantha Paradi-Maropakis",
            "Tony Baltoiu",
            "Fengyu Xie",
            "Tianyang Chen",
            "Kexin Huang",
            "Weiliang Luo",
            "Meijing Fang",
            "Xin Yang",
            "Lixue Cheng",
            "Jiajun He",
            "Soha Hassoun",
            "Xiangliang Zhang",
            "Wei Wang",
            "Chandan K. Reddy",
            "Chao Zhang",
            "Zhiling Zheng",
            "Mengdi Wang",
            "Le Cong",
            "Carla P. Gomes",
            "Chang-Yu Hsieh",
            "Aditya Nandy",
            "Philippe Schwaller",
            "Heather J. Kulik",
            "Haojun Jia",
            "Huan Sun",
            "Seyed Mohamad Moosavi",
            "Chenru Duan"
        ],
        "title": "Evaluating Large Language Models in Scientific Discovery",
        "abstract": "arXiv:2512.15567v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.",
        "arxiv_id": "2512.15567",
        "ARXIVID": "2512.15567",
        "COMMENT": "Does not match any specific criterion but is related to evaluating LLMs in scientific discovery, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2512.15662": {
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Xuejin Chen",
            "Yan LU"
        ],
        "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning",
        "abstract": "arXiv:2512.15662v1 Announce Type: new  Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.",
        "arxiv_id": "2512.15662",
        "ARXIVID": "2512.15662",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of reasoning and interpretability in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.16771": {
        "authors": [
            "Enis Baty",
            "C. P. Bridges",
            "Simon Hadfield"
        ],
        "title": "FlowDet: Unifying Object Detection and Generative Transport Flows",
        "abstract": "arXiv:2512.16771v1 Announce Type: new  Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.",
        "arxiv_id": "2512.16771",
        "ARXIVID": "2512.16771",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.15712": {
        "authors": [
            "Vincent Huang",
            "Dami Choi",
            "Daniel D. Johnson",
            "Sarah Schwettmann",
            "Jacob Steinhardt"
        ],
        "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
        "abstract": "arXiv:2512.15712v1 Announce Type: new  Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.",
        "arxiv_id": "2512.15712",
        "ARXIVID": "2512.15712",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to interpretability in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.16360": {
        "authors": [
            "Haotian Ling",
            "Zequn Chen",
            "Qiuying Chen",
            "Donglin Di",
            "Yongjia Ma",
            "Hao Li",
            "Chen Wei",
            "Zhulin Tao",
            "Xun Yang"
        ],
        "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
        "abstract": "arXiv:2512.16360v1 Announce Type: new  Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
        "arxiv_id": "2512.16360",
        "ARXIVID": "2512.16360",
        "COMMENT": "Does not match any specific criterion but is related to animation and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14792": {
        "authors": [
            "Roman Nekrasov",
            "Stefano Fossati",
            "Indika Kumara",
            "Damian Andrew Tamburri",
            "Willem-Jan van den Heuvel"
        ],
        "title": "IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection",
        "abstract": "arXiv:2512.14792v1 Announce Type: new  Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a \"Correctness-Congruence Gap\" where LLMs can become proficient \"coders\" but remain limited \"architects\" in fulfilling nuanced user intent.",
        "arxiv_id": "2512.14792",
        "ARXIVID": "2512.14792",
        "COMMENT": "Does not match any specific criteria but is tangentially related to general interest in LLMs and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16493": {
        "authors": [
            "Huma Hafeez",
            "Matthew Garratt",
            "Jo Plested",
            "Sankaran Iyer",
            "Arcot Sowmya"
        ],
        "title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images",
        "abstract": "arXiv:2512.16493v1 Announce Type: new  Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.",
        "arxiv_id": "2512.16493",
        "ARXIVID": "2512.16493",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of object detection and high-resolution imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16266": {
        "authors": [
            "Paloma Casteleiro Costa",
            "Parnian Ghapandar Kashani",
            "Xuhui Liu",
            "Alexander Chen",
            "Ary Portes",
            "Julien Bec",
            "Laura Marcu",
            "Aydogan Ozcan"
        ],
        "title": "Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning",
        "abstract": "arXiv:2512.16266v1 Announce Type: new  Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.",
        "arxiv_id": "2512.16266",
        "ARXIVID": "2512.16266",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of imaging and deep learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15044": {
        "authors": [
            "Wenwen Xie",
            "Geng Sun",
            "Ruichen Zhang",
            "Xuejie Liu",
            "Yinqiu Liu",
            "Jiacheng Wang",
            "Dusit Niyato",
            "Ping Zhang"
        ],
        "title": "Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study",
        "abstract": "arXiv:2512.15044v1 Announce Type: new  Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.",
        "arxiv_id": "2512.15044",
        "ARXIVID": "2512.15044",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of AI applications in sensing and communication.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16874": {
        "authors": [
            "Tom\\'a\\v{s} Sou\\v{c}ek",
            "Pierre Fernandez",
            "Hady Elsahar",
            "Sylvestre-Alvise Rebuffi",
            "Valeriu Lacatusu",
            "Tuan Tran",
            "Tom Sander",
            "Alexandre Mourachko"
        ],
        "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
        "abstract": "arXiv:2512.16874v1 Announce Type: new  Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
        "arxiv_id": "2512.16874",
        "ARXIVID": "2512.16874",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of image and video processing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16485": {
        "authors": [
            "Kejun Liu",
            "Yuanyuan Liu",
            "Lin Wei",
            "Chang Tang",
            "Yibing Zhan",
            "Zijing Chen",
            "Zhe Chen"
        ],
        "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
        "abstract": "arXiv:2512.16485v1 Announce Type: new  Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
        "arxiv_id": "2512.16485",
        "ARXIVID": "2512.16485",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multimodal learning and emotion recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16905": {
        "authors": [
            "Kaixin Ding",
            "Yang Zhou",
            "Xi Chen",
            "Miao Yang",
            "Jiarong Ou",
            "Rui Chen",
            "Xin Tao",
            "Hengshuang Zhao"
        ],
        "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
        "abstract": "arXiv:2512.16905v1 Announce Type: new  Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.",
        "arxiv_id": "2512.16905",
        "ARXIVID": "2512.16905",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and data efficiency in training.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16178": {
        "authors": [
            "M. Oltan Sevinc",
            "Liao Wu",
            "Francisco Cruz"
        ],
        "title": "Towards Closing the Domain Gap with Event Cameras",
        "abstract": "arXiv:2512.16178v1 Announce Type: new  Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.",
        "arxiv_id": "2512.16178",
        "ARXIVID": "2512.16178",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to domain adaptation and sensor-based AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16880": {
        "authors": [
            "Valay Bundele",
            "Mehran Hosseinzadeh",
            "Hendrik P. A. Lensch"
        ],
        "title": "Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation",
        "abstract": "arXiv:2512.16880v1 Announce Type: new  Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.",
        "arxiv_id": "2512.16880",
        "ARXIVID": "2512.16880",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to video segmentation and medical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15231": {
        "authors": [
            "Zhengchao Chen",
            "Haoran Wang",
            "Jing Yao",
            "Pedram Ghamisi",
            "Jun Zhou",
            "Peter M. Atkinson",
            "Bing Zhang"
        ],
        "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
        "abstract": "arXiv:2512.15231v1 Announce Type: new  Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
        "arxiv_id": "2512.15231",
        "ARXIVID": "2512.15231",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to AI and machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.16243": {
        "authors": [
            "Qi Zhang",
            "Yunfei Gong",
            "Zhidan Xie",
            "Zhizi Wang",
            "Antoni B. Chan",
            "Hui Huang"
        ],
        "title": "Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models",
        "abstract": "arXiv:2512.16243v1 Announce Type: new  Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.",
        "arxiv_id": "2512.16243",
        "ARXIVID": "2512.16243",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
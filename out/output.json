{
    "2601.20354": {
        "authors": [
            "Zengbin Wang",
            "Xuecai Hu",
            "Yong Wang",
            "Feng Xiong",
            "Man Zhang",
            "Xiangxiang Chu"
        ],
        "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
        "abstract": "arXiv:2601.20354v1 Announce Type: new  Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.",
        "arxiv_id": "2601.20354",
        "ARXIVID": "2601.20354",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating spatial intelligence in text-to-image models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.20168": {
        "authors": [
            "Zhewen Wan",
            "Tianchen Song",
            "Chen Lin",
            "Zhiyong Zhao",
            "Xianpeng Lang"
        ],
        "title": "Efficient Token Pruning for LLaDA-V",
        "abstract": "arXiv:2601.20168v1 Announce Type: new  Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.",
        "arxiv_id": "2601.20168",
        "ARXIVID": "2601.20168",
        "COMMENT": "Matches criterion 2 as it explores novel methods for improving efficiency in diffusion-based large multimodal models (LLaDA-V), which are a type of Visual Large Language Model (VLLM).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.20075": {
        "authors": [
            "Chuan Qin",
            "Constantin Venhoff",
            "Sonia Joseph",
            "Fanyi Xiao",
            "Stefan Scherer"
        ],
        "title": "Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning",
        "abstract": "arXiv:2601.20075v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.   We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.",
        "arxiv_id": "2601.20075",
        "ARXIVID": "2601.20075",
        "COMMENT": "Matches criterion 2 as it explores improvements in vision-language representation learning (Sparse CLIP) and multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.20742": {
        "authors": [
            "Xin Jin",
            "Jinming Liu",
            "Yuntao Wei",
            "Junyan Lin",
            "Zhicheng Wang",
            "Jianguo Huang",
            "Xudong Yang",
            "Yanxiao Liu",
            "Wenjun Zeng"
        ],
        "title": "Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification",
        "abstract": "arXiv:2601.20742v1 Announce Type: new  Abstract: \"Compression Tells Intelligence\", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.",
        "arxiv_id": "2601.20742",
        "ARXIVID": "2601.20742",
        "COMMENT": "Matches criterion 2 as it discusses multimodal large language models (MLLMs) and their integration with visual token technology.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.20705": {
        "authors": [
            "Zhuang Yu",
            "Lei Shen",
            "Jing Zhao",
            "Shiliang Sun"
        ],
        "title": "LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?",
        "abstract": "arXiv:2601.20705v1 Announce Type: new  Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.",
        "arxiv_id": "2601.20705",
        "ARXIVID": "2601.20705",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark (LEMON) for multimodal understanding of instructional videos, focusing on video-based tasks with temporal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.20380": {
        "authors": [
            "Le Zhang",
            "Yixiong Xiao",
            "Xinjiang Lu",
            "Jingjia Cao",
            "Yusai Zhao",
            "Jingbo Zhou",
            "Lang An",
            "Zikan Feng",
            "Wanxiang Sha",
            "Yu Shi",
            "Congxi Xiao",
            "Jian Xiong",
            "Yankai Zhang",
            "Hua Wu",
            "Haifeng Wang"
        ],
        "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
        "abstract": "arXiv:2601.20380v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
        "arxiv_id": "2601.20380",
        "ARXIVID": "2601.20380",
        "COMMENT": "Matches criterion 3 as it introduces a GUI agent model for autonomous task execution, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.20856": {
        "authors": [
            "Sebastiano Monti",
            "Carlo Nicolini",
            "Gianni Pellegrini",
            "Jacopo Staiano",
            "Bruno Lepri"
        ],
        "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
        "abstract": "arXiv:2601.20856v1 Announce Type: new  Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.",
        "arxiv_id": "2601.20856",
        "ARXIVID": "2601.20856",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SokoBench) for long-horizon planning and reasoning in large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2601.20540": {
        "authors": [
            "Robbyant Team",
            "Zelin Gao",
            "Qiuyu Wang",
            "Yanhong Zeng",
            "Jiapeng Zhu",
            "Ka Leong Cheng",
            "Yixuan Li",
            "Hanlin Wang",
            "Yinghao Xu",
            "Shuailei Ma",
            "Yihang Chen",
            "Jie Liu",
            "Yansong Cheng",
            "Yao Yao",
            "Jiayi Zhu",
            "Yihao Meng",
            "Kecheng Zheng",
            "Qingyan Bai",
            "Jingye Chen",
            "Zehong Shen",
            "Yue Yu",
            "Xing Zhu",
            "Yujun Shen",
            "Hao Ouyang"
        ],
        "title": "Advancing Open-source World Models",
        "abstract": "arXiv:2601.20540v1 Announce Type: new  Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
        "arxiv_id": "2601.20540",
        "ARXIVID": "2601.20540",
        "COMMENT": "Matches criterion 3 as it introduces a new world simulator for robot learning and content creation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.20419": {
        "authors": [
            "Yuhao Sun",
            "Chengyi Cai",
            "Jiacheng Zhang",
            "Zesheng Ye",
            "Xingliang Yuan",
            "Feng Liu"
        ],
        "title": "Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models",
        "abstract": "arXiv:2601.20419v1 Announce Type: new  Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \\emph{View Refinement} and \\emph{Description refinement}, termed as \\textit{\\textbf{Bi}-refinement for \\textbf{F}ine-grained \\textbf{T}ext-visual \\textbf{A}lignment} (BiFTA). \\emph{View refinement} removes redundant image patches with high \\emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \\emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.",
        "arxiv_id": "2601.20419",
        "ARXIVID": "2601.20419",
        "COMMENT": "Matches criterion 5 as it focuses on fine-grained text-visual alignment in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.20433": {
        "authors": [
            "Wenbo Xu",
            "Wei Lu",
            "Xiangyang Luo",
            "Jiantao Zhou"
        ],
        "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
        "abstract": "arXiv:2601.20433v1 Announce Type: new  Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.",
        "arxiv_id": "2601.20433",
        "ARXIVID": "2601.20433",
        "COMMENT": "Matches criterion 5 as it focuses on explainable deepfake detection using vision-language models, integrating image understanding and language reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.20597": {
        "authors": [
            "Shaokun Wang",
            "Weili Guan",
            "Jizhou Han",
            "Jianlong Wu",
            "Yupeng Hu",
            "Liqiang Nie"
        ],
        "title": "StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval",
        "abstract": "arXiv:2601.20597v1 Announce Type: new  Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.",
        "arxiv_id": "2601.20597",
        "ARXIVID": "2601.20597",
        "COMMENT": "Matches criterion 5 as it focuses on structured cross-modal alignment for text-to-video retrieval, integrating video understanding and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.20206": {
        "authors": [
            "Zixuan Xiao",
            "Chunguang Hu",
            "Jun Ma"
        ],
        "title": "Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis",
        "abstract": "arXiv:2601.20206v1 Announce Type: new  Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.",
        "arxiv_id": "2601.20206",
        "ARXIVID": "2601.20206",
        "COMMENT": "Matches criterion 5 as it proposes a multi-modal LLM agent framework for integrating image and language understanding in urban park monitoring.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.20279": {
        "authors": [
            "Xiaofeng Zhang",
            "Yuanchao Zhu",
            "Chaochen Gu",
            "Xiaosong Yuan",
            "Qiyan Zhao",
            "Jiawei Cao",
            "Feilong Tang",
            "Sinan Fan",
            "Yaomin Shen",
            "Chen Shen",
            "Hao Tang"
        ],
        "title": "Hallucination Begins Where Saliency Drops",
        "abstract": "arXiv:2601.20279v1 Announce Type: new  Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency",
        "arxiv_id": "2601.20279",
        "ARXIVID": "2601.20279",
        "COMMENT": "Matches criterion 2 as it focuses on reducing hallucinations in large vision-language models using saliency-guided techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.20232": {
        "authors": [
            "Junze Wang",
            "Lei Fan",
            "Dezheng Zhang",
            "Weipeng Jing",
            "Donglin Di",
            "Yang Song",
            "Sidong Liu",
            "Cong Cong"
        ],
        "title": "Visual Prompt-Agnostic Evolution",
        "abstract": "arXiv:2601.20232v1 Announce Type: new  Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\\mathtt{PAE}$ accelerates convergence with an average $1.41\\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.",
        "arxiv_id": "2601.20232",
        "ARXIVID": "2601.20232",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications through visual prompt tuning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.20520": {
        "authors": [
            "Qiyan Zhao",
            "Xiaofeng Zhang",
            "Shuochen Chang",
            "Qianyu Chen",
            "Xiaosong Yuan",
            "Xuhang Chen",
            "Luoqi Liu",
            "Jiajun Zhang",
            "Xu-Yao Zhang",
            "Da-Han Wang"
        ],
        "title": "Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective",
        "abstract": "arXiv:2601.20520v1 Announce Type: new  Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \\textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \\textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA",
        "arxiv_id": "2601.20520",
        "ARXIVID": "2601.20520",
        "COMMENT": "Matches criterion 2 as it explores diffusion-based multimodal large language models (dMLLMs) and addresses challenges in their decoding process.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.20090": {
        "authors": [
            "Amirmohammad Farzaneh",
            "Salvatore D'Oro",
            "Osvaldo Simeone"
        ],
        "title": "Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control",
        "abstract": "arXiv:2601.20090v1 Announce Type: new  Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.",
        "arxiv_id": "2601.20090",
        "ARXIVID": "2601.20090",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and embodied agents through counterfactual reasoning in LLM-based control scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.20661": {
        "authors": [
            "Michele Mazzamuto",
            "Daniele Di Mauro",
            "Gianpiero Francesca",
            "Giovanni Maria Farinella",
            "Antonino Furnari"
        ],
        "title": "ProSkill: Segment-Level Skill Assessment in Procedural Videos",
        "abstract": "arXiv:2601.20661v1 Announce Type: new  Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/",
        "arxiv_id": "2601.20661",
        "ARXIVID": "2601.20661",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark dataset (ProSkill) for skill assessment in procedural videos, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.20304": {
        "authors": [
            "Genyuan Zhang",
            "Zihao Wang",
            "Zhifan Gao",
            "Lei Xu",
            "Zhen Zhou",
            "Haijun Yu",
            "Jianjia Zhang",
            "Xiujian Liu",
            "Weiwei Zhang",
            "Shaoyu Wang",
            "Huazhu Fu",
            "Fenglin Liu",
            "Weiwen Wu"
        ],
        "title": "Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction",
        "abstract": "arXiv:2601.20304v1 Announce Type: new  Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.",
        "arxiv_id": "2601.20304",
        "ARXIVID": "2601.20304",
        "COMMENT": "Matches criterion 1 as it integrates spatial intelligence and reasoning in a medical imaging context.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2601.20539": {
        "authors": [
            "Oguzhan Gungordu",
            "Siheng Xiong",
            "Faramarz Fekri"
        ],
        "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs",
        "abstract": "arXiv:2601.20539v1 Announce Type: new  Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.",
        "arxiv_id": "2601.20539",
        "ARXIVID": "2601.20539",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to your friend's interest in LLMs and reasoning frameworks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.20246": {
        "authors": [
            "Jan Niklas Kolf",
            "Ozan Tezcan",
            "Justin Theiss",
            "Hyung Jun Kim",
            "Wentao Bao",
            "Bhargav Bhushanam",
            "Khushi Gupta",
            "Arun Kejariwal",
            "Naser Damer",
            "Fadi Boutros"
        ],
        "title": "BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning",
        "abstract": "arXiv:2601.20246v1 Announce Type: new  Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.",
        "arxiv_id": "2601.20246",
        "ARXIVID": "2601.20246",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for deep metric learning, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.20352": {
        "authors": [
            "Weiquan Huang",
            "Zixuan Wang",
            "Hehai Lin",
            "Sudong Wang",
            "Bo Xu",
            "Qian Li",
            "Beier Zhu",
            "Linyi Yang",
            "Chengwei Qin"
        ],
        "title": "AMA: Adaptive Memory via Multi-Agent Collaboration",
        "abstract": "arXiv:2601.20352v1 Announce Type: new  Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.",
        "arxiv_id": "2601.20352",
        "ARXIVID": "2601.20352",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to your friend's interest in LLMs and memory systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.20351": {
        "authors": [
            "Chenke Zhang",
            "Ziyuan Yang",
            "Licheng Yan",
            "Shuyi Li",
            "Andrew Beng Jin Teoh",
            "Bob Zhang",
            "Yi Zhang"
        ],
        "title": "PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification",
        "abstract": "arXiv:2601.20351v1 Announce Type: new  Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.",
        "arxiv_id": "2601.20351",
        "ARXIVID": "2601.20351",
        "COMMENT": "Does not match any specific criterion but is related to general machine learning and biometric systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.20306": {
        "authors": [
            "Yanjie Tu",
            "Qingsen Yan",
            "Axi Niu",
            "Jiacong Tang"
        ],
        "title": "TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration",
        "abstract": "arXiv:2601.20306v1 Announce Type: new  Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.",
        "arxiv_id": "2601.20306",
        "ARXIVID": "2601.20306",
        "COMMENT": "Does not match any specific criteria. Focuses on image restoration using diffusion models, which is unrelated to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.20331": {
        "authors": [
            "Mai Su",
            "Qihan Yu",
            "Zhongtao Wang",
            "Yilong Li",
            "Chengwei Pan",
            "Yisong Chen",
            "Guoping Wang"
        ],
        "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
        "abstract": "arXiv:2601.20331v1 Announce Type: new  Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.",
        "arxiv_id": "2601.20331",
        "ARXIVID": "2601.20331",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D surface reconstruction, which is unrelated to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.20333": {
        "authors": [
            "Ali Zia",
            "Usman Ali",
            "Umer Ramzan",
            "Abdul Rehman",
            "Abdelwahed Khamis",
            "Wei Xiang"
        ],
        "title": "Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining",
        "abstract": "arXiv:2601.20333v1 Announce Type: new  Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.",
        "arxiv_id": "2601.20333",
        "ARXIVID": "2601.20333",
        "COMMENT": "Does not match any specific criteria. Focuses on anomaly segmentation using topological data analysis, which is unrelated to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.20601": {
        "authors": [
            "Zhuonan Wang",
            "Wenjie Yan",
            "Wenqiao Zhang",
            "Xiaohui Song",
            "Jian Ma",
            "Ke Yao",
            "Yibo Yu",
            "Beng Chin Ooi"
        ],
        "title": "CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification",
        "abstract": "arXiv:2601.20601v1 Announce Type: new  Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.",
        "arxiv_id": "2601.20601",
        "ARXIVID": "2601.20601",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image classification, not spatial intelligence, vision-language models, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
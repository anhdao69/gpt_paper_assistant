{
    "2512.14234": {
        "authors": [
            "Juze Zhang",
            "Changan Chen",
            "Xin Chen",
            "Heng Yu",
            "Tiange Xiang",
            "Ali Sartaz Khan",
            "Shrinidhi K. Lakshmikanth",
            "Ehsan Adeli"
        ],
        "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
        "abstract": "arXiv:2512.14234v1 Announce Type: new  Abstract: Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/",
        "arxiv_id": "2512.14234",
        "ARXIVID": "2512.14234",
        "COMMENT": "Matches criteria 1 and 2 closely. The paper introduces a novel conversational 3D agent (ViBES) that integrates spatial intelligence and embodied agent capabilities (criterion 1) with multimodal large language model techniques (criterion 2).",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14654": {
        "authors": [
            "Lihong Wang",
            "Liangqi Li",
            "Weiwei Feng",
            "Jiamin Wu",
            "Changtao Miao",
            "Tieru Wu",
            "Rui Ma",
            "Bo Zhang",
            "Zhe Li"
        ],
        "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking",
        "abstract": "arXiv:2512.14654v1 Announce Type: new  Abstract: CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.",
        "arxiv_id": "2512.14654",
        "ARXIVID": "2512.14654",
        "COMMENT": "Matches criteria 2 and 5 as it explores multimodal large language models (MLLMs) with a focus on mathematical reasoning and introduces a novel Reason Chunking mechanism.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.14614": {
        "authors": [
            "Wenqiang Sun",
            "Haiyu Zhang",
            "Haoyuan Wang",
            "Junta Wu",
            "Zehan Wang",
            "Zhenwei Wang",
            "Yunhong Wang",
            "Jun Zhang",
            "Tengfei Wang",
            "Chunchao Guo"
        ],
        "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "abstract": "arXiv:2512.14614v1 Announce Type: new  Abstract: This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
        "arxiv_id": "2512.14614",
        "ARXIVID": "2512.14614",
        "COMMENT": "Matches criteria 3 and 6 closely. The paper presents WorldPlay, a new benchmark and method for real-time interactive world modeling (criterion 3) and addresses video understanding challenges with long-term geometric consistency (criterion 6).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.14014": {
        "authors": [
            "Shufan Li",
            "Konstantinos Kallidromitis",
            "Akash Gokul",
            "Yusuke Kato",
            "Kazuki Kozuka",
            "Aditya Grover"
        ],
        "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
        "abstract": "arXiv:2512.14014v1 Announce Type: new  Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
        "arxiv_id": "2512.14014",
        "ARXIVID": "2512.14014",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MobileWorldBench) for embodied agents, specifically focusing on semantic world modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.14698": {
        "authors": [
            "Jun Zhang",
            "Teng Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Xinhao Li",
            "Ying Shan",
            "Limin Wang"
        ],
        "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
        "abstract": "arXiv:2512.14698v1 Announce Type: new  Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
        "arxiv_id": "2512.14698",
        "ARXIVID": "2512.14698",
        "COMMENT": "Matches criteria 6 as it focuses on video temporal grounding, a core video understanding task, and introduces new benchmarks and insights.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.14157": {
        "authors": [
            "Yankai Jiang",
            "Yujie Zhang",
            "Peng Zhang",
            "Yichen Li",
            "Jintai Chen",
            "Xiaoming Shi",
            "Shihui Zhen"
        ],
        "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis",
        "abstract": "arXiv:2512.14157v1 Announce Type: new  Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.",
        "arxiv_id": "2512.14157",
        "ARXIVID": "2512.14157",
        "COMMENT": "Matches criterion 2 as it focuses on reasoning-based multimodal large language models for medical image analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14257": {
        "authors": [
            "Wentao Wan",
            "Kaiyu Wu",
            "Qingyang Ma",
            "Nan Kang",
            "Yunjie Chen",
            "Liang Lin",
            "Keze Wang"
        ],
        "title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs",
        "abstract": "arXiv:2512.14257v1 Announce Type: new  Abstract: Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.",
        "arxiv_id": "2512.14257",
        "ARXIVID": "2512.14257",
        "COMMENT": "Matches criterion 2 as it enhances visual reasoning using probabilistic graphs, which aligns with advancements in visual and multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14696": {
        "authors": [
            "Zihan Wang",
            "Jiashun Wang",
            "Jeff Tan",
            "Yiwen Zhao",
            "Jessica Hodgins",
            "Shubham Tulsiani",
            "Deva Ramanan"
        ],
        "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
        "abstract": "arXiv:2512.14696v1 Announce Type: new  Abstract: We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
        "arxiv_id": "2512.14696",
        "ARXIVID": "2512.14696",
        "COMMENT": "Matches criterion 3 as it introduces a method for simulatable human motion and scene geometry recovery, relevant to embodied AI and robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14225": {
        "authors": [
            "Tao Tang",
            "Enhui Ma",
            "xia zhou",
            "Letian Wang",
            "Tianyi Yan",
            "Xueyang Zhang",
            "Kun Zhan",
            "Peng Jia",
            "XianPeng Lang",
            "Jia-Wang Bian",
            "Kaicheng Yu",
            "Xiaodan Liang"
        ],
        "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
        "abstract": "arXiv:2512.14225v1 Announce Type: new  Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
        "arxiv_id": "2512.14225",
        "ARXIVID": "2512.14225",
        "COMMENT": "Matches criterion 5 as it focuses on integrating multimodal sensor data (image and LiDAR) with a unified framework, which aligns with combining image/video tasks and LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14699": {
        "authors": [
            "Sihui Ji",
            "Xi Chen",
            "Shuai Yang",
            "Xin Tao",
            "Pengfei Wan",
            "Hengshuang Zhao"
        ],
        "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
        "abstract": "arXiv:2512.14699v1 Announce Type: new  Abstract: The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
        "arxiv_id": "2512.14699",
        "ARXIVID": "2512.14699",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically long video narrative generation with novel memory design.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14008": {
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
        "abstract": "arXiv:2512.14008v1 Announce Type: new  Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.",
        "arxiv_id": "2512.14008",
        "ARXIVID": "2512.14008",
        "COMMENT": "Matches criteria 2 as it focuses on multimodal discrete diffusion language models and improves inference speed for multimodal tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14162": {
        "authors": [
            "Qingyuan Cai",
            "Linxin Zhang",
            "Xuecai Hu",
            "Saihui Hou",
            "Yongzhen Huang"
        ],
        "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
        "abstract": "arXiv:2512.14162v1 Announce Type: new  Abstract: Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
        "arxiv_id": "2512.14162",
        "ARXIVID": "2512.14162",
        "COMMENT": "Matches criterion 3 as it introduces a new framework and method for 3D human pose estimation, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.13991": {
        "authors": [
            "Yao He",
            "Youngjoong Kwon",
            "Tiange Xiang",
            "Wenxiao Cai",
            "Ehsan Adeli"
        ],
        "title": "Repurposing 2D Diffusion Models for 3D Shape Completion",
        "abstract": "arXiv:2512.13991v1 Announce Type: new  Abstract: We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.",
        "arxiv_id": "2512.13991",
        "ARXIVID": "2512.13991",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it adapts 2D diffusion models for 3D shape completion, integrating 2D and 3D modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.14044": {
        "authors": [
            "Zhenguo Zhang",
            "Haohan Zhen",
            "Yishen Wang",
            "Le Xu",
            "Tianchen Deng",
            "Xuefeng Chen",
            "Qu Chen",
            "Bo Zhang",
            "Wuxiong Huang"
        ],
        "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving",
        "abstract": "arXiv:2512.14044v1 Announce Type: new  Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.",
        "arxiv_id": "2512.14044",
        "ARXIVID": "2512.14044",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) with a novel interleaved multi-modal chain-of-thought mechanism.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.14032": {
        "authors": [
            "Ignacio Alzugaray",
            "Marwan Taher",
            "Andrew J. Davison"
        ],
        "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
        "abstract": "arXiv:2512.14032v1 Announce Type: new  Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.   Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
        "arxiv_id": "2512.14032",
        "ARXIVID": "2512.14032",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied/robotic AI with a neural implicit SLAM system.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.14126": {
        "authors": [
            "Junyi Wu",
            "Van Nguyen Nguyen",
            "Benjamin Planche",
            "Jiachen Tao",
            "Changchang Sun",
            "Zhongpai Gao",
            "Zhenghao Zhao",
            "Anwesa Choudhuri",
            "Gengyu Zhang",
            "Meng Zheng",
            "Feiran Wang",
            "Terrence Chen",
            "Yan Yan",
            "Ziyan Wu"
        ],
        "title": "Consistent Instance Field for Dynamic Scene Understanding",
        "abstract": "arXiv:2512.14126v1 Announce Type: new  Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
        "arxiv_id": "2512.14126",
        "ARXIVID": "2512.14126",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel spatio-temporal representation for dynamic scene understanding, which is relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.13747": {
        "authors": [
            "Siyuan Dai",
            "Lunxiao Li",
            "Kun Zhao",
            "Eardi Lila",
            "Paul K. Crane",
            "Heng Huang",
            "Dongkuan Xu",
            "Haoteng Tang",
            "Liang Zhan"
        ],
        "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making",
        "abstract": "arXiv:2512.13747v1 Announce Type: new  Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.",
        "arxiv_id": "2512.13747",
        "ARXIVID": "2512.13747",
        "COMMENT": "Matches criterion 2 as it investigates limitations and strategies for multimodal large language models (MLLMs) in medical decision-making.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14236": {
        "authors": [
            "Nando Metzger",
            "Prune Truong",
            "Goutam Bhat",
            "Konrad Schindler",
            "Federico Tombari"
        ],
        "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
        "abstract": "arXiv:2512.14236v1 Announce Type: new  Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
        "arxiv_id": "2512.14236",
        "ARXIVID": "2512.14236",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding with a novel method for stereo video conversion.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14499": {
        "authors": [
            "Jia Guo",
            "Jiawei Du",
            "Shengzhu Yang",
            "Shuai Lu",
            "Wenquan Cheng",
            "Kaiwen Zhang",
            "Yihua Sun",
            "Chuhong Yang",
            "Weihang Zhang",
            "Fang Chen",
            "Yilan Wu",
            "Lie Ju",
            "Guochen Ning",
            "Longfei Ma",
            "Huiping Yao",
            "Jinyuan Wang",
            "Peilun Shi",
            "Yukun Zhou",
            "Jie Xu",
            "Pearse A. Keane",
            "Hanruo Liu",
            "Hongen Liao",
            "Ningli Wang",
            "Huiqi Li"
        ],
        "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency",
        "abstract": "arXiv:2512.14499v1 Announce Type: new  Abstract: Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.",
        "arxiv_id": "2512.14499",
        "ARXIVID": "2512.14499",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model in computer vision with real-world applications in retinal imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14406": {
        "authors": [
            "Le Jiang",
            "Shaotong Zhu",
            "Yedi Luo",
            "Shayda Moezzi",
            "Sarah Ostadabbas"
        ],
        "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
        "abstract": "arXiv:2512.14406v1 Announce Type: new  Abstract: In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
        "arxiv_id": "2512.14406",
        "ARXIVID": "2512.14406",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks with novel methodologies for dynamic scene view synthesis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14594": {
        "authors": [
            "Chenyu Zhao",
            "Yingxue Xu",
            "Fengtao Zhou",
            "Yihui Wang",
            "Hao Chen"
        ],
        "title": "LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction",
        "abstract": "arXiv:2512.14594v1 Announce Type: new  Abstract: Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.",
        "arxiv_id": "2512.14594",
        "ARXIVID": "2512.14594",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) with novel integration of expert reports and prognostic background knowledge.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14621": {
        "authors": [
            "Zhenghao Zhao",
            "Haoxuan Wang",
            "Kai Wang",
            "Yuzhang Shang",
            "Yuan Hong",
            "Yan Yan"
        ],
        "title": "Distill Video Datasets into Images",
        "abstract": "arXiv:2512.14621v1 Announce Type: new  Abstract: Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.",
        "arxiv_id": "2512.14621",
        "ARXIVID": "2512.14621",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video dataset distillation, a video-based task, with novel methodologies for optimization.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2512.14697": {
        "authors": [
            "Yue Zhao",
            "Hanwen Jiang",
            "Zhenlin Xu",
            "Chutong Yang",
            "Ehsan Adeli",
            "Philipp Kr\\\"ahenb\\\"uhl"
        ],
        "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "abstract": "arXiv:2512.14697v1 Announce Type: new  Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\\Lambda_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.",
        "arxiv_id": "2512.14697",
        "ARXIVID": "2512.14697",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it explores a novel quantization method for image tokenization and generation, which is relevant to foundational vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14601": {
        "authors": [
            "Zhaolun Li",
            "Jichang Li",
            "Yinqi Cai",
            "Junye Chen",
            "Xiaonan Luo",
            "Guanbin Li",
            "Rushi Lan"
        ],
        "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos",
        "abstract": "arXiv:2512.14601v1 Announce Type: new  Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.",
        "arxiv_id": "2512.14601",
        "ARXIVID": "2512.14601",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on deepfake video detection, a video-based task, with novel methodologies for cross-domain generalization.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2512.13876": {
        "authors": [
            "Ye Zhang",
            "Qi Chen",
            "Wenyou Huang",
            "Rui Liu",
            "Zhengjian Kang"
        ],
        "title": "Route-DETR: Pairwise Query Routing in Transformers for Object Detection",
        "abstract": "arXiv:2512.13876v1 Announce Type: new  Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.",
        "arxiv_id": "2512.13876",
        "ARXIVID": "2512.13876",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes improvements to DETR, a foundational model for object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2512.14028": {
        "authors": [
            "Jiaheng Li",
            "Qiyu Dai",
            "Lihan Li",
            "Praneeth Chakravarthula",
            "He Sun",
            "Baoquan Chen",
            "Wenzheng Chen"
        ],
        "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
        "abstract": "arXiv:2512.14028v1 Announce Type: new  Abstract: We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
        "arxiv_id": "2512.14028",
        "ARXIVID": "2512.14028",
        "COMMENT": "Does not match any specific criteria but is related to 3D imaging and neural feature decoding, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2512.14341": {
        "authors": [
            "Jie Zhang",
            "Shuai Dong",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Towards Transferable Defense Against Malicious Image Edits",
        "abstract": "arXiv:2512.14341v1 Announce Type: new  Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.",
        "arxiv_id": "2512.14341",
        "ARXIVID": "2512.14341",
        "COMMENT": "Does not match any specific criteria but is related to defense against malicious image edits, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.13996": {
        "authors": [
            "Can Jin",
            "Hongwu Peng",
            "Mingcan Xiang",
            "Qixin Zhang",
            "Xiangchi Yuan",
            "Amit Hasan",
            "Ohiremen Dibua",
            "Yifan Gong",
            "Yan Kang",
            "Dimitris N. Metaxas"
        ],
        "title": "Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training",
        "abstract": "arXiv:2512.13996v1 Announce Type: new  Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.",
        "arxiv_id": "2512.13996",
        "ARXIVID": "2512.13996",
        "COMMENT": "Does not match any specific criteria but is related to sparse mixture-of-experts architectures, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14320": {
        "authors": [
            "Shuai Dong",
            "Jie Zhang",
            "Guoying Zhao",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
        "abstract": "arXiv:2512.14320v1 Announce Type: new  Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
        "arxiv_id": "2512.14320",
        "ARXIVID": "2512.14320",
        "COMMENT": "Does not match any specific criteria but is related to image editing immunity, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14048": {
        "authors": [
            "Shen Li",
            "Li Huang",
            "Shaoxiong Zhan",
            "Weifeng Sun",
            "Tao Yin",
            "Zhongxin Liu",
            "Meng Yan"
        ],
        "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation",
        "abstract": "arXiv:2512.14048v1 Announce Type: new  Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.",
        "arxiv_id": "2512.14048",
        "ARXIVID": "2512.14048",
        "COMMENT": "Does not match any specific criteria but is related to structured reasoning and code generation, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14333": {
        "authors": [
            "Jie Zhang",
            "Shuai Dong",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Dual Attention Guided Defense Against Malicious Edits",
        "abstract": "arXiv:2512.14333v1 Announce Type: new  Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.",
        "arxiv_id": "2512.14333",
        "ARXIVID": "2512.14333",
        "COMMENT": "Does not match any specific criteria but is tangentially related to image editing and defense mechanisms, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.14550": {
        "authors": [
            "Zhiwen Yang",
            "Jiaju Zhang",
            "Yang Yi",
            "Jian Liang",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
        "abstract": "arXiv:2512.14550v1 Announce Type: new  Abstract: Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.",
        "arxiv_id": "2512.14550",
        "ARXIVID": "2512.14550",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on medical image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14158": {
        "authors": [
            "Shuxin Zhao",
            "Bo Lang",
            "Nan Xiao",
            "Yilang Zhang"
        ],
        "title": "CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World",
        "abstract": "arXiv:2512.14158v1 Announce Type: new  Abstract: Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.",
        "arxiv_id": "2512.14158",
        "ARXIVID": "2512.14158",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on backdoor attacks in object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14648": {
        "authors": [
            "Daniel Capell\\'an-Mart\\'in",
            "Abhijeet Parida",
            "Zhifan Jiang",
            "Nishad Kulkarni",
            "Krithika Iyer",
            "Austin Tapp",
            "Syed Muhammad Anwar",
            "Mar\\'ia J. Ledesma-Carbayo",
            "Marius George Linguraru"
        ],
        "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble",
        "abstract": "arXiv:2512.14648v1 Announce Type: new  Abstract: Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.",
        "arxiv_id": "2512.14648",
        "ARXIVID": "2512.14648",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on segmentation pipelines.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.13970": {
        "authors": [
            "Miaohua Zhang",
            "Mohammad Ali Armin",
            "Xuesong Li",
            "Sisi Liang",
            "Lars Petersson",
            "Changming Sun",
            "David Ahmedt-Aristizabal",
            "Zeeshan Hayder"
        ],
        "title": "Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation",
        "abstract": "arXiv:2512.13970v1 Announce Type: new  Abstract: Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.",
        "arxiv_id": "2512.13970",
        "ARXIVID": "2512.13970",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.13716": {
        "authors": [
            "Yitong Luo",
            "Ziang Chen",
            "Hou Hei Lam",
            "Jiayu zhan",
            "Junqi Wang",
            "Zhenliang Zhang",
            "Xue Feng"
        ],
        "title": "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
        "abstract": "arXiv:2512.13716v1 Announce Type: new  Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.",
        "arxiv_id": "2512.13716",
        "ARXIVID": "2512.13716",
        "COMMENT": "Does not directly match any specific criterion but is generally relevant to machine learning due to its focus on value-driven decision-making for personalized AI agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.13950": {
        "authors": [
            "Alban Gauthier",
            "Valentin Deschaintre",
            "Alexandre Lanvin",
            "Fredo Durand",
            "Adrien Bousseau",
            "George Drettakis"
        ],
        "title": "An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes",
        "abstract": "arXiv:2512.13950v1 Announce Type: new  Abstract: Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation",
        "arxiv_id": "2512.13950",
        "ARXIVID": "2512.13950",
        "COMMENT": "Does not match any specific criteria but is related to SVBRDF prediction and appearance modeling, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14489": {
        "authors": [
            "Alessia Micieli",
            "Giovanni Maria Farinella",
            "Francesco Ragusa"
        ],
        "title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition",
        "abstract": "arXiv:2512.14489v1 Announce Type: new  Abstract: In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.",
        "arxiv_id": "2512.14489",
        "ARXIVID": "2512.14489",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.14050": {
        "authors": [
            "Wenjun Liu",
            "Qian Wu",
            "Yifeng Hu",
            "Yuke Li"
        ],
        "title": "SELECT: Detecting Label Errors in Real-world Scene Text Data",
        "abstract": "arXiv:2512.14050v1 Announce Type: new  Abstract: We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.",
        "arxiv_id": "2512.14050",
        "ARXIVID": "2512.14050",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.13979": {
        "authors": [
            "Ge Yan (Lily)",
            "Chung-En Sun (Lily)",
            "Tsui-Wei (Lily)",
            "Weng"
        ],
        "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering",
        "abstract": "arXiv:2512.13979v1 Announce Type: new  Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.",
        "arxiv_id": "2512.13979",
        "ARXIVID": "2512.13979",
        "COMMENT": "Does not directly match any specific criterion but is generally relevant to machine learning due to its focus on controlling LLM reflection via representation engineering.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.14640": {
        "authors": [
            "Rao Muhammad Umer",
            "Daniel Sens",
            "Jonathan Noll",
            "Christian Matek",
            "Lukas Wolfseher",
            "Rainer Spang",
            "Ralf Huss",
            "Johannes Raffler",
            "Sarah Reinke",
            "Wolfram Klapper",
            "Katja Steiger",
            "Kristina Schwamborn",
            "Carsten Marr"
        ],
        "title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images",
        "abstract": "arXiv:2512.14640v1 Announce Type: new  Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.",
        "arxiv_id": "2512.14640",
        "ARXIVID": "2512.14640",
        "COMMENT": "Does not directly match any specific criterion but is generally relevant to computer vision due to its focus on benchmarking pathology models for lymphoma subtyping.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.14360": {
        "authors": [
            "Ankita Raj",
            "Kaashika Prajaapat",
            "Tapan Kumar Gandhi",
            "Chetan Arora"
        ],
        "title": "Mimicking Human Visual Development for Learning Robust Image Representations",
        "abstract": "arXiv:2512.14360v1 Announce Type: new  Abstract: The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.",
        "arxiv_id": "2512.14360",
        "ARXIVID": "2512.14360",
        "COMMENT": "Does not directly match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on improving CNN robustness inspired by human visual development.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
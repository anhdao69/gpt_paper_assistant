{
    "2601.12224": {
        "authors": [
            "Meng Wei",
            "Kun Yuan",
            "Shi Li",
            "Yue Zhou",
            "Long Bai",
            "Nassir Navab",
            "Hongliang Ren",
            "Hong Joo Lee",
            "Tom Vercauteren",
            "Nicolas Padoy"
        ],
        "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion",
        "abstract": "arXiv:2601.12224v1 Announce Type: new  Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.",
        "arxiv_id": "2601.12224",
        "ARXIVID": "2601.12224",
        "COMMENT": "Matches criteria 5 as it integrates motion-based video understanding with language-driven interaction for surgical instrument segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.14056": {
        "authors": [
            "Andrea Rigo",
            "Luca Stornaiuolo",
            "Weijie Wang",
            "Mauro Martino",
            "Bruno Lepri",
            "Nicu Sebe"
        ],
        "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion",
        "abstract": "arXiv:2601.14056v1 Announce Type: new  Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",
        "arxiv_id": "2601.14056",
        "ARXIVID": "2601.14056",
        "COMMENT": "Matches criteria 1 and 5 closely as it introduces a novel method for spatial reasoning and 3D layout control in text-to-image generation, integrating image understanding with generative tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.12312": {
        "authors": [
            "Yongjun Jeon",
            "Jongmin Shin",
            "Kanggil Park",
            "Seonmin Park",
            "Soyoung Lim",
            "Jung Yong Kim",
            "Jinsoo Rhu",
            "Jongman Kim",
            "Gyu-Seong Choi",
            "Namkee Oh",
            "Kyu-Hwan Jung"
        ],
        "title": "CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding",
        "abstract": "arXiv:2601.12312v1 Announce Type: new  Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.",
        "arxiv_id": "2601.12312",
        "ARXIVID": "2601.12312",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a spatio-temporal framework for surgical workflow understanding, addressing hierarchical video tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.12126": {
        "authors": [
            "Guocun Wang",
            "Kenkun Liu",
            "Jing Lin",
            "Guorui Song",
            "Jian Li",
            "Xiaoguang Han"
        ],
        "title": "UniMo: Unified Motion Generation and Understanding with Chain of Thought",
        "abstract": "arXiv:2601.12126v1 Announce Type: new  Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.",
        "arxiv_id": "2601.12126",
        "ARXIVID": "2601.12126",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for motion generation and understanding with interpretable reasoning and reinforcement learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.14250": {
        "authors": [
            "Pengze Zhang",
            "Yanze Wu",
            "Mengtian Li",
            "Xu Bai",
            "Songtao Zhao",
            "Fulong Ye",
            "Chong Mou",
            "Xinghui Li",
            "Zhuowei Chen",
            "Qian He",
            "Mingyuan Gao"
        ],
        "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
        "abstract": "arXiv:2601.14250v1 Announce Type: new  Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
        "arxiv_id": "2601.14250",
        "ARXIVID": "2601.14250",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a unified framework for spatio-temporal video transfer, addressing video generation and transfer tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.13148": {
        "authors": [
            "Richard Shaw",
            "Youngkyoon Jang",
            "Athanasios Papaioannou",
            "Arthur Moreau",
            "Helisa Dhamo",
            "Zhensong Zhang",
            "Eduardo P\\'erez-Pellitero"
        ],
        "title": "ICo3D: An Interactive Conversational 3D Virtual Human",
        "abstract": "arXiv:2601.13148v1 Announce Type: new  Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/",
        "arxiv_id": "2601.13148",
        "ARXIVID": "2601.13148",
        "COMMENT": "Matches criteria 2 as it presents a conversational 3D virtual human integrating vision and language models for real-time interaction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.11990": {
        "authors": [
            "Yiming Li",
            "Chen Cai",
            "Tianyi Liu",
            "Dan Lin",
            "Wenqian Wang",
            "Wenfei Liang",
            "Bingbing Li",
            "Kim-Hui Yap"
        ],
        "title": "DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset",
        "abstract": "arXiv:2601.11990v1 Announce Type: new  Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.",
        "arxiv_id": "2601.11990",
        "ARXIVID": "2601.11990",
        "COMMENT": "Matches criteria 6 as it introduces a new dataset and method for video-based driver action recognition, focusing on multi-modal and multi-view data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.12051": {
        "authors": [
            "Weixin Ye",
            "Wei Wang",
            "Yahui Liu",
            "Yue Song",
            "Bin Ren",
            "Wei Bi",
            "Rita Cucchiara",
            "Nicu Sebe"
        ],
        "title": "A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models",
        "abstract": "arXiv:2601.12051v1 Announce Type: new  Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \\textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack",
        "arxiv_id": "2601.12051",
        "ARXIVID": "2601.12051",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a unified framework for vision and language tasks using a masked jigsaw puzzle approach.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.11930": {
        "authors": [
            "Xulei Shi",
            "Maoyu Wang",
            "Yuning Peng",
            "Guanbo Wang",
            "Xin Wang",
            "Qi Chen",
            "Pengjie Tao"
        ],
        "title": "SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM",
        "abstract": "arXiv:2601.11930v1 Announce Type: new  Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.",
        "arxiv_id": "2601.11930",
        "ARXIVID": "2601.11930",
        "COMMENT": "Matches criteria 4 as it introduces a novel global descriptor for image retrieval in SfM, leveraging visual foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.12729": {
        "authors": [
            "Hanyu Zhu",
            "Zhihao Zhan",
            "Yuhang Ming",
            "Liang Li",
            "Dibo Hou",
            "Javier Civera",
            "Wanzeng Kong"
        ],
        "title": "DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition",
        "abstract": "arXiv:2601.12729v1 Announce Type: new  Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.",
        "arxiv_id": "2601.12729",
        "ARXIVID": "2601.12729",
        "COMMENT": "Matches criteria 4 as it focuses on visual foundation models and their application to visual place recognition, introducing a novel aggregation method.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.14037": {
        "authors": [
            "Kumar Ashutosh",
            "XuDong Wang",
            "Xi Yin",
            "Kristen Grauman",
            "Adam Polyak",
            "Ishan Misra",
            "Rohit Girdhar"
        ],
        "title": "Human detectors are surprisingly powerful reward models",
        "abstract": "arXiv:2601.14037v1 Announce Type: new  Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.",
        "arxiv_id": "2601.14037",
        "ARXIVID": "2601.14037",
        "COMMENT": "Matches criteria 6 as it focuses on improving video generation quality, particularly for human motion, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.14084": {
        "authors": [
            "Abdurrahim Yilmaz",
            "Ozan Erdem",
            "Ece Gokyayla",
            "Ayda Acar",
            "Burc Bugra Dagtas",
            "Dilara Ilhan Erdil",
            "Gulsum Gencoglan",
            "Burak Temelkuran"
        ],
        "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
        "abstract": "arXiv:2601.14084v1 Announce Type: new  Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
        "arxiv_id": "2601.14084",
        "ARXIVID": "2601.14084",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for evaluating vision-language models in dermatology, focusing on VQA tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.13126": {
        "authors": [
            "Mattia D'Urso",
            "Emanuele Santellani",
            "Christian Sormann",
            "Mattia Rossi",
            "Andreas Kuhn",
            "Friedrich Fraundorfer"
        ],
        "title": "A Streamlined Attention-Based Network for Descriptor Extraction",
        "abstract": "arXiv:2601.13126v1 Announce Type: new  Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.   Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.   Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.   As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.",
        "arxiv_id": "2601.13126",
        "ARXIVID": "2601.13126",
        "COMMENT": "Matches criteria 4 as it introduces a novel descriptor extraction method for keypoint matching, improving on existing architectures.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.12382": {
        "authors": [
            "Furkan Yuceyalcin",
            "Abdurrahim Yilmaz",
            "Burak Temelkuran"
        ],
        "title": "A Hierarchical Benchmark of Foundation Models for Dermatology",
        "abstract": "arXiv:2601.12382v1 Announce Type: new  Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a \"granularity gap\" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.",
        "arxiv_id": "2601.12382",
        "ARXIVID": "2601.12382",
        "COMMENT": "Matches criteria 4 as it evaluates foundation models in medical imaging, focusing on their hierarchical performance in dermatology.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.12155": {
        "authors": [
            "Xiang Gao",
            "Xinmu Wang",
            "Yuanpeng Liu",
            "Yue Wang",
            "Junqi Huang",
            "Wei Chen",
            "Xianfeng Gu"
        ],
        "title": "Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors",
        "abstract": "arXiv:2601.12155v1 Announce Type: new  Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.",
        "arxiv_id": "2601.12155",
        "ARXIVID": "2601.12155",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D surface mesh reconstruction with topological priors, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.14055": {
        "authors": [
            "Andrea Protani",
            "Marc Molina Van Den Bosch",
            "Lorenzo Giusti",
            "Heloisa Barbosa Da Silva",
            "Paolo Cacace",
            "Albert Sund Aillet",
            "Miguel Angel Gonzalez Ballester",
            "Friedhelm Hummel",
            "Luigi Serio"
        ],
        "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI",
        "abstract": "arXiv:2601.14055v1 Announce Type: new  Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.",
        "arxiv_id": "2601.14055",
        "ARXIVID": "2601.14055",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.13969": {
        "authors": [
            "Joaqu\\'in Polonuer (Department of Biomedical Informatics",
            "Harvard Medical School",
            "Boston",
            "MA",
            "USA",
            "Departamento de Computaci\\'on",
            "FCEyN",
            "Universidad de Buenos Aires",
            "Buenos Aires",
            "Argentina)",
            "Lucas Vittor (Department of Biomedical Informatics",
            "Harvard Medical School",
            "Boston",
            "MA",
            "USA)",
            "I\\~naki Arango (Department of Biomedical Informatics",
            "Harvard Medical School",
            "Boston",
            "MA",
            "USA)",
            "Ayush Noori (Department of Biomedical Informatics",
            "Harvard Medical School",
            "Boston",
            "MA",
            "USA",
            "Department of Engineering Science",
            "University of Oxford",
            "Oxford",
            "UK)",
            "David A. Clifton (Department of Engineering Science",
            "University of Oxford",
            "Oxford",
            "UK",
            "Oxford Suzhou Centre for Advanced Research",
            "University of Oxford",
            "Suzhou",
            "Jiangsu",
            "China)",
            "Luciano Del Corro (ELIAS Lab",
            "Departamento de Ingenier\\'ia",
            "Universidad de San Andr\\'es",
            "Victoria",
            "Argentina",
            "Lumina Labs",
            "Buenos Aires",
            "Argentina)",
            "Marinka Zitnik (Department of Biomedical Informatics",
            "Harvard Medical School",
            "Boston",
            "MA",
            "USA",
            "Kempner Institute for the Study of Natural and Artificial Intelligence",
            "Allston",
            "MA",
            "USA",
            "Broad Institute of MIT and Harvard",
            "Cambridge",
            "MA",
            "USA",
            "Harvard Data Science Initiative",
            "Cambridge",
            "MA",
            "USA)"
        ],
        "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
        "abstract": "arXiv:2601.13969v1 Announce Type: new  Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
        "arxiv_id": "2601.13969",
        "ARXIVID": "2601.13969",
        "COMMENT": "Does not match any specific criteria. Focuses on knowledge graph exploration and retrieval, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.11779": {
        "authors": [
            "Vinicius F. Arruda",
            "Rodrigo F. Berriel",
            "Thiago M. Paix\\~ao",
            "Claudine Badue",
            "Alberto F. De Souza",
            "Nicu Sebe",
            "Thiago Oliveira-Santos"
        ],
        "title": "Cross-Domain Object Detection Using Unsupervised Image Translation",
        "abstract": "arXiv:2601.11779v1 Announce Type: new  Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.",
        "arxiv_id": "2601.11779",
        "ARXIVID": "2601.11779",
        "COMMENT": "Does not match any specific criteria. Focuses on cross-domain object detection using unsupervised image translation, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
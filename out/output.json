{
    "2602.06566": {
        "authors": [
            "Niccolo Avogaro",
            "Nayanika Debnath",
            "Li Mi",
            "Thomas Frick",
            "Junling Wang",
            "Zexue He",
            "Hang Hua",
            "Konrad Schindler",
            "Mattia Rigotti"
        ],
        "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
        "abstract": "arXiv:2602.06566v1 Announce Type: new  Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200$\\times$ lower token budget.",
        "arxiv_id": "2602.06566",
        "ARXIVID": "2602.06566",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it introduces a modular framework for vision-language models with test-time scaling.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.06871": {
        "authors": [
            "Mohammadreza Salehi",
            "Mehdi Noroozi",
            "Luca Morreale",
            "Ruchika Chavhan",
            "Malcolm Chadwick",
            "Alberto Gil Ramos",
            "Abhinav Mehrotra"
        ],
        "title": "RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing",
        "abstract": "arXiv:2602.06871v1 Announce Type: new  Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/",
        "arxiv_id": "2602.06871",
        "ARXIVID": "2602.06871",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it introduces a causal video editing model for instructional video editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.06333": {
        "authors": [
            "Gensheng Pei",
            "Xiruo Jiang",
            "Yazhou Yao",
            "Xiangbo Shu",
            "Fumin Shen",
            "Byeungwoo Jeon"
        ],
        "title": "Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation",
        "abstract": "arXiv:2602.06333v1 Announce Type: new  Abstract: The recent introduction of \\texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \\textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\\textit{data drift}) or conditional label distributions evolve (\\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \\textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\\textit{ii}) mines representative supports to suppress outliers under data drift, and (\\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \\textsc{ConceptBank} effectively adapts \\texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.",
        "arxiv_id": "2602.06333",
        "ARXIVID": "2602.06333",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it focuses on adapting SAM3 for open-vocabulary segmentation under distribution drifts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.06442": {
        "authors": [
            "Wenxun Dai",
            "Zhiyuan Zhao",
            "Yule Zhong",
            "Yiji Cheng",
            "Jianwei Zhang",
            "Linqing Wang",
            "Shiyi Zhang",
            "Yunlong Lin",
            "Runze He",
            "Fellix Song",
            "Wayne Zhuang",
            "Yong Liu",
            "Haoji Zhang",
            "Yansong Tang",
            "Qinglin Lu",
            "Chunyu Wang"
        ],
        "title": "ChatUMM: Robust Context Tracking for Conversational Interleaved Generation",
        "abstract": "arXiv:2602.06442v1 Announce Type: new  Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.",
        "arxiv_id": "2602.06442",
        "ARXIVID": "2602.06442",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it focuses on a unified multimodal model for conversational interleaved generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.06159": {
        "authors": [
            "Xuyang Chen",
            "Conglang Zhang",
            "Chuanheng Fu",
            "Zihao Yang",
            "Kaixuan Zhou",
            "Yizhi Zhang",
            "Jianan He",
            "Yanfeng Zhang",
            "Mingwei Sun",
            "Zengmao Wang",
            "Zhen Dong",
            "Xiaoxiao Long",
            "Liqiu Meng"
        ],
        "title": "Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving",
        "abstract": "arXiv:2602.06159v1 Announce Type: new  Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by \"baking in\" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for \"texture baking,\" while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/",
        "arxiv_id": "2602.06159",
        "ARXIVID": "2602.06159",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it focuses on leveraging Vision Foundation Module (VFM) features for sim-to-real generation in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.06335": {
        "authors": [
            "Yihan Shang",
            "Wei Wang",
            "Chao Huang",
            "Xinghui Dong"
        ],
        "title": "SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation",
        "abstract": "arXiv:2602.06335v1 Announce Type: new  Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.",
        "arxiv_id": "2602.06335",
        "ARXIVID": "2602.06335",
        "COMMENT": "Matches criterion 1. Proposes a depth-aware and self-prompted SAM for instance segmentation, improving spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.06959": {
        "authors": [
            "Kaiyi Huang",
            "Yukun Huang",
            "Yu Li",
            "Jianhong Bai",
            "Xintao Wang",
            "Zinan Lin",
            "Xuefei Ning",
            "Jiwen Yu",
            "Pengfei Wan",
            "Yu Wang",
            "Xihui Liu"
        ],
        "title": "CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation",
        "abstract": "arXiv:2602.06959v1 Announce Type: new  Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.",
        "arxiv_id": "2602.06959",
        "ARXIVID": "2602.06959",
        "COMMENT": "Matches criterion 6. Proposes a framework for cinematic video generation with implicit 3D scene representation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.06850": {
        "authors": [
            "Chao Zhou",
            "Tianyi Wei",
            "Yiling Chen",
            "Wenbo Zhou",
            "Nenghai Yu"
        ],
        "title": "Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping",
        "abstract": "arXiv:2602.06850v1 Announce Type: new  Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\\times$ inference speedup and a 5.1$\\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.",
        "arxiv_id": "2602.06850",
        "ARXIVID": "2602.06850",
        "COMMENT": "Matches criterion 5. Proposes efficient multi-condition control for text-to-image generation, integrating spatial and semantic alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.06402": {
        "authors": [
            "Wenjie Wang",
            "Wei Wu",
            "Ying Liu",
            "Yuan Zhao",
            "Xiaole Lv",
            "Liang Diao",
            "Zengjian Fan",
            "Wenfeng Xie",
            "Ziling Lin",
            "De Shi",
            "Lin Huang",
            "Kaihe Xu",
            "Hong Li"
        ],
        "title": "MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing",
        "abstract": "arXiv:2602.06402v1 Announce Type: new  Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.",
        "arxiv_id": "2602.06402",
        "ARXIVID": "2602.06402",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it introduces a vision-language model for medical document parsing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.06343": {
        "authors": [
            "Weiquan Wang",
            "Feifei Shao",
            "Lin Li",
            "Zhen Wang",
            "Jun Xiao",
            "Long Chen"
        ],
        "title": "Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering",
        "abstract": "arXiv:2602.06343v1 Announce Type: new  Abstract: High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.",
        "arxiv_id": "2602.06343",
        "ARXIVID": "2602.06343",
        "COMMENT": "Matches criterion 6. Proposes a method for rendering dynamic humans in videos with uncertainty-aware 4D Gaussian splatting.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.06474": {
        "authors": [
            "Xu Zhang",
            "Zhe Chen",
            "Jing Zhang",
            "Dacheng Tao"
        ],
        "title": "LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection",
        "abstract": "arXiv:2602.06474v1 Announce Type: new  Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.",
        "arxiv_id": "2602.06474",
        "ARXIVID": "2602.06474",
        "COMMENT": "Matches criterion 3. Introduces a training-free one-shot domain generalization method for object detection, addressing domain adaptation challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.06855": {
        "authors": [
            "Alisia Lupidi",
            "Bhavul Gauri",
            "Thomas Simon Foster",
            "Bassel Al Omari",
            "Despoina Magka",
            "Alberto Pepe",
            "Alexis Audran-Reiss",
            "Muna Aghamelu",
            "Nicolas Baldwin",
            "Lucia Cipolina-Kun",
            "Jean-Christophe Gagnon-Audet",
            "Chee Hau Leow",
            "Sandra Lefdal",
            "Hossam Mossalam",
            "Abhinav Moudgil",
            "Saba Nazir",
            "Emanuel Tewolde",
            "Isabel Urrego",
            "Jordi Armengol Estape",
            "Amar Budhiraja",
            "Gaurav Chaurasia",
            "Abhishek Charnalia",
            "Derek Dunfield",
            "Karen Hambardzumyan",
            "Daniel Izcovich",
            "Martin Josifoski",
            "Ishita Mediratta",
            "Kelvin Niu",
            "Parth Pathak",
            "Michael Shvartsman",
            "Edan Toledo",
            "Anton Protopopov",
            "Roberta Raileanu",
            "Alexander Miller",
            "Tatiana Shavrina",
            "Jakob Foerster",
            "Yoram Bachrach"
        ],
        "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
        "abstract": "arXiv:2602.06855v1 Announce Type: new  Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
        "arxiv_id": "2602.06855",
        "ARXIVID": "2602.06855",
        "COMMENT": "Matches criterion 3. Introduces a benchmark suite for AI research agents, focusing on diverse tasks and agentic capabilities.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.06184": {
        "authors": [
            "Cheng Liang",
            "Chaoyi Wu",
            "Weike Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "title": "PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining",
        "abstract": "arXiv:2602.06184v1 Announce Type: new  Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.",
        "arxiv_id": "2602.06184",
        "ARXIVID": "2602.06184",
        "COMMENT": "Matches criterion 2. Focuses on integrating phenotype ontology knowledge into medical vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.06478": {
        "authors": [
            "Xiaosong Jia",
            "Yihang Sun",
            "Junqi You",
            "Songbur Wong",
            "Zichen Zou",
            "Junchi Yan",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention",
        "abstract": "arXiv:2602.06478v1 Announce Type: new  Abstract: Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number of input views and rigid parameter sharing among heterogeneous tokens. We propose Efficient-LVSM, a dual-stream architecture that avoids these issues with a decoupled co-refinement mechanism. It applies intra-view self-attention for input views and self-then-cross attention for target views, eliminating unnecessary computation. Efficient-LVSM achieves 29.86 dB PSNR on RealEstate10K with 2 input views, surpassing LVSM by 0.2 dB, with 2x faster training convergence and 4.4x faster inference speed. Efficient-LVSM achieves state-of-the-art performance on multiple benchmarks, exhibits strong zero-shot generalization to unseen view counts, and enables incremental inference with KV-cache, thanks to its decoupled designs.",
        "arxiv_id": "2602.06478",
        "ARXIVID": "2602.06478",
        "COMMENT": "This paper aligns with criterion 4 as it focuses on a novel architecture for vision foundation models, specifically for large view synthesis, and demonstrates improvements in efficiency and performance.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.06203": {
        "authors": [
            "Parv Maheshwari",
            "Jay Karhade",
            "Yogesh Chawla",
            "Isaiah Adu",
            "Florian Heisen",
            "Andrew Porco",
            "Andrew Jong",
            "Yifei Liu",
            "Santosh Pitla",
            "Sebastian Scherer",
            "Wenshan Wang"
        ],
        "title": "AnyThermal: Towards Learning Universal Representations for Thermal Perception",
        "abstract": "arXiv:2602.06203v1 Announce Type: new  Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.",
        "arxiv_id": "2602.06203",
        "ARXIVID": "2602.06203",
        "COMMENT": "Matches criterion 4. Focuses on a thermal vision foundation model and its applications across diverse tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.06346": {
        "authors": [
            "Tianyi Zhang",
            "Chengcheng Liu",
            "Jinwei Chen",
            "Chun-Le Guo",
            "Chongyi Li",
            "Ming-Ming Cheng",
            "Bo Li",
            "Peng-Tao Jiang"
        ],
        "title": "FlowConsist: Make Your Flow Consistent with Real Trajectory",
        "abstract": "arXiv:2602.06346v1 Announce Type: new  Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\\times$256, achieving an FID of 1.52 with only 1 sampling step.",
        "arxiv_id": "2602.06346",
        "ARXIVID": "2602.06346",
        "COMMENT": "Does not closely match any specific criteria but discusses fast flow models for image generation, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.06548": {
        "authors": [
            "Mingxi Xu",
            "Qi Wang",
            "Zhengyu Wen",
            "Phong Dao Thien",
            "Zhengyu Li",
            "Ning Zhang",
            "Xiaoyu He",
            "Wei Zhao",
            "Kehong Gong",
            "Mingyuan Zhang"
        ],
        "title": "NECromancer: Breathing Life into Skeletons via BVH Animation",
        "abstract": "arXiv:2602.06548v1 Announce Type: new  Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/",
        "arxiv_id": "2602.06548",
        "ARXIVID": "2602.06548",
        "COMMENT": "Does not closely match any specific criteria but discusses motion tokenization for diverse skeletons, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.06375": {
        "authors": [
            "Yu Zhao",
            "Fan Jiang",
            "Tianle Liu",
            "Bo Zeng",
            "Yu Liu",
            "Longyue Wang",
            "Weihua Luo"
        ],
        "title": "Difficulty-Estimated Policy Optimization",
        "abstract": "arXiv:2602.06375v1 Announce Type: new  Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.",
        "arxiv_id": "2602.06375",
        "ARXIVID": "2602.06375",
        "COMMENT": "This paper does not directly match any of the specific criteria. It focuses on optimizing reasoning models, which is tangentially related to embodied AI but does not introduce new benchmarks, methods, or spatial reasoning improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.06450": {
        "authors": [
            "Xingsong Ye",
            "Yongkun Du",
            "JiaXin Zhang",
            "Chen Li",
            "Jing LYU",
            "Zhineng Chen"
        ],
        "title": "What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution",
        "abstract": "arXiv:2602.06450v1 Announce Type: new  Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.",
        "arxiv_id": "2602.06450",
        "ARXIVID": "2602.06450",
        "COMMENT": "Does not closely match any specific criteria but discusses synthetic data generation for scene text recognition, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.06743": {
        "authors": [
            "Dong Chen",
            "Zizhuang Wei",
            "Jialei Xu",
            "Xinyang Sun",
            "Zonglin He",
            "Meiru An",
            "Huili Peng",
            "Yong Hu",
            "Kenneth MC Cheung"
        ],
        "title": "Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening",
        "abstract": "arXiv:2602.06743v1 Announce Type: new  Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.",
        "arxiv_id": "2602.06743",
        "ARXIVID": "2602.06743",
        "COMMENT": "Does not closely match any specific criteria but is related to multi-modal learning in a medical application.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.06503": {
        "authors": [
            "Yongkang Lai",
            "Xihan Mu",
            "Tim R. McVicar",
            "Dasheng Fan",
            "Donghui Xie",
            "Shanxin Guo",
            "Wenli Huang",
            "Tianjie Zhao",
            "Guangjian Yan"
        ],
        "title": "Forest canopy height estimation from satellite RGB imagery using large-scale airborne LiDAR-derived training data and monocular depth estimation",
        "abstract": "arXiv:2602.06503v1 Announce Type: new  Abstract: Large-scale, high-resolution forest canopy height mapping plays a crucial role in understanding regional and global carbon and water cycles. Spaceborne LiDAR missions, including the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) and the Global Ecosystem Dynamics Investigation (GEDI), provide global observations of forest structure but are spatially sparse and subject to inherent uncertainties. In contrast, near-surface LiDAR platforms, such as airborne and unmanned aerial vehicle (UAV) LiDAR systems, offer much finer measurements of forest canopy structure, and a growing number of countries have made these datasets openly available. In this study, a state-of-the-art monocular depth estimation model, Depth Anything V2, was trained using approximately 16,000 km2 of canopy height models (CHMs) derived from publicly available airborne LiDAR point clouds and related products across multiple countries, together with 3 m resolution PlanetScope and airborne RGB imagery. The trained model, referred to as Depth2CHM, enables the estimation of spatially continuous CHMs directly from PlanetScope RGB imagery. Independent validation was conducted at sites in China (approximately 1 km2) and the United States (approximately 116 km2). The results showed that Depth2CHM could accurately estimate canopy height, with biases of 0.59 m and 0.41 m and root mean square errors (RMSEs) of 2.54 m and 5.75 m for these two sites, respectively. Compared with an existing global meter-resolution CHM product, the mean absolute error is reduced by approximately 1.5 m and the RMSE by approximately 2 m. These results demonstrated that monocular depth estimation networks trained with large-scale airborne LiDAR-derived canopy height data provide a promising and scalable pathway for high-resolution, spatially continuous forest canopy height estimation from satellite RGB imagery.",
        "arxiv_id": "2602.06503",
        "ARXIVID": "2602.06503",
        "COMMENT": "Does not match any specific criteria. Focuses on forest canopy height estimation using monocular depth estimation and LiDAR data.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
{
    "2512.02009": {
        "authors": [
            "Xian Ge",
            "Yuling Pan",
            "Yuhang Zhang",
            "Xiang Li",
            "Weijun Zhang",
            "Dizhe Zhang",
            "Zhaoliang Wan",
            "Xin Lin",
            "Xiangkai Zhang",
            "Juntao Liang",
            "Jason Li",
            "Wenjie Jiang",
            "Bo Du",
            "Ming-Hsuan Yang",
            "Lu Qi"
        ],
        "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
        "abstract": "arXiv:2512.02009v1 Announce Type: new  Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
        "arxiv_id": "2512.02009",
        "ARXIVID": "2512.02009",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark and simulation platform (AirSim360) for embodied AI, focusing on spatial intelligence and omnidirectional understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.00534": {
        "authors": [
            "Zeyuan An",
            "Yanghang Xiao",
            "Zhiying Leng",
            "Frederick W. B. Li",
            "Xiaohui Liang"
        ],
        "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
        "abstract": "arXiv:2512.00534v1 Announce Type: new  Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
        "arxiv_id": "2512.00534",
        "ARXIVID": "2512.00534",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on spatial intelligence and embodied agents with novel methods for cross-temporal 3D scene updates.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.00547": {
        "authors": [
            "Sandika Biswas",
            "Qianyi Wu",
            "Biplab Banerjee",
            "Hamid Rezatofighi"
        ],
        "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions",
        "abstract": "arXiv:2512.00547v1 Announce Type: new  Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.",
        "arxiv_id": "2512.00547",
        "ARXIVID": "2512.00547",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on spatial intelligence and embodied agents with novel methods for dynamic scene reconstruction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.01636": {
        "authors": [
            "Xin Wang",
            "Haipeng Zhang",
            "Mang Li",
            "Zhaohui Xia",
            "Yueguo Chen",
            "Yu Zhang",
            "Chunyu Wei"
        ],
        "title": "Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval",
        "abstract": "arXiv:2512.01636v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.",
        "arxiv_id": "2512.01636",
        "ARXIVID": "2512.01636",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel generative editing framework in the joint vision-language space for zero-shot composed image retrieval.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.01821": {
        "authors": [
            "Meng Cao",
            "Haokun Lin",
            "Haoyuan Li",
            "Haoran Tang",
            "Rongtao Xu",
            "Dong An",
            "Xue Liu",
            "Ian Reid",
            "Xiaodan Liang"
        ],
        "title": "Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling",
        "abstract": "arXiv:2512.01821v1 Announce Type: new  Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.",
        "arxiv_id": "2512.01821",
        "ARXIVID": "2512.01821",
        "COMMENT": "Matches criteria 2 as it introduces a novel approach to spatial reasoning in Multimodal Large Language Models (MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.00743": {
        "authors": [
            "Qiang Lyu",
            "Zicong Chen",
            "Chongxiao Wang",
            "Haolin Shi",
            "Shibo Gao",
            "Ran Piao",
            "Youwei Zeng",
            "Jianlou Si",
            "Fei Ding",
            "Jing Li",
            "Chun Pong Lau",
            "Weiqiang Wang"
        ],
        "title": "Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards",
        "abstract": "arXiv:2512.00743v1 Announce Type: new  Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \\textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \\textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \\textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \\emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \\emph{reward-based grouping} to compute advantages for each reward function \\textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \\textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \\textit{PickScore-25k} and multi-objective \\textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \\href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.",
        "arxiv_id": "2512.00743",
        "ARXIVID": "2512.00743",
        "COMMENT": "Matches criteria 5 as it focuses on text-to-image generation and multi-objective optimization, integrating image understanding with language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00475": {
        "authors": [
            "Xin Gu",
            "Congcong Li",
            "Xinyao Wang",
            "Dexiang Hong",
            "Libo Zhang",
            "Tiejian Luo",
            "Longyin Wen",
            "Heng Fan"
        ],
        "title": "Structured Context Learning for Generic Event Boundary Detection",
        "abstract": "arXiv:2512.00475v1 Announce Type: new  Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.",
        "arxiv_id": "2512.00475",
        "ARXIVID": "2512.00475",
        "COMMENT": "Matches criteria 6 as it introduces a novel method for event boundary detection in videos, pushing the boundaries of video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01242": {
        "authors": [
            "Zirui Zhao",
            "Boye Niu",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "title": "Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation",
        "abstract": "arXiv:2512.01242v1 Announce Type: new  Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.",
        "arxiv_id": "2512.01242",
        "ARXIVID": "2512.01242",
        "COMMENT": "Matches criteria 1 as it focuses on spatial reasoning and abstract visual composition generation, which involves spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01352": {
        "authors": [
            "In-Jae Lee",
            "Mungyeom Kim",
            "Kwonyoung Ryu",
            "Pierre Musacchio",
            "Jaesik Park"
        ],
        "title": "OpenBox: Annotate Any Bounding Boxes in 3D",
        "abstract": "arXiv:2512.01352v1 Announce Type: new  Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.",
        "arxiv_id": "2512.01352",
        "ARXIVID": "2512.01352",
        "COMMENT": "Matches criteria 3 as it introduces a novel pipeline for unsupervised and open-vocabulary 3D object detection, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00305": {
        "authors": [
            "Zhengzhuo Xu",
            "SiNan Du",
            "Yiyan Qi",
            "SiwenLu",
            "Chengjin Xu",
            "Chun Yuan",
            "Jian Guo"
        ],
        "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
        "abstract": "arXiv:2512.00305v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\\% on ChartBench.",
        "arxiv_id": "2512.00305",
        "ARXIVID": "2512.00305",
        "COMMENT": "Matches criteria 2 as it focuses on improving multimodal large language models (MLLMs) for chart reasoning, which involves vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.01427": {
        "authors": [
            "Klara Reichard",
            "Nikolas Brasch",
            "Nassir Navab",
            "Federico Tombari"
        ],
        "title": "Language-Guided Open-World Anomaly Segmentation",
        "abstract": "arXiv:2512.01427v1 Announce Type: new  Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.",
        "arxiv_id": "2512.01427",
        "ARXIVID": "2512.01427",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel method for spatial reasoning and anomaly segmentation in autonomous driving, which is relevant to embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.00807": {
        "authors": [
            "Yujie Lin",
            "Jiayao Ma",
            "Qingguo Hu",
            "Derek F. Wong",
            "Jinsong Su"
        ],
        "title": "BioPro: On Difference-Aware Gender Fairness for Vision-Language Models",
        "abstract": "arXiv:2512.00807v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.",
        "arxiv_id": "2512.00807",
        "ARXIVID": "2512.00807",
        "COMMENT": "Matches criteria 2 as it focuses on fairness in Vision-Language Models (VLMs), which is a subtopic of multimodal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.00911": {
        "authors": [
            "Yuhao Shan",
            "Qianyi Yuan",
            "Jingguo Liu",
            "Shigang Li",
            "Jianfeng Li",
            "Tong Chen"
        ],
        "title": "Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision",
        "abstract": "arXiv:2512.00911v1 Announce Type: new  Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360{\\deg} continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.",
        "arxiv_id": "2512.00911",
        "ARXIVID": "2512.00911",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for upright panorama generation in robotic vision, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.00756": {
        "authors": [
            "Ruihan Chen",
            "Qiming Li",
            "Xiaocheng Feng",
            "Xiaoliang Yang",
            "Weihong Zhong",
            "Yuxuan Gu",
            "Zekun Zhou",
            "Bing Qin"
        ],
        "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents",
        "abstract": "arXiv:2512.00756v1 Announce Type: new  Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.",
        "arxiv_id": "2512.00756",
        "ARXIVID": "2512.00756",
        "COMMENT": "Matches criteria 2 as it benchmarks and enhances multilingual perception and reasoning in large vision-language models (LVLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.02015": {
        "authors": [
            "Yao-Chih Lee",
            "Zhoutong Zhang",
            "Jiahui Huang",
            "Jui-Hsien Wang",
            "Joon-Young Lee",
            "Jia-Bin Huang",
            "Eli Shechtman",
            "Zhengqi Li"
        ],
        "title": "Generative Video Motion Editing with 3D Point Tracks",
        "abstract": "arXiv:2512.02015v1 Announce Type: new  Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
        "arxiv_id": "2512.02015",
        "ARXIVID": "2512.02015",
        "COMMENT": "This paper aligns closely with criterion 5 as it showcases techniques combining video understanding tasks (motion editing) with generative modeling, which could be extended to integration with large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.00408": {
        "authors": [
            "Lingdong Wang",
            "Guan-Ming Su",
            "Divya Kothandaraman",
            "Tsung-Wei Huang",
            "Mohammad Hajiesmaili",
            "Ramesh K. Sitaraman"
        ],
        "title": "Low-Bitrate Video Compression through Semantic-Conditioned Diffusion",
        "abstract": "arXiv:2512.00408v1 Announce Type: new  Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.",
        "arxiv_id": "2512.00408",
        "ARXIVID": "2512.00408",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through semantic-conditioned diffusion for video compression.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.00557": {
        "authors": [
            "Haomiao Chen",
            "Keith W Jamison",
            "Mert R. Sabuncu",
            "Amy Kuceyeski"
        ],
        "title": "NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives",
        "abstract": "arXiv:2512.00557v1 Announce Type: new  Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.",
        "arxiv_id": "2512.00557",
        "ARXIVID": "2512.00557",
        "COMMENT": "Matches criterion 2 as it explores vision-language integration using a generative framework for brain-guided image synthesis.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.00762": {
        "authors": [
            "Zhiyuan Gao",
            "Jiageng Mao",
            "Hong-Xing Yu",
            "Haozhe Lou",
            "Emily Yue-Ting Jia",
            "Jernej Barbic",
            "Jiajun Wu",
            "Yue Wang"
        ],
        "title": "Seeing the Wind from a Falling Leaf",
        "abstract": "arXiv:2512.00762v1 Announce Type: new  Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \\href{https://chaoren2357.github.io/seeingthewind/}{project page}.",
        "arxiv_id": "2512.00762",
        "ARXIVID": "2512.00762",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically modeling motions and inferring physical interactions from videos.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.01850": {
        "authors": [
            "Yue Pan",
            "Tao Sun",
            "Liyuan Zhu",
            "Lucas Nunes",
            "Iro Armeni",
            "Jens Behley",
            "Cyrill Stachniss"
        ],
        "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching",
        "abstract": "arXiv:2512.01850v1 Announce Type: new  Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.",
        "arxiv_id": "2512.01850",
        "ARXIVID": "2512.01850",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for 3D point cloud registration, relevant to robotic AI and SLAM tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.01204": {
        "authors": [
            "Ziqian Wang",
            "Yonghao He",
            "Licheng Yang",
            "Wei Zou",
            "Hongxuan Ma",
            "Liu Liu",
            "Wei Sui",
            "Yuxin Guo",
            "Hu Su"
        ],
        "title": "TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image",
        "abstract": "arXiv:2512.01204v1 Announce Type: new  Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.",
        "arxiv_id": "2512.01204",
        "ARXIVID": "2512.01204",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for generating 3D tabletop scenes, which is relevant to embodied AI and robotic manipulation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.01340": {
        "authors": [
            "Yingjie Zhou",
            "Xilei Zhu",
            "Siyu Ren",
            "Ziyi Zhao",
            "Ziwen Wang",
            "Farong Wen",
            "Yu Zhou",
            "Jiezhang Cao",
            "Xiongkuo Min",
            "Fengjiao Chen",
            "Xiaoyu Li",
            "Xuezhi Cao",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans",
        "abstract": "arXiv:2512.01340v1 Announce Type: new  Abstract: Speech-driven Talking Human (TH) generation, commonly known as \"Talker,\" currently faces limitations in multi-subject driving capabilities. Extending this paradigm to \"Multi-Talker,\" capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.",
        "arxiv_id": "2512.01340",
        "ARXIVID": "2512.01340",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically evaluating multi-subject talking humans.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00343": {
        "authors": [
            "Zhongqi Wang",
            "Jie Zhang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models",
        "abstract": "arXiv:2512.00343v1 Announce Type: new  Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET",
        "arxiv_id": "2512.00343",
        "ARXIVID": "2512.00343",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language pretrained models and their vulnerabilities, specifically backdoor detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00834": {
        "authors": [
            "Lin Zhu",
            "Kezhi Wang",
            "Luping Xiang",
            "Kun Yang"
        ],
        "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks",
        "abstract": "arXiv:2512.00834v1 Announce Type: new  Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.",
        "arxiv_id": "2512.00834",
        "ARXIVID": "2512.00834",
        "COMMENT": "Matches criterion 3 as it presents a trajectory prediction framework integrating semantic reasoning, relevant to embodied AI in vehicular networks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00818": {
        "authors": [
            "Haozhen Gong",
            "Xiaozhong Ji",
            "Yuansen Liu",
            "Wenbin Wu",
            "Xiaoxiao Yan",
            "Jingjing Liu",
            "Kai Wu",
            "Jiazhen Pan",
            "Bailiang Jian",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Hongwei Bran Li"
        ],
        "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning",
        "abstract": "arXiv:2512.00818v1 Announce Type: new  Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.",
        "arxiv_id": "2512.00818",
        "ARXIVID": "2512.00818",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for multimodal reasoning in medical contexts, which involves MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.00365": {
        "authors": [
            "Andrey Gizdov",
            "Andrea Procopio",
            "Yichen Li",
            "Daniel Harari",
            "Tomer Ullman"
        ],
        "title": "Towards aligned body representations in vision models",
        "abstract": "arXiv:2512.00365v1 Announce Type: new  Abstract: Human physical reasoning relies on internal \"body\" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.",
        "arxiv_id": "2512.00365",
        "ARXIVID": "2512.00365",
        "COMMENT": "Matches criterion 1 as it explores spatial reasoning and body representations in vision models, which are relevant to embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.01153": {
        "authors": [
            "Han-Jin Lee",
            "Han-Ju Lee",
            "Jin-Seong Kim",
            "Seok-Hwan Choi"
        ],
        "title": "DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling",
        "abstract": "arXiv:2512.01153v1 Announce Type: new  Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fr\\'echet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O({\\Delta}t) leading error term in the Wasserstein distance, achieving an O({\\Delta}t^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.",
        "arxiv_id": "2512.01153",
        "ARXIVID": "2512.01153",
        "COMMENT": "Does not match any specific criterion but is related to diffusion models and adversarial control.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00308": {
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Wengang Zhou",
            "Hongsheng Li",
            "Houqiang Li"
        ],
        "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation",
        "abstract": "arXiv:2512.00308v1 Announce Type: new  Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.",
        "arxiv_id": "2512.00308",
        "ARXIVID": "2512.00308",
        "COMMENT": "Does not match any specific criterion but is related to dataset distillation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00601": {
        "authors": [
            "Boyang Gu",
            "Hongjian Zhou",
            "Bradley Max Segal",
            "Jinge Wu",
            "Zeyu Cao",
            "Hantao Zhong",
            "Lei Clifton",
            "Fenglin Liu",
            "David A. Clifton"
        ],
        "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization",
        "abstract": "arXiv:2512.00601v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.",
        "arxiv_id": "2512.00601",
        "ARXIVID": "2512.00601",
        "COMMENT": "Does not match any specific criterion but is related to large language models and their applications in clinical reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00422": {
        "authors": [
            "Yingxuan You",
            "Chen Zhao",
            "Hantao Zhang",
            "Mingda Xu",
            "Pascal Fua"
        ],
        "title": "PhysGen: Physically Grounded 3D Shape Generation for Industrial Design",
        "abstract": "arXiv:2512.00422v1 Announce Type: new  Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.",
        "arxiv_id": "2512.00422",
        "ARXIVID": "2512.00422",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to embodied AI and generative modeling in 3D shape generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.01189": {
        "authors": [
            "Chunzheng Zhu",
            "Jialin Shao",
            "Jianxin Lin",
            "Yijun Wang",
            "Jing Wang",
            "Jinhui Tang",
            "Kenli Li"
        ],
        "title": "fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment",
        "abstract": "arXiv:2512.01189v1 Announce Type: new  Abstract: Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \\{brain, speech, gesture\\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \\textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \\textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.",
        "arxiv_id": "2512.01189",
        "ARXIVID": "2512.01189",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to neuroscience and gesture decoding, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00438": {
        "authors": [
            "Hang Xu",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "title": "FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal",
        "abstract": "arXiv:2512.00438v1 Announce Type: new  Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \\href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.",
        "arxiv_id": "2512.00438",
        "ARXIVID": "2512.00438",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00456": {
        "authors": [
            "Guanyu Hu",
            "Tangzheng Lian",
            "Dimitrios Kollias",
            "Oya Celiktutan",
            "Xinyu Yang"
        ],
        "title": "CausalAffect: Causal Discovery for Facial Affective Understanding",
        "abstract": "arXiv:2512.00456v1 Announce Type: new  Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.",
        "arxiv_id": "2512.00456",
        "ARXIVID": "2512.00456",
        "COMMENT": "Does not match any specific criteria but is related to causal discovery in facial affect analysis, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.01422": {
        "authors": [
            "Yongkun Du",
            "Miaomiao Zhao",
            "Songlin Fan",
            "Zhineng Chen",
            "Caiyan Jia",
            "Yu-Gang Jiang"
        ],
        "title": "MDiff4STR: Mask Diffusion Model for Scene Text Recognition",
        "abstract": "arXiv:2512.01422v1 Announce Type: new  Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.",
        "arxiv_id": "2512.01422",
        "ARXIVID": "2512.01422",
        "COMMENT": "Does not match any specific criteria but is related to scene text recognition using diffusion models, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00369": {
        "authors": [
            "Wenshuo Chen",
            "Haosen Li",
            "Shaofeng Liang",
            "Lei Wang",
            "Haozhe Jia",
            "Kaishen Yuan",
            "Jieming Wu",
            "Bowen Tian",
            "Yutao Yue"
        ],
        "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
        "abstract": "arXiv:2512.00369v1 Announce Type: new  Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale {\\omega} as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.",
        "arxiv_id": "2512.00369",
        "ARXIVID": "2512.00369",
        "COMMENT": "Does not match any specific criteria but is related to diffusion models and image editing, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00796": {
        "authors": [
            "Jiajian He",
            "Enjie Hu",
            "Shiqi Chen",
            "Tianchen Qiu",
            "Huajun Feng",
            "Zhihai Xu",
            "Yueting Chen"
        ],
        "title": "CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target",
        "abstract": "arXiv:2512.00796v1 Announce Type: new  Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.",
        "arxiv_id": "2512.00796",
        "ARXIVID": "2512.00796",
        "COMMENT": "Does not match any specific criteria but is related to computational vision and optical characterization, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.00331": {
        "authors": [
            "Yefeng Wu",
            "Yuchen Song",
            "Yecheng Zhao",
            "Ling Wu",
            "Shan Wan"
        ],
        "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System",
        "abstract": "arXiv:2512.00331v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.",
        "arxiv_id": "2512.00331",
        "ARXIVID": "2512.00331",
        "COMMENT": "Does not match any specific criterion but is related to multi-agent systems and educational applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.00709": {
        "authors": [
            "Yifan Xu",
            "Xichen Ye",
            "Yifan Chen",
            "Qiaosheng Zhang"
        ],
        "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF",
        "abstract": "arXiv:2512.00709v1 Announce Type: new  Abstract: Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.",
        "arxiv_id": "2512.00709",
        "ARXIVID": "2512.00709",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reinforcement learning and human feedback alignment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02017": {
        "authors": [
            "Shaowei Liu",
            "David Yifan Yao",
            "Saurabh Gupta",
            "Shenlong Wang"
        ],
        "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
        "abstract": "arXiv:2512.02017v1 Announce Type: new  Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
        "arxiv_id": "2512.02017",
        "ARXIVID": "2512.02017",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to video synchronization and computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.00075": {
        "authors": [
            "Jun Jia",
            "Hongyi Miao",
            "Yingjie Zhou",
            "Wangqiu Zhou",
            "Jianbo Zhang",
            "Linhan Cao",
            "Dandan Zhu",
            "Hua Yang",
            "Xiongkuo Min",
            "Wei Sun",
            "Guangtao Zhai"
        ],
        "title": "Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation",
        "abstract": "arXiv:2512.00075v1 Announce Type: new  Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.",
        "arxiv_id": "2512.00075",
        "ARXIVID": "2512.00075",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to image synthesis and security in generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.01321": {
        "authors": [
            "Juan Hernandez",
            "Diego Fern\\'andez",
            "Manuel Cifuentes",
            "Denis Parra",
            "Rodrigo Toro Icarte"
        ],
        "title": "Extending NGU to Multi-Agent RL: A Preliminary Study",
        "abstract": "arXiv:2512.01321v1 Announce Type: new  Abstract: The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.",
        "arxiv_id": "2512.01321",
        "ARXIVID": "2512.01321",
        "COMMENT": "Does not match any specific criteria but is related to reinforcement learning in multi-agent settings, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
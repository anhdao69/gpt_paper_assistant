{
    "2601.08273": {
        "authors": [
            "Qitan Lv",
            "Tianyu Liu",
            "Wen Wu",
            "Xuenan Xu",
            "Bowen Zhou",
            "Feng Wu",
            "Chao Zhang"
        ],
        "title": "HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding",
        "abstract": "arXiv:2601.08273v1 Announce Type: new  Abstract: Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.",
        "arxiv_id": "2601.08273",
        "ARXIVID": "2601.08273",
        "COMMENT": "This paper matches criterion 2 as it focuses on accelerating inference for Video Large Language Models (Video LLMs) with novel speculative decoding techniques.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2601.08079": {
        "authors": [
            "Hongjin Qian",
            "Zhao Cao",
            "Zheng Liu"
        ],
        "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
        "abstract": "arXiv:2601.08079v1 Announce Type: new  Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.   We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.   We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
        "arxiv_id": "2601.08079",
        "ARXIVID": "2601.08079",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel memory mechanism for tool-augmented agents, which is relevant to embodied AI and long-horizon reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.07855": {
        "authors": [
            "Subeen Lee",
            "Siyeong Lee",
            "Namil Kim",
            "Jaesik Choi"
        ],
        "title": "An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds",
        "abstract": "arXiv:2601.07855v1 Announce Type: new  Abstract: For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.",
        "arxiv_id": "2601.07855",
        "ARXIVID": "2601.07855",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces the ROAD benchmark for robust 3D perception under domain and label shifts.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.08408": {
        "authors": [
            "Yizhan Feng",
            "Hichem Snoussi",
            "Jing Teng",
            "Jian Liu",
            "Yuyang Wang",
            "Abel Cherouat",
            "Tian Wang"
        ],
        "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2",
        "abstract": "arXiv:2601.08408v1 Announce Type: new  Abstract: The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.",
        "arxiv_id": "2601.08408",
        "ARXIVID": "2601.08408",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a lightweight multimodal task platform based on BLIP-2 for UAV video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.08462": {
        "authors": [
            "Sixiong Xie",
            "Zhuofan Shi",
            "Haiyang Shen",
            "Gang Huang",
            "Yun Ma",
            "Xiang Jing"
        ],
        "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games",
        "abstract": "arXiv:2601.08462v1 Announce Type: new  Abstract: As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.",
        "arxiv_id": "2601.08462",
        "ARXIVID": "2601.08462",
        "COMMENT": "This paper does not match any specific criteria but explores benchmarks for social behaviors in LLM agents, which may be tangentially interesting for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08336": {
        "authors": [
            "Junzhuo Liu",
            "Xuemei Du",
            "Daniel Reisenbuchler",
            "Ye Chen",
            "Markus Eckstein",
            "Christian Matek",
            "Friedrich Feuerhake",
            "Dorit Merhof"
        ],
        "title": "Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways",
        "abstract": "arXiv:2601.08336v1 Announce Type: new  Abstract: Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.",
        "arxiv_id": "2601.08336",
        "ARXIVID": "2601.08336",
        "COMMENT": "This paper does not match any specific criteria but is related to multimodal learning with tissue morphology and gene expression integration.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08617": {
        "authors": [
            "Leo Fillioux",
            "Omprakash Chakraborty",
            "Ismail Ben Ayed",
            "Paul-Henry Courn\\`ede",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou",
            "Jose Dolz"
        ],
        "title": "SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning",
        "abstract": "arXiv:2601.08617v1 Announce Type: new  Abstract: With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.",
        "arxiv_id": "2601.08617",
        "ARXIVID": "2601.08617",
        "COMMENT": "Does not match any specific criteria. Focuses on calibration of vision-language models for test-time prompt tuning, which is tangentially related to criterion 2 but not directly.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08125": {
        "authors": [
            "Kequan Chen",
            "Yuxuan Wang",
            "Pan Liu",
            "Victor L. Knoop",
            "David Z. W. Wang",
            "Yu Han"
        ],
        "title": "How vehicles change lanes after encountering crashes: Empirical analysis and modeling",
        "abstract": "arXiv:2601.08125v1 Announce Type: new  Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.",
        "arxiv_id": "2601.08125",
        "ARXIVID": "2601.08125",
        "COMMENT": "This paper does not match any specific criteria but is related to trajectory prediction and modeling in traffic scenarios, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.07975": {
        "authors": [
            "Fei Li",
            "Lang Qiao",
            "Jiahao Fan",
            "Yijia Xu",
            "Shawn M. Kaeppler",
            "Zhou Zhang"
        ],
        "title": "An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery",
        "abstract": "arXiv:2601.07975v1 Announce Type: new  Abstract: High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.   To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.   Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.",
        "arxiv_id": "2601.07975",
        "ARXIVID": "2601.07975",
        "COMMENT": "This paper does not match any specific criteria but is related to computer vision and machine learning in the context of UAV imagery and small-object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
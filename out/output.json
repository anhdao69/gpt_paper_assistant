{
    "2512.10942": {
        "authors": [
            "Delong Chen",
            "Mustafa Shukor",
            "Theo Moutakanni",
            "Willy Chung",
            "Jade Yu",
            "Tejaswi Kasarla",
            "Allen Bolourchi",
            "Yann LeCun",
            "Pascale Fung"
        ],
        "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
        "abstract": "arXiv:2512.10942v1 Announce Type: new  Abstract: We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
        "arxiv_id": "2512.10942",
        "ARXIVID": "2512.10942",
        "COMMENT": "Matches criterion 2. Proposes VL-JEPA, a vision-language model with a novel joint embedding predictive architecture, relevant to VLLMs and multimodal LLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.10863": {
        "authors": [
            "Jingli Lin",
            "Runsen Xu",
            "Shaohao Zhu",
            "Sihan Yang",
            "Peizhou Cao",
            "Yunlong Ran",
            "Miao Hu",
            "Chenming Zhu",
            "Yiman Xie",
            "Yilin Long",
            "Wenbo Hu",
            "Dahua Lin",
            "Tai Wang",
            "Jiangmiao Pang"
        ],
        "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
        "abstract": "arXiv:2512.10863v1 Announce Type: new  Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
        "arxiv_id": "2512.10863",
        "ARXIVID": "2512.10863",
        "COMMENT": "Matches criterion 3. Introduces MMSI-Video-Bench, a benchmark for video-based spatial intelligence in MLLMs, which is relevant to embodied/robotic AI and spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.10226": {
        "authors": [
            "Shuhan Tan",
            "Kashyap Chitta",
            "Yuxiao Chen",
            "Ran Tian",
            "Yurong You",
            "Yan Wang",
            "Wenjie Luo",
            "Yulong Cao",
            "Philipp Krahenbuhl",
            "Marco Pavone",
            "Boris Ivanovic"
        ],
        "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "abstract": "arXiv:2512.10226v1 Announce Type: new  Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
        "arxiv_id": "2512.10226",
        "ARXIVID": "2512.10226",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel latent chain-of-thought reasoning model for autonomous driving, improving spatial reasoning and decision-making.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.10940": {
        "authors": [
            "Xiang Fan",
            "Sharath Girish",
            "Vivek Ramanujan",
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Petr Sushko",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Ranjay Krishna"
        ],
        "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
        "abstract": "arXiv:2512.10940v1 Announce Type: new  Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
        "arxiv_id": "2512.10940",
        "ARXIVID": "2512.10940",
        "COMMENT": "Matches criterion 6. Introduces OmniView, a unified framework for 3D and 4D view synthesis, relevant to video understanding and generation tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.10046": {
        "authors": [
            "Yan Zhuang",
            "Jiawei Ren",
            "Xiaokang Ye",
            "Jianzhi Shen",
            "Ruixuan Zhang",
            "Tianai Yue",
            "Muhammad Faayez",
            "Xuhong He",
            "Ziqiao Ma",
            "Lianhui Qin",
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
        "abstract": "arXiv:2512.10046v1 Announce Type: new  Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
        "arxiv_id": "2512.10046",
        "ARXIVID": "2512.10046",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its introduction of a new simulation platform and benchmarks for multimodal robot navigation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.10927": {
        "authors": [
            "Yulu Gan",
            "Ligeng Zhu",
            "Dandan Shan",
            "Baifeng Shi",
            "Hongxu Yin",
            "Boris Ivanovic",
            "Song Han",
            "Trevor Darrell",
            "Jitendra Malik",
            "Marco Pavone",
            "Boyi Li"
        ],
        "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
        "abstract": "arXiv:2512.10927v1 Announce Type: new  Abstract: Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
        "arxiv_id": "2512.10927",
        "ARXIVID": "2512.10927",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) and 6 (Video Understanding) due to its focus on motion understanding, spatial reasoning, and fine-tuning vision-language models for video tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.10660": {
        "authors": [
            "Hanfeng Wu",
            "Marlon Steiner",
            "Michael Schmidt",
            "Alvaro Marcos-Ramiro",
            "Christoph Stiller"
        ],
        "title": "NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation",
        "abstract": "arXiv:2512.10660v1 Announce Type: new  Abstract: The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.",
        "arxiv_id": "2512.10660",
        "ARXIVID": "2512.10660",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it presents a controllable navigation-guided end-to-end model for autonomous driving with a novel compliance metric.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.10322": {
        "authors": [
            "Yongqiang Yu",
            "Xuhui Li",
            "Hazza Mahmood",
            "Jinxing Zhou",
            "Haodong Hong",
            "Longtao Jiang",
            "Zhiqiang Xu",
            "Qi Wu",
            "Xiaojun Chang"
        ],
        "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation",
        "abstract": "arXiv:2512.10322v1 Announce Type: new  Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.",
        "arxiv_id": "2512.10322",
        "ARXIVID": "2512.10322",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on vision-and-language navigation and user-feedback-driven adaptation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.10955": {
        "authors": [
            "Tsai-Shien Chen",
            "Aliaksandr Siarohin",
            "Guocheng Gordon Qian",
            "Kuan-Chieh Jackson Wang",
            "Egor Nemchinov",
            "Moayed Haji-Ali",
            "Riza Alp Guler",
            "Willi Menapace",
            "Ivan Skorokhodov",
            "Anil Kag",
            "Jun-Yan Zhu",
            "Sergey Tulyakov"
        ],
        "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
        "abstract": "arXiv:2512.10955v1 Announce Type: new  Abstract: Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
        "arxiv_id": "2512.10955",
        "ARXIVID": "2512.10955",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces an open-vocabulary attribute encoder for visual concept personalization, which aligns with vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10959": {
        "authors": [
            "Tjark Behrens",
            "Anton Obukhov",
            "Bingxin Ke",
            "Fabio Tosi",
            "Matteo Poggi",
            "Konrad Schindler"
        ],
        "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "abstract": "arXiv:2512.10959v1 Announce Type: new  Abstract: We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
        "arxiv_id": "2512.10959",
        "ARXIVID": "2512.10959",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a diffusion-based framework for stereo geometry synthesis, focusing on a novel viewpoint-conditioned approach.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10943": {
        "authors": [
            "Sharath Girish",
            "Viacheslav Ivanov",
            "Tsai-Shien Chen",
            "Hao Chen",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov"
        ],
        "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
        "abstract": "arXiv:2512.10943v1 Announce Type: new  Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
        "arxiv_id": "2512.10943",
        "ARXIVID": "2512.10943",
        "COMMENT": "Matches criterion 6. Proposes AlcheMinT, a framework for fine-grained temporal control in multi-reference consistent video generation, relevant to video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10958": {
        "authors": [
            "Ao Liang",
            "Lingdong Kong",
            "Tianyi Yan",
            "Hongsi Liu",
            "Wesley Yang",
            "Ziqi Huang",
            "Wei Yin",
            "Jialong Zuo",
            "Yixuan Hu",
            "Dekai Zhu",
            "Dongyue Lu",
            "Youquan Liu",
            "Guangfeng Jiang",
            "Linfeng Li",
            "Xiangtai Li",
            "Long Zhuo",
            "Lai Xing Ng",
            "Benoit R. Cottereau",
            "Changxin Gao",
            "Liang Pan",
            "Wei Tsang Ooi",
            "Ziwei Liu"
        ],
        "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "abstract": "arXiv:2512.10958v1 Announce Type: new  Abstract: Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
        "arxiv_id": "2512.10958",
        "ARXIVID": "2512.10958",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its comprehensive evaluation framework for generative world models in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10860": {
        "authors": [
            "Kehong Gong",
            "Zhengyu Wen",
            "Mingxi Xu",
            "Weixia He",
            "Qi Wang",
            "Ning Zhang",
            "Zhengyu Li",
            "Chenbin Li",
            "Dongze Lian",
            "Wei Zhao",
            "Xiaoyu He",
            "Mingyuan Zhang"
        ],
        "title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation",
        "abstract": "arXiv:2512.10860v1 Announce Type: new  Abstract: Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/",
        "arxiv_id": "2512.10860",
        "ARXIVID": "2512.10860",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on temporal 4D mesh generation from videos and spatial-temporal modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10957": {
        "authors": [
            "Yukai Shi",
            "Weiyu Li",
            "Zihao Wang",
            "Hongyang Li",
            "Xingyu Chen",
            "Ping Tan",
            "Lei Zhang"
        ],
        "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "abstract": "arXiv:2512.10957v1 Announce Type: new  Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
        "arxiv_id": "2512.10957",
        "ARXIVID": "2512.10957",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on spatial reasoning and pose estimation in 3D scene generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.10947": {
        "authors": [
            "Jiawei Yang",
            "Ziyu Chen",
            "Yurong You",
            "Yan Wang",
            "Yiming Li",
            "Yuxiao Chen",
            "Boyi Li",
            "Boris Ivanovic",
            "Marco Pavone",
            "Yue Wang"
        ],
        "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
        "abstract": "arXiv:2512.10947v1 Announce Type: new  Abstract: We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
        "arxiv_id": "2512.10947",
        "ARXIVID": "2512.10947",
        "COMMENT": "Matches criterion 3. Proposes a novel scene encoder (Flex) for multi-camera data in autonomous driving, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.10548": {
        "authors": [
            "Yuchen Feng",
            "Zhenyu Zhang",
            "Naibin Gu",
            "Yilong Chen",
            "Peng Fu",
            "Zheng Lin",
            "Shuohuan Wang",
            "Yu Sun",
            "Hua Wu",
            "Weiping Wang",
            "Haifeng Wang"
        ],
        "title": "Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding",
        "abstract": "arXiv:2512.10548v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential \"blink-like\" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.",
        "arxiv_id": "2512.10548",
        "ARXIVID": "2512.10548",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework (Blink) for enhancing multimodal large language models with dynamic visual token resolution.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.10324": {
        "authors": [
            "Chao Gong",
            "Depeng Wang",
            "Zhipeng Wei",
            "Ya Guo",
            "Huijia Zhu",
            "Jingjing Chen"
        ],
        "title": "EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs",
        "abstract": "arXiv:2512.10324v1 Announce Type: new  Abstract: Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.",
        "arxiv_id": "2512.10324",
        "ARXIVID": "2512.10324",
        "COMMENT": "Matches criterion 2 as it explores a novel framework for audio-visual large language models (AV-LLMs) with token reduction strategies.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.10949": {
        "authors": [
            "Yiwen Tang",
            "Zoey Guo",
            "Kaixin Zhu",
            "Ray Zhang",
            "Qizhi Chen",
            "Dongzhi Jiang",
            "Junli Liu",
            "Bohan Zeng",
            "Haoming Song",
            "Delin Qu",
            "Tianyi Bai",
            "Dan Xu",
            "Wentao Zhang",
            "Bin Zhao"
        ],
        "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
        "abstract": "arXiv:2512.10949v1 Announce Type: new  Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
        "arxiv_id": "2512.10949",
        "ARXIVID": "2512.10949",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MME-3DR) and explores novel methods for text-to-3D generation using reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.10807": {
        "authors": [
            "Wang Lu",
            "Yao Zhu",
            "Jindong Wang"
        ],
        "title": "HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition",
        "abstract": "arXiv:2512.10807v1 Announce Type: new  Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.",
        "arxiv_id": "2512.10807",
        "ARXIVID": "2512.10807",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (HAROOD) for out-of-distribution generalization in human activity recognition, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.10252": {
        "authors": [
            "Rui Wang",
            "Yimu Sun",
            "Jingxing Guo",
            "Huisi Wu",
            "Jing Qin"
        ],
        "title": "GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule",
        "abstract": "arXiv:2512.10252v1 Announce Type: new  Abstract: Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.",
        "arxiv_id": "2512.10252",
        "ARXIVID": "2512.10252",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding for echocardiography video segmentation with novel spatiotemporal modeling techniques.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.10102": {
        "authors": [
            "Neelima Prasad",
            "Jarek Reynolds",
            "Neel Karsanbhai",
            "Tanusree Sharma",
            "Lotus Zhang",
            "Abigale Stangl",
            "Yang Wang",
            "Leah Findlater",
            "Danna Gurari"
        ],
        "title": "Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information",
        "abstract": "arXiv:2512.10102v1 Announce Type: new  Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/",
        "arxiv_id": "2512.10102",
        "ARXIVID": "2512.10102",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for hierarchical instance tracking, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.10867": {
        "authors": [
            "Zongzhao Li",
            "Xiangzhe Kong",
            "Jiahui Su",
            "Zongyang Ma",
            "Mingze Li",
            "Songyou Li",
            "Yuelin Zhang",
            "Yu Rong",
            "Tingyang Xu",
            "Deli Zhao",
            "Wenbing Huang"
        ],
        "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
        "abstract": "arXiv:2512.10867v1 Announce Type: new  Abstract: This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
        "arxiv_id": "2512.10867",
        "ARXIVID": "2512.10867",
        "COMMENT": "Matches criterion 1 as it benchmarks spatial intelligence using vision-language models, specifically in the microscopic domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.10881": {
        "authors": [
            "Kehong Gong",
            "Zhengyu Wen",
            "Weixia He",
            "Mingxi Xu",
            "Qi Wang",
            "Ning Zhang",
            "Zhengyu Li",
            "Dongze Lian",
            "Wei Zhao",
            "Xiaoyu He",
            "Mingyuan Zhang"
        ],
        "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
        "abstract": "arXiv:2512.10881v1 Announce Type: new  Abstract: Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
        "arxiv_id": "2512.10881",
        "ARXIVID": "2512.10881",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for motion capture from monocular videos, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.10275": {
        "authors": [
            "Hongsin Lee",
            "Hye Won Chung"
        ],
        "title": "Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation",
        "abstract": "arXiv:2512.10275v1 Announce Type: new  Abstract: Adversarial distillation in the standard min-max adversarial training framework aims to transfer adversarial robustness from a large, robust teacher network to a compact student. However, existing work often neglects to incorporate state-of-the-art robust teachers. Through extensive analysis, we find that stronger teachers do not necessarily yield more robust students-a phenomenon known as robust saturation. While typically attributed to capacity gaps, we show that such explanations are incomplete. Instead, we identify adversarial transferability-the fraction of student-crafted adversarial examples that remain effective against the teacher-as a key factor in successful robustness transfer. Based on this insight, we propose Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by their measured transferability without incurring additional computational cost. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SAAD consistently improves AutoAttack robustness over prior methods. Our code is available at https://github.com/HongsinLee/saad.",
        "arxiv_id": "2512.10275",
        "ARXIVID": "2512.10275",
        "COMMENT": "Does not match any specific criterion but is related to adversarial robustness in distillation, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10715": {
        "authors": [
            "Matias Cosarinsky",
            "Nicolas Gaggion",
            "Rodrigo Echeveste",
            "Enzo Ferrante"
        ],
        "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
        "abstract": "arXiv:2512.10715v1 Announce Type: new  Abstract: Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
        "arxiv_id": "2512.10715",
        "ARXIVID": "2512.10715",
        "COMMENT": "Does not match any specific criterion but is related to uncertainty estimation in medical imaging, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10939": {
        "authors": [
            "Madhav Agarwal",
            "Mingtian Zhang",
            "Laura Sevilla-Lara",
            "Steven McDonagh"
        ],
        "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
        "abstract": "arXiv:2512.10939v1 Announce Type: new  Abstract: Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
        "arxiv_id": "2512.10939",
        "ARXIVID": "2512.10939",
        "COMMENT": "Does not match any specific criteria but involves generative modeling for 3D talking heads, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10041": {
        "authors": [
            "Yihao Liu",
            "Chenyu Gao",
            "Lianrui Zuo",
            "Michael E. Kim",
            "Brian D. Boyd",
            "Lisa L. Barnes",
            "Walter A. Kukull",
            "Lori L. Beason-Held",
            "Susan M. Resnick",
            "Timothy J. Hohman",
            "Warren D. Taylor",
            "Bennett A. Landman"
        ],
        "title": "MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata",
        "abstract": "arXiv:2512.10041v1 Announce Type: new  Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.",
        "arxiv_id": "2512.10041",
        "ARXIVID": "2512.10041",
        "COMMENT": "Does not match any specific criteria but involves generative modeling and multimodal learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10095": {
        "authors": [
            "Jiachen Tao",
            "Junyi Wu",
            "Haoxuan Wang",
            "Zongxin Yang",
            "Dawen Cai",
            "Yan Yan"
        ],
        "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing",
        "abstract": "arXiv:2512.10095v1 Announce Type: new  Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.",
        "arxiv_id": "2512.10095",
        "ARXIVID": "2512.10095",
        "COMMENT": "Does not match any specific criteria. Focuses on dynamic 3D reconstruction of specular scenes, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.09969": {
        "authors": [
            "Paul Hueber",
            "Luca Peres",
            "Florian Pitters",
            "Alejandro Gloriani",
            "Oliver Rhodes"
        ],
        "title": "Neuromorphic Eye Tracking for Low-Latency Pupil Detection",
        "abstract": "arXiv:2512.09969v1 Announce Type: new  Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.",
        "arxiv_id": "2512.09969",
        "ARXIVID": "2512.09969",
        "COMMENT": "Does not match any specific criteria. Focuses on neuromorphic eye tracking for wearable systems, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10273": {
        "authors": [
            "Yuxin Liu",
            "Chaojie Gu",
            "Yihang Zhang",
            "Bin Qian",
            "Shibo He"
        ],
        "title": "Reverse Thinking Enhances Missing Information Detection in Large Language Models",
        "abstract": "arXiv:2512.10273v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.",
        "arxiv_id": "2512.10273",
        "ARXIVID": "2512.10273",
        "COMMENT": "Does not match any specific criteria. Focuses on reverse thinking for missing information detection in LLMs, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10534": {
        "authors": [
            "Haiteng Zhao",
            "Junhao Shen",
            "Yiming Zhang",
            "Songyang Gao",
            "Kuikun Liu",
            "Tianyou Ma",
            "Fan Zheng",
            "Dahua Lin",
            "Wenwei Zhang",
            "Kai Chen"
        ],
        "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
        "abstract": "arXiv:2512.10534v1 Announce Type: new  Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
        "arxiv_id": "2512.10534",
        "ARXIVID": "2512.10534",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of large language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10206": {
        "authors": [
            "Yakun Zhu",
            "Zhongzhen Huang",
            "Qianhan Feng",
            "Linjie Mu",
            "Yannian Gu",
            "Shaoting Zhang",
            "Qi Dou",
            "Xiaofan Zhang"
        ],
        "title": "CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment",
        "abstract": "arXiv:2512.10206v1 Announce Type: new  Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.",
        "arxiv_id": "2512.10206",
        "ARXIVID": "2512.10206",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of benchmarks and evaluation in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.10386": {
        "authors": [
            "Ge Zhang",
            "Chunyang Wang",
            "Bo Xiao",
            "Xuelian Liu",
            "Bin Liu"
        ],
        "title": "Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method",
        "abstract": "arXiv:2512.10386v1 Announce Type: new  Abstract: High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.",
        "arxiv_id": "2512.10386",
        "ARXIVID": "2512.10386",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of 3D data processing and denoising.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.10652": {
        "authors": [
            "Jian-Yu Jiang-Lin",
            "Kang-Yang Huang",
            "Ling Zou",
            "Ling Lo",
            "Sheng-Ping Yang",
            "Yu-Wen Tseng",
            "Kun-Hsiang Lin",
            "Chia-Ling Chen",
            "Yu-Ting Ta",
            "Yan-Tsung Wang",
            "Po-Ching Chen",
            "Hongxia Xie",
            "Hong-Han Shuai",
            "Wen-Huang Cheng"
        ],
        "title": "TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection",
        "abstract": "arXiv:2512.10652v1 Announce Type: new  Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.",
        "arxiv_id": "2512.10652",
        "ARXIVID": "2512.10652",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of generative modeling and detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
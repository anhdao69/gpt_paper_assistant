{
    "2512.17640": {
        "authors": [
            "Zhaolin Cai",
            "Huiyu Duan",
            "Zitong Xu",
            "Fan Li",
            "Zhi Liu",
            "Jing Liu",
            "Wei Shen",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs",
        "abstract": "arXiv:2512.17640v1 Announce Type: new  Abstract: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.",
        "arxiv_id": "2512.17640",
        "ARXIVID": "2512.17640",
        "COMMENT": "Matches criterion 2 as it explores a novel framework for integrating multi-modal large language models (MLLMs) with human-object interaction detection, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.17907": {
        "authors": [
            "Byungjun Kim",
            "Taeksoo Kim",
            "Junyoung Lee",
            "Hanbyul Joo"
        ],
        "title": "Dexterous World Models",
        "abstract": "arXiv:2512.17907v1 Announce Type: new  Abstract: Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.   Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.   Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.",
        "arxiv_id": "2512.17907",
        "ARXIVID": "2512.17907",
        "COMMENT": "Matches criterion 3 as it introduces a framework for modeling human-scene interactions in 3D environments, enabling embodied interactivity, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2512.17012": {
        "authors": [
            "Chiao-An Yang",
            "Ryo Hachiuma",
            "Sifei Liu",
            "Subhashree Radhakrishnan",
            "Raymond A. Yeh",
            "Yu-Chiang Frank Wang",
            "Min-Hung Chen"
        ],
        "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
        "abstract": "arXiv:2512.17012v2 Announce Type: new  Abstract: Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
        "arxiv_id": "2512.17012",
        "ARXIVID": "2512.17012",
        "COMMENT": "Matches criterion 6 as it introduces a specialized MLLM for 4D video understanding and a new benchmark for depth-aware dynamic scenes, pushing the boundaries of video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2512.17573": {
        "authors": [
            "Qilong Wang",
            "Xiaofan Ming",
            "Zhenyi Lin",
            "Jinwen Li",
            "Dongwei Ren",
            "Wangmeng Zuo",
            "Qinghua Hu"
        ],
        "title": "RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis",
        "abstract": "arXiv:2512.17573v1 Announce Type: new  Abstract: Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \\url{https://github.com/stonecutter-21/roomeditor}.",
        "arxiv_id": "2512.17573",
        "ARXIVID": "2512.17573",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (RoomBench++) and a novel method (RoomEditor++) for high-fidelity furniture synthesis, relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.18189": {
        "authors": [
            "Zihao Deng",
            "Yijia Li",
            "Renrui Zhang",
            "Peijun Ye"
        ],
        "title": "NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework",
        "abstract": "arXiv:2512.18189v1 Announce Type: new  Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.",
        "arxiv_id": "2512.18189",
        "ARXIVID": "2512.18189",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for auto-formalizing cognitive decision-making rules and constructing cognitive agents, which relates to spatial intelligence and embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.17504": {
        "authors": [
            "Hoiyeong Jin",
            "Hyojin Jang",
            "Jeongho Kim",
            "Junha Hyung",
            "Kinam Kim",
            "Dongjin Kim",
            "Huijin Choi",
            "Hyeonji Kim",
            "Jaegul Choo"
        ],
        "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
        "abstract": "arXiv:2512.17504v1 Announce Type: new  Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
        "arxiv_id": "2512.17504",
        "ARXIVID": "2512.17504",
        "COMMENT": "Matches criteria 6 as it focuses on video object insertion with 4D scene geometry and diffusion models, pushing the boundaries of video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.17040": {
        "authors": [
            "Min-Jung Kim",
            "Jeongho Kim",
            "Hoiyeong Jin",
            "Junha Hyung",
            "Jaegul Choo"
        ],
        "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
        "abstract": "arXiv:2512.17040v1 Announce Type: new  Abstract: Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
        "arxiv_id": "2512.17040",
        "ARXIVID": "2512.17040",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding with camera-controlled video generation using novel methodologies.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.17306": {
        "authors": [
            "Wenhao Yang",
            "Yu Xia",
            "Jinlong Huang",
            "Shiyin Lu",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Yuanyu Wan",
            "Lijun Zhang"
        ],
        "title": "Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images",
        "abstract": "arXiv:2512.17306v1 Announce Type: new  Abstract: Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.",
        "arxiv_id": "2512.17306",
        "ARXIVID": "2512.17306",
        "COMMENT": "Matches criteria 2 as it explores reasoning capabilities in Vision-Language Models with a focus on multi-turn reasoning and multimodal CoT.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.17229": {
        "authors": [
            "Henghui Du",
            "Chang Zhou",
            "Chunjie Zhang",
            "Xi Chen",
            "Di Hu"
        ],
        "title": "Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos",
        "abstract": "arXiv:2512.17229v1 Announce Type: new  Abstract: Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.",
        "arxiv_id": "2512.17229",
        "ARXIVID": "2512.17229",
        "COMMENT": "Matches criteria 6 as it addresses video-based question answering with a novel memory mechanism for long video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.17897": {
        "authors": [
            "Tomer Borreda",
            "Fangqiang Ding",
            "Sanja Fidler",
            "Shengyu Huang",
            "Or Litany"
        ],
        "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
        "abstract": "arXiv:2512.17897v1 Announce Type: new  Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
        "arxiv_id": "2512.17897",
        "ARXIVID": "2512.17897",
        "COMMENT": "Matches criteria 5 as it combines image understanding tasks with radar point cloud generation using diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.17151": {
        "authors": [
            "Taewon Kang",
            "Joseph K J",
            "Chris Tensmeyer",
            "Jihyung Kil",
            "Wanrong Zhu",
            "Ming C. Lin",
            "Vlad I. Morariu"
        ],
        "title": "Text-Conditioned Background Generation for Editable Multi-Layer Documents",
        "abstract": "arXiv:2512.17151v1 Announce Type: new  Abstract: We present a framework for document-centric background generation with multi-page editing and thematic continuity. To ensure text regions remain readable, we employ a \\emph{latent masking} formulation that softly attenuates updates in the diffusion space, inspired by smooth barrier functions in physics and numerical optimization. In addition, we introduce \\emph{Automated Readability Optimization (ARO)}, which automatically places semi-transparent, rounded backing shapes behind text regions. ARO determines the minimal opacity needed to satisfy perceptual contrast standards (WCAG 2.2) relative to the underlying background, ensuring readability while maintaining aesthetic harmony without human intervention. Multi-page consistency is maintained through a summarization-and-instruction process, where each page is distilled into a compact representation that recursively guides subsequent generations. This design reflects how humans build continuity by retaining prior context, ensuring that visual motifs evolve coherently across an entire document. Our method further treats a document as a structured composition in which text, figures, and backgrounds are preserved or regenerated as separate layers, allowing targeted background editing without compromising readability. Finally, user-provided prompts allow stylistic adjustments in color and texture, balancing automated consistency with flexible customization. Our training-free framework produces visually coherent, text-preserving, and thematically aligned documents, bridging generative modeling with natural design workflows.",
        "arxiv_id": "2512.17151",
        "ARXIVID": "2512.17151",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to generative modeling and multi-modal learning through its focus on document-centric background generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.17655": {
        "authors": [
            "Evangelos Sariyanidi",
            "Gokul Nair",
            "Lisa Yankowitz",
            "Casey J. Zampella",
            "Mohan Kashyap Pargi",
            "Aashvi Manakiwala",
            "Maya McNealis",
            "John D. Herrington",
            "Jeffrey Cohn",
            "Robert T. Schultz",
            "Birkan Tunc"
        ],
        "title": "Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos",
        "abstract": "arXiv:2512.17655v1 Announce Type: new  Abstract: Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.",
        "arxiv_id": "2512.17655",
        "ARXIVID": "2512.17655",
        "COMMENT": "Does not match any specific criteria but introduces a toolkit for behavioral analysis from videos, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.17143": {
        "authors": [
            "Sandeep Mishra",
            "Yasamin Jafarian",
            "Andreas Lugmayr",
            "Yingwei Li",
            "Varsha Ramakrishnan",
            "Srivatsan Varadharajan",
            "Alan C. Bovik",
            "Ira Kemelmacher-Shlizerman"
        ],
        "title": "Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps",
        "abstract": "arXiv:2512.17143v1 Announce Type: new  Abstract: Photographs of people taken by professional photographers typically present the person in beautiful lighting, with an interesting pose, and flattering quality. This is unlike common photos people can take of themselves. In this paper, we explore how to create a ``professional'' version of a person's photograph, i.e., in a chosen pose, in a simple environment, with good lighting, and standard black top/bottom clothing. A key challenge is to preserve the person's unique identity, face and body features while transforming the photo. If there would exist a large paired dataset of the same person photographed both ``in the wild'' and by a professional photographer, the problem would potentially be easier to solve. However, such data does not exist, especially for a large variety of identities. To that end, we propose two key insights: 1) Our method transforms the input photo and person's face to a canonical UV space, which is further coupled with reposing methodology to model occlusions and novel view synthesis. Operating in UV space allows us to leverage existing unpaired datasets. 2) We personalize the output photo via multi image finetuning. Our approach yields high-quality, reposed portraits and achieves strong qualitative and quantitative performance on real-world imagery.",
        "arxiv_id": "2512.17143",
        "ARXIVID": "2512.17143",
        "COMMENT": "Does not match any specific criteria but involves generative modeling for portrait synthesis, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
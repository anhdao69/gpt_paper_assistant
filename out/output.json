{
    "2511.23170": {
        "authors": [
            "Masaki Kawamura",
            "Nakamasa Inoue",
            "Rintaro Yanagi",
            "Hirokatsu Kataoka",
            "Rio Yokota"
        ],
        "title": "PowerCLIP: Powerset Alignment for Contrastive Pre-Training",
        "abstract": "arXiv:2511.23170v1 Announce Type: new  Abstract: Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.",
        "arxiv_id": "2511.23170",
        "ARXIVID": "2511.23170",
        "COMMENT": "Matches criterion 2 as it proposes a novel contrastive pre-training framework for vision-language models, enhancing compositional understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.22973": {
        "authors": [
            "Zeyu Zhang",
            "Shuning Chang",
            "Yuanyu He",
            "Yizeng Han",
            "Jiasheng Tang",
            "Fan Wang",
            "Bohan Zhuang"
        ],
        "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
        "abstract": "arXiv:2511.22973v1 Announce Type: new  Abstract: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
        "arxiv_id": "2511.22973",
        "ARXIVID": "2511.22973",
        "COMMENT": "Matches criterion 6 as it focuses on video generation and introduces novel methodologies and benchmarks for long-video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.21750": {
        "authors": [
            "Di Feng",
            "Kaixin Ma",
            "Feng Nan",
            "Haofeng Chen",
            "Bohan Zhai",
            "David Griffiths",
            "Mingfei Gao",
            "Zhe Gan",
            "Eshan Verma",
            "Yinfei Yang",
            "Zhifeng Chen",
            "Afshin Dehghan"
        ],
        "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
        "abstract": "arXiv:2511.21750v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
        "arxiv_id": "2511.21750",
        "ARXIVID": "2511.21750",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark (SO-Bench) for evaluating multimodal large language models with structured outputs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.23429": {
        "authors": [
            "Junshu Tang",
            "Jiacheng Liu",
            "Jiaqi Li",
            "Longhuang Wu",
            "Haoyu Yang",
            "Penghao Zhao",
            "Siruis Gong",
            "Xiang Yuan",
            "Shuai Shao",
            "Qinglin Lu"
        ],
        "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
        "abstract": "arXiv:2511.23429v1 Announce Type: new  Abstract: Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
        "arxiv_id": "2511.23429",
        "ARXIVID": "2511.23429",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (InterBench) for evaluating interaction in generative game world models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.22663": {
        "authors": [
            "Dian Zheng",
            "Manyuan Zhang",
            "Hongyu Li",
            "Kai Zou",
            "Hongbo Liu",
            "Ziyu Guo",
            "Kaituo Feng",
            "Yexin Liu",
            "Ying Luo",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Hongsheng Li"
        ],
        "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
        "abstract": "arXiv:2511.22663v1 Announce Type: new  Abstract: Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
        "arxiv_id": "2511.22663",
        "ARXIVID": "2511.22663",
        "COMMENT": "Matches criterion 2 as it explores novel training strategies for unified multimodal models, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.23469": {
        "authors": [
            "Jiahao Guo",
            "Sinan Du",
            "Jingfeng Yao",
            "Wenyu Liu",
            "Bo Li",
            "Haoxiang Cao",
            "Kun Gai",
            "Chun Yuan",
            "Kai Wu",
            "Xinggang Wang"
        ],
        "title": "Visual Generation Tuning",
        "abstract": "arXiv:2511.23469v1 Announce Type: new  Abstract: Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "arxiv_id": "2511.23469",
        "ARXIVID": "2511.23469",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it explores visual generation tuning for vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.23386": {
        "authors": [
            "Sinan Du",
            "Jiahao Guo",
            "Bo Li",
            "Shuhao Cui",
            "Zhengzhuo Xu",
            "Yifu Luo",
            "Yongxian Wei",
            "Kun Gai",
            "Xinggang Wang",
            "Kai Wu",
            "Chun Yuan"
        ],
        "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
        "abstract": "arXiv:2511.23386v1 Announce Type: new  Abstract: Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
        "arxiv_id": "2511.23386",
        "ARXIVID": "2511.23386",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a unified tokenizer for multimodal understanding, generation, and reconstruction.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.21998": {
        "authors": [
            "Apratim Bhattacharyya",
            "Bicheng Xu",
            "Sanjay Haresh",
            "Reza Pourreza",
            "Litian Liu",
            "Sunny Panchal",
            "Pulkit Madan",
            "Leonid Sigal",
            "Roland Memisevic"
        ],
        "title": "Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?",
        "abstract": "arXiv:2511.21998v1 Announce Type: new  Abstract: Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.",
        "arxiv_id": "2511.21998",
        "ARXIVID": "2511.21998",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores multi-modal LLMs for live, interactive task guidance.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.23262": {
        "authors": [
            "Yang Li",
            "Zhiyuan He",
            "Yuxuan Huang",
            "Zhuhanling Xiao",
            "Chao Yu",
            "Meng Fang",
            "Kun Shao",
            "Jun Wang"
        ],
        "title": "Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning",
        "abstract": "arXiv:2511.23262v1 Announce Type: new  Abstract: Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.",
        "arxiv_id": "2511.23262",
        "ARXIVID": "2511.23262",
        "COMMENT": "Matches criterion 2 as it explores vision-language models with test-time reasoning and adaptation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.23151": {
        "authors": [
            "Jin-Seop Lee",
            "SungJoon Lee",
            "SeongJun Jung",
            "Boyang Li",
            "Jee-Hyong Lee"
        ],
        "title": "Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding",
        "abstract": "arXiv:2511.23151v1 Announce Type: new  Abstract: Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.",
        "arxiv_id": "2511.23151",
        "ARXIVID": "2511.23151",
        "COMMENT": "Matches criterion 6 as it focuses on video temporal grounding and introduces novel methods for handling hard-irrelevant queries.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.22625": {
        "authors": [
            "Fukun Yin",
            "Shiyu Liu",
            "Yucheng Han",
            "Zhibo Wang",
            "Peng Xing",
            "Rui Wang",
            "Wei Cheng",
            "Yingming Wang",
            "Aojie Li",
            "Zixin Yin",
            "Pengtao Chen",
            "Xiangyu Zhang",
            "Daxin Jiang",
            "Xianfang Zeng",
            "Gang Yu"
        ],
        "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
        "abstract": "arXiv:2511.22625v1 Announce Type: new  Abstract: Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
        "arxiv_id": "2511.22625",
        "ARXIVID": "2511.22625",
        "COMMENT": "Matches criterion 5 as it discusses integration of image editing tasks with multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.23269": {
        "authors": [
            "Timothy Ossowski",
            "Sheng Zhang",
            "Qianchu Liu",
            "Guanghui Qin",
            "Reuben Tan",
            "Tristan Naumann",
            "Junjie Hu",
            "Hoifung Poon"
        ],
        "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
        "abstract": "arXiv:2511.23269v1 Announce Type: new  Abstract: High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
        "arxiv_id": "2511.23269",
        "ARXIVID": "2511.23269",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models in the medical domain.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.22019": {
        "authors": [
            "Zhenxiang Lin",
            "Maryam Haghighat",
            "Will Browne",
            "Dimity Miller"
        ],
        "title": "Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models",
        "abstract": "arXiv:2511.22019v1 Announce Type: new  Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.",
        "arxiv_id": "2511.22019",
        "ARXIVID": "2511.22019",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a method for uncertainty estimation in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.23191": {
        "authors": [
            "Yuhao Wan",
            "Lijuan Liu",
            "Jingzhi Zhou",
            "Zihan Zhou",
            "Xuying Zhang",
            "Dongbo Zhang",
            "Shaohui Jiao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "title": "GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation",
        "abstract": "arXiv:2511.23191v1 Announce Type: new  Abstract: Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.",
        "arxiv_id": "2511.23191",
        "ARXIVID": "2511.23191",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on generating high-fidelity 3D scenes from video frames, enhancing geometric consistency.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.22154": {
        "authors": [
            "Eun Chang",
            "Zhuangqun Huang",
            "Yiwei Liao",
            "Sagar Ravi Bhavsar",
            "Amogh Param",
            "Tammy Stark",
            "Adel Ahmadyan",
            "Xiao Yang",
            "Jiaqi Wang",
            "Ahsan Abdullah",
            "Giang Nguyen",
            "Akil Iyer",
            "David Hall",
            "Elissa Li",
            "Shane Moon",
            "Nicolas Scheffer",
            "Kirmani Ahmed",
            "Babak Damavandi",
            "Rakesh Wanga",
            "Anuj Kumar",
            "Rohit Patel",
            "Xin Luna Dong"
        ],
        "title": "WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios",
        "abstract": "arXiv:2511.22154v1 Announce Type: new  Abstract: We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.",
        "arxiv_id": "2511.22154",
        "ARXIVID": "2511.22154",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark (WearVQA) for visual question answering in wearable devices.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.23377": {
        "authors": [
            "Rui Zhang",
            "Hongxia Wang",
            "Hangqing Liu",
            "Yang Zhou",
            "Qiang Zeng"
        ],
        "title": "DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline",
        "abstract": "arXiv:2511.23377v1 Announce Type: new  Abstract: Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.",
        "arxiv_id": "2511.23377",
        "ARXIVID": "2511.23377",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) due to the use of a Visual Foundation Model (VFM) for diffusion-based image editing localization.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.22471": {
        "authors": [
            "Zhenglin Huang",
            "Jason Li",
            "Haiquan Wen",
            "Tianxiao Li",
            "Xi Yang",
            "Lu Qi",
            "Bei Peng",
            "Xiaowei Huang",
            "Ming-Hsuan Yang",
            "Guangliang Cheng"
        ],
        "title": "Rethinking Cross-Generator Image Forgery Detection through DINOv3",
        "abstract": "arXiv:2511.22471v1 Announce Type: new  Abstract: As generative models become increasingly diverse and powerful, cross-generator detection has emerged as a new challenge. Existing detection methods often memorize artifacts of specific generative models rather than learning transferable cues, leading to substantial failures on unseen generators. Surprisingly, this work finds that frozen visual foundation models, especially DINOv3, already exhibit strong cross-generator detection capability without any fine-tuning. Through systematic studies on frequency, spatial, and token perspectives, we observe that DINOv3 tends to rely on global, low-frequency structures as weak but transferable authenticity cues instead of high-frequency, generator-specific artifacts. Motivated by this insight, we introduce a simple, training-free token-ranking strategy followed by a lightweight linear probe to select a small subset of authenticity-relevant tokens. This token subset consistently improves detection accuracy across all evaluated datasets. Our study provides empirical evidence and a feasible hypothesis for understanding why foundation models generalize across diverse generators, offering a universal, efficient, and interpretable baseline for image forgery detection.",
        "arxiv_id": "2511.22471",
        "ARXIVID": "2511.22471",
        "COMMENT": "Does not match any specific criterion but discusses cross-generator image forgery detection, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22242": {
        "authors": [
            "Qingtao Yu",
            "Changlin Song",
            "Minghao Sun",
            "Zhengyang Yu",
            "Vinay Kumar Verma",
            "Soumya Roy",
            "Sumit Negi",
            "Hongdong Li",
            "Dylan Campbell"
        ],
        "title": "TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning",
        "abstract": "arXiv:2511.22242v1 Announce Type: new  Abstract: A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.",
        "arxiv_id": "2511.22242",
        "ARXIVID": "2511.22242",
        "COMMENT": "Does not match any specific criterion but discusses test-time scaling for diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22167": {
        "authors": [
            "Bo Chen",
            "Tao Liu",
            "Qi Chen",
            "Xie Chen",
            "Zilong Zheng"
        ],
        "title": "IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer",
        "abstract": "arXiv:2511.22167v1 Announce Type: new  Abstract: Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.",
        "arxiv_id": "2511.22167",
        "ARXIVID": "2511.22167",
        "COMMENT": "Does not match any specific criterion but discusses audio-driven talking face generation, which is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22249": {
        "authors": [
            "Bolin Lai",
            "Xudong Wang",
            "Saketh Rambhatla",
            "James M. Rehg",
            "Zsolt Kira",
            "Rohit Girdhar",
            "Ishan Misra"
        ],
        "title": "Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective",
        "abstract": "arXiv:2511.22249v1 Announce Type: new  Abstract: Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.",
        "arxiv_id": "2511.22249",
        "ARXIVID": "2511.22249",
        "COMMENT": "Does not match any specific criterion but discusses latent diffusion improvements, which are tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22009": {
        "authors": [
            "Sen Fang",
            "Hongbin Zhong",
            "Yalin Feng",
            "Dimitris N. Metaxas"
        ],
        "title": "StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation",
        "abstract": "arXiv:2511.22009v1 Announce Type: new  Abstract: New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.",
        "arxiv_id": "2511.22009",
        "ARXIVID": "2511.22009",
        "COMMENT": "Does not match any specific criterion but discusses generative modeling improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22533": {
        "authors": [
            "Mengyu Yang",
            "Yanming Yang",
            "Chenyi Xu",
            "Chenxi Song",
            "Yufan Zuo",
            "Tong Zhao",
            "Ruibo Li",
            "Chi Zhang"
        ],
        "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
        "abstract": "arXiv:2511.22533v1 Announce Type: new  Abstract: Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
        "arxiv_id": "2511.22533",
        "ARXIVID": "2511.22533",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of 3D geometry synthesis and acceleration techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22064": {
        "authors": [
            "Tsai-Ling Huang",
            "Nhat-Tuong Do-Tran",
            "Ngoc-Hoang-Lam Le",
            "Hong-Han Shuai",
            "Ching-Chun Huang"
        ],
        "title": "DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation",
        "abstract": "arXiv:2511.22064v1 Announce Type: new  Abstract: Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.",
        "arxiv_id": "2511.22064",
        "ARXIVID": "2511.22064",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and handwriting synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22345": {
        "authors": [
            "Yang Chen",
            "Xiaowei Xu",
            "Shuai Wang",
            "Chenhui Zhu",
            "Ruxue Wen",
            "Xubin Li",
            "Tiezheng Ge",
            "Limin Wang"
        ],
        "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
        "abstract": "arXiv:2511.22345v1 Announce Type: new  Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\\times$64 and 256$\\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.",
        "arxiv_id": "2511.22345",
        "ARXIVID": "2511.22345",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.23052": {
        "authors": [
            "Grigorios Aris Cheimariotis",
            "Antonis Karakottas",
            "Vangelis Chatzis",
            "Angelos Kanlis",
            "Dimitrios Zarpalas"
        ],
        "title": "Image Valuation in NeRF-based 3D reconstruction",
        "abstract": "arXiv:2511.23052v1 Announce Type: new  Abstract: Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media. In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output. Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images. However, in-the-wild scenes often include image captures of varying quality, occlusions, and transient objects, resulting in uneven utility across inputs. In this paper we propose a method to quantify the individual contribution of each image to NeRF-based reconstructions of in-the-wild image sets. Contribution is assessed through reconstruction quality metrics based on PSNR and MSE. We validate our approach by removing low-contributing images during training and measuring the resulting impact on reconstruction fidelity.",
        "arxiv_id": "2511.23052",
        "ARXIVID": "2511.23052",
        "COMMENT": "Does not match any specific criterion but discusses NeRF-based 3D reconstruction, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.22237": {
        "authors": [
            "Qi Song",
            "Ziyuan Luo",
            "Renjie Wan"
        ],
        "title": "Creating Blank Canvas Against AI-enabled Image Forgery",
        "abstract": "arXiv:2511.22237v1 Announce Type: new  Abstract: AIGC-based image editing technology has greatly simplified the realistic-level image modification, causing serious potential risks of image forgery. This paper introduces a new approach to tampering detection using the Segment Anything Model (SAM). Instead of training SAM to identify tampered areas, we propose a novel strategy. The entire image is transformed into a blank canvas from the perspective of neural models. Any modifications to this blank canvas would be noticeable to the models. To achieve this idea, we introduce adversarial perturbations to prevent SAM from ``seeing anything'', allowing it to identify forged regions when the image is tampered with. Due to SAM's powerful perceiving capabilities, naive adversarial attacks cannot completely tame SAM. To thoroughly deceive SAM and make it blind to the image, we introduce a frequency-aware optimization strategy, which further enhances the capability of tamper localization. Extensive experimental results demonstrate the effectiveness of our method.",
        "arxiv_id": "2511.22237",
        "ARXIVID": "2511.22237",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and adversarial techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.23071": {
        "authors": [
            "Anik De",
            "Abhirama Subramanyam Penamakuri",
            "Rajeev Yadav",
            "Aditya Rathore",
            "Harshiv Shah",
            "Devesh Sharma",
            "Sagar Agarwal",
            "Pravin Kumar",
            "Anand Mishra"
        ],
        "title": "Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding",
        "abstract": "arXiv:2511.23071v1 Announce Type: new  Abstract: Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.",
        "arxiv_id": "2511.23071",
        "ARXIVID": "2511.23071",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and datasets.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.22147": {
        "authors": [
            "Yanping Li",
            "Zhening Liu",
            "Zijian Li",
            "Zehong Lin",
            "Jun Zhang"
        ],
        "title": "RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks",
        "abstract": "arXiv:2511.22147v1 Announce Type: new  Abstract: As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.",
        "arxiv_id": "2511.22147",
        "ARXIVID": "2511.22147",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of 3D reconstruction and defense mechanisms.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.22131": {
        "authors": [
            "Xilin Yang",
            "Musa Aydin",
            "Yuhong Lu",
            "Sahan Yoruc Selcuk",
            "Bijie Bai",
            "Yijie Zhang",
            "Andrew Birkeland",
            "Katjana Ehrlich",
            "Julien Bec",
            "Laura Marcu",
            "Nir Pillar",
            "Aydogan Ozcan"
        ],
        "title": "Autonomous labeling of surgical resection margins using a foundation model",
        "abstract": "arXiv:2511.22131v1 Announce Type: new  Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.",
        "arxiv_id": "2511.22131",
        "ARXIVID": "2511.22131",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
{
    "2512.22207": {
        "authors": [
            "Ryan Spencer",
            "Roey Yaari",
            "Ritvik Vemavarapu",
            "Joyce Yang",
            "Steven Ngo",
            "Utkarsh Sharma"
        ],
        "title": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks",
        "abstract": "arXiv:2512.22207v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.",
        "arxiv_id": "2512.22207",
        "ARXIVID": "2512.22207",
        "COMMENT": "Matches criterion 1 and 6. Introduces a benchmark for spatial reasoning and 2D-to-3D planning in multimodal large language models, which aligns with spatial intelligence and video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.23328": {
        "authors": [
            "Huan-ang Gao",
            "Zikang Zhang",
            "Tianwei Luo",
            "Kaisen Yang",
            "Xinzhe Juan",
            "Jiahao Qiu",
            "Tianxing Chen",
            "Bingxiang He",
            "Hao Zhao",
            "Hao Zhou",
            "Shilong Liu",
            "Mengdi Wang"
        ],
        "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
        "abstract": "arXiv:2512.23328v1 Announce Type: new  Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.",
        "arxiv_id": "2512.23328",
        "ARXIVID": "2512.23328",
        "COMMENT": "Matches criterion 1 as it introduces CubeBench, a benchmark for spatial reasoning and embodied agent evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.23412": {
        "authors": [
            "Jiawei Chen",
            "Xintian Shen",
            "Lihao Zheng",
            "Zhenwei Shao",
            "Hongyuan Zhang",
            "Pengfei Yu",
            "Xudong Rao",
            "Ning Mao",
            "Xiaobo Liu",
            "Lian Wen",
            "Chaoqun Du",
            "Feng Gu",
            "Wei He",
            "Qizhen Li",
            "Shanshan Li",
            "Zide Liu",
            "Jing Luo",
            "Lifu Mu",
            "Xuhao Pan",
            "Chang Ren",
            "Haoyi Sun",
            "Qian Wang",
            "Wei Wang",
            "Hongfu Yang",
            "Jiqing Zhan",
            "Chunpeng Zhou",
            "Zheng Zhou",
            "Hao Ma",
            "Tao Wei",
            "Pan Zhou",
            "Wei Chen"
        ],
        "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
        "abstract": "arXiv:2512.23412v1 Announce Type: new  Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.",
        "arxiv_id": "2512.23412",
        "ARXIVID": "2512.23412",
        "COMMENT": "Matches criterion 2 and 5. Focuses on multimodal chain-of-thought reasoning and tool-integrated reasoning, which aligns with visual and multimodal large language models and their integration with reasoning tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.22673": {
        "authors": [
            "Xiang Cheng",
            "Yulan Hu",
            "Xiangwen Zhang",
            "Lu Xu",
            "Zheng Pan",
            "Xin Li",
            "Yong Liu"
        ],
        "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning",
        "abstract": "arXiv:2512.22673v1 Announce Type: new  Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.",
        "arxiv_id": "2512.22673",
        "ARXIVID": "2512.22673",
        "COMMENT": "Matches criterion 3 as it introduces TravelBench, a benchmark for multi-turn and tool-augmented planning, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.23292": {
        "authors": [
            "Yoonpyo Lee",
            "Kazuma Kobayashi",
            "Sai Puppala",
            "Sajedul Talukder",
            "Seid Koric",
            "Souvik Chakraborty",
            "Syed Bahauddin Alam"
        ],
        "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control",
        "abstract": "arXiv:2512.23292v1 Announce Type: new  Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.",
        "arxiv_id": "2512.23292",
        "ARXIVID": "2512.23292",
        "COMMENT": "Matches criterion 3 as it introduces a novel domain-specific foundation model for embodied AI in nuclear reactor control.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.22336": {
        "authors": [
            "Mengkang Hu",
            "Bowei Xia",
            "Yuran Wu",
            "Ailing Yu",
            "Yude Zou",
            "Qiguang Chen",
            "Shijian Wang",
            "Jiarui Jin",
            "Kexin Li",
            "Wenxiang Jiao",
            "Yuan Lu",
            "Ping Luo"
        ],
        "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback",
        "abstract": "arXiv:2512.22336v1 Announce Type: new  Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.",
        "arxiv_id": "2512.22336",
        "ARXIVID": "2512.22336",
        "COMMENT": "Matches criterion 3. Introduces a novel framework for generating symbolic world models with multi-agent feedback, which is relevant to embodied/robotic AI methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22605": {
        "authors": [
            "Junshu Dai",
            "Yu Wang",
            "Tongya Zheng",
            "Wei Ji",
            "Qinghong Guo",
            "Ji Cao",
            "Jie Song",
            "Canghong Jin",
            "Mingli Song"
        ],
        "title": "Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation",
        "abstract": "arXiv:2512.22605v1 Announce Type: new  Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \\textbf{M}ulti-\\textbf{M}odal \\textbf{Mob}ility (\\textbf{M}$^3$\\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.",
        "arxiv_id": "2512.22605",
        "ARXIVID": "2512.22605",
        "COMMENT": "Matches criterion 2 as it explores multi-modal spatial-temporal knowledge integration for mobility dynamics, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22568": {
        "authors": [
            "Rajesh P. N. Rao",
            "Vishwas Sathish",
            "Linxing Preston Jiang",
            "Matthew Bryan",
            "Prashant Rangarajan"
        ],
        "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI",
        "abstract": "arXiv:2512.22568v1 Announce Type: new  Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.",
        "arxiv_id": "2512.22568",
        "ARXIVID": "2512.22568",
        "COMMENT": "Matches criterion 1. Discusses integrating actions, compositional structure, and memory for human-like AI, which aligns with spatial intelligence and embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2512.22933": {
        "authors": [
            "Danni Xu",
            "Shaojing Fan",
            "Xuanang Cheng",
            "Mohan Kankanhalli"
        ],
        "title": "Multimodal Fact-Checking: An Agent-based Approach",
        "abstract": "arXiv:2512.22933v1 Announce Type: new  Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.",
        "arxiv_id": "2512.22933",
        "ARXIVID": "2512.22933",
        "COMMENT": "Matches criterion 2. Focuses on multimodal fact-checking and introduces a framework (AgentFact) leveraging vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.22899": {
        "authors": [
            "Yaping Zhang",
            "Qixuan Zhang",
            "Xingquan Zhang",
            "Zhiyuan Chen",
            "Wenwen Zhuang",
            "Yupu Liang",
            "Lu Xiang",
            "Yang Zhao",
            "Jiajun Zhang",
            "Yu Zhou",
            "Chengqing Zong"
        ],
        "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery",
        "abstract": "arXiv:2512.22899v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \\textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \\textit{Scientific Literacy} (L1), \\textit{Literature Parsing} (L2), \\textit{Literature-based Question Answering} (L3), \\textit{Literature Review Generation} (L4), and \\textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\\% accuracy on basic literacy tasks, performance declines sharply to 25\\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.",
        "arxiv_id": "2512.22899",
        "ARXIVID": "2512.22899",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (HiSciBench) for evaluating scientific intelligence, which aligns with embodied/robotic AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.23090": {
        "authors": [
            "Armin Berger",
            "Manuela Bergau",
            "Helen Schneider",
            "Saad Ahmad",
            "Tom Anglim Lagones",
            "Gianluca Brugnara",
            "Martha Foltyn-Dumitru",
            "Kai Schlamp",
            "Philipp Vollmuth",
            "Rafet Sifa"
        ],
        "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
        "abstract": "arXiv:2512.23090v1 Announce Type: new  Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.",
        "arxiv_id": "2512.23090",
        "ARXIVID": "2512.23090",
        "COMMENT": "Matches criterion 4. Discusses reinforcement learning in vision-language models for medical imaging, which aligns with vision foundation models and applications.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2512.22255": {
        "authors": [
            "Abhranil Chandra",
            "Ayush Agrawal",
            "Arian Hosseini",
            "Sebastian Fischmeister",
            "Rishabh Agarwal",
            "Navin Goyal",
            "Aaron Courville"
        ],
        "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
        "abstract": "arXiv:2512.22255v1 Announce Type: new  Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.",
        "arxiv_id": "2512.22255",
        "ARXIVID": "2512.22255",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning tasks and synthetic datasets but not in a vision or multimodal context.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.23167": {
        "authors": [
            "Yifan Zhang",
            "Giridhar Ganapavarapu",
            "Srideepika Jayaraman",
            "Bhavna Agrawal",
            "Dhaval Patel",
            "Achille Fokoue"
        ],
        "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search",
        "abstract": "arXiv:2512.23167v1 Announce Type: new  Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.",
        "arxiv_id": "2512.23167",
        "ARXIVID": "2512.23167",
        "COMMENT": "Does not match any specific criteria. Focuses on symbolic planning and LLMs but not in a vision or embodied context.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.22334": {
        "authors": [
            "Yiheng Wang",
            "Yixin Chen",
            "Shuo Li",
            "Yifan Zhou",
            "Bo Liu",
            "Hengjian Gao",
            "Jiakang Yuan",
            "Jia Bu",
            "Wanghan Xu",
            "Yuhao Zhou",
            "Xiangyu Zhao",
            "Zhiwang Zhou",
            "Fengxiang Wang",
            "Haodong Duan",
            "Songyang Zhang",
            "Jun Yao",
            "Han Deng",
            "Yizhou Wang",
            "Jiabei Xiao",
            "Jiaqi Liu",
            "Encheng Su",
            "Yujie Liu",
            "Weida Wang",
            "Junchi Yao",
            "Shenghe Zheng",
            "Haoran Sun",
            "Runmin Ma",
            "Xiangchao Yan",
            "Bo Zhang",
            "Dongzhan Zhou",
            "Shufei Zhang",
            "Peng Ye",
            "Xiaosong Wang",
            "Shixiang Tang",
            "Wenlong Zhang",
            "Lei Bai"
        ],
        "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
        "abstract": "arXiv:2512.22334v1 Announce Type: new  Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
        "arxiv_id": "2512.22334",
        "ARXIVID": "2512.22334",
        "COMMENT": "Does not match any specific criteria. Focuses on benchmarking AI models for scientific intelligence, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22258": {
        "authors": [
            "Satvik Tripathi"
        ],
        "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method",
        "abstract": "arXiv:2512.22258v1 Announce Type: new  Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.",
        "arxiv_id": "2512.22258",
        "ARXIVID": "2512.22258",
        "COMMENT": "Does not match any specific criteria. Focuses on deterministic and interpretable prompting methods for LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
{
    "2512.15160": {
        "authors": [
            "Jiaxu Wan",
            "Xu Wang",
            "Mengwei Xie",
            "Hang Zhang",
            "Mu Xu",
            "Yang Han",
            "Hong Zhang",
            "Ding Yuan",
            "Yifan Yang"
        ],
        "title": "EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence",
        "abstract": "arXiv:2512.15160v1 Announce Type: new  Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for \"thinking with images\" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.",
        "arxiv_id": "2512.15160",
        "ARXIVID": "2512.15160",
        "COMMENT": "Matches criterion 1 as it introduces a novel framework for spatial intelligence using BEV-grounded reasoning and reinforcement learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.14878": {
        "authors": [
            "Wenshuo Li",
            "Majid Mirmehdi",
            "Tilo Burghardt"
        ],
        "title": "Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris",
        "abstract": "arXiv:2512.14878v1 Announce Type: new  Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.",
        "arxiv_id": "2512.14878",
        "ARXIVID": "2512.14878",
        "COMMENT": "Matches criterion 5. Combines image understanding tasks with textual descriptors for animal biometrics, showcasing integration of image and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.15649": {
        "authors": [
            "Hongbo Zhao",
            "Meng Wang",
            "Fei Zhu",
            "Wenzhuo Liu",
            "Bolin Ni",
            "Fanhu Zeng",
            "Gaofeng Meng",
            "Zhaoxiang Zhang"
        ],
        "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
        "abstract": "arXiv:2512.15649v1 Announce Type: new  Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
        "arxiv_id": "2512.15649",
        "ARXIVID": "2512.15649",
        "COMMENT": "Matches criterion 2. Explores vision-language models (VLMs) and their ability to handle long-context understanding with vision-text compression, which aligns with the exploration of VLLMs and MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.15716": {
        "authors": [
            "Jinjing Zhao",
            "Fangyun Wei",
            "Zhening Liu",
            "Hongyang Zhang",
            "Chang Xu",
            "Yan Lu"
        ],
        "title": "Spatia: Video Generation with Updatable Spatial Memory",
        "abstract": "arXiv:2512.15716v1 Announce Type: new  Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
        "arxiv_id": "2512.15716",
        "ARXIVID": "2512.15716",
        "COMMENT": "Matches criterion 6 as it proposes a novel video generation framework with spatial memory, addressing long-term spatial and temporal consistency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.15693": {
        "authors": [
            "Yifei Li",
            "Wenzhao Zheng",
            "Yanran Zhang",
            "Runze Sun",
            "Yu Zheng",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
        "abstract": "arXiv:2512.15693v1 Announce Type: new  Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
        "arxiv_id": "2512.15693",
        "ARXIVID": "2512.15693",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model for AI-generated video detection, focusing on artifact reasoning and explainability.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.15431": {
        "authors": [
            "Haolong Yan",
            "Jia Wang",
            "Xin Huang",
            "Yeqing Shen",
            "Ziyang Meng",
            "Zhimin Fan",
            "Kaijun Tan",
            "Jin Gao",
            "Lieyu Shi",
            "Mi Yang",
            "Shiliang Yang",
            "Zhirui Wang",
            "Brian Li",
            "Kang An",
            "Chenyang Li",
            "Lei Lei",
            "Mengmeng Duan",
            "Danxun Liang",
            "Guodong Liu",
            "Hang Cheng",
            "Hao Wu",
            "Jie Dong",
            "Junhao Huang",
            "Mei Chen",
            "Renjie Yu",
            "Shunshan Li",
            "Xu Zhou",
            "Yiting Dai",
            "Yineng Deng",
            "Yingdan Liang",
            "Zelin Chen",
            "Wen Sun",
            "Chengxu Yan",
            "Chunqin Xu",
            "Dong Li",
            "Fengqiong Xiao",
            "Guanghao Fan",
            "Guopeng Li",
            "Guozhen Peng",
            "Hongbing Li",
            "Hang Li",
            "Hongming Chen",
            "Jingjing Xie",
            "Jianyong Li",
            "Jingyang Zhang",
            "Jiaju Ren",
            "Jiayu Yuan",
            "Jianpeng Yin",
            "Kai Cao",
            "Liang Zhao",
            "Liguo Tan",
            "Liying Shi",
            "Mengqiang Ren",
            "Min Xu",
            "Manjiao Liu",
            "Mao Luo",
            "Mingxin Wan",
            "Na Wang",
            "Nan Wu",
            "Ning Wang",
            "Peiyao Ma",
            "Qingzhou Zhang",
            "Qiao Wang",
            "Qinlin Zeng",
            "Qiong Gao",
            "Qiongyao Li",
            "Shangwu Zhong",
            "Shuli Gao",
            "Shaofan Liu",
            "Shisi Gao",
            "Shuang Luo",
            "Xingbin Liu",
            "Xiaojia Liu",
            "Xiaojie Hou",
            "Xin Liu",
            "Xuanti Feng",
            "Xuedan Cai",
            "Xuan Wen",
            "Xianwei Zhu",
            "Xin Liang",
            "Xin Liu",
            "Xin Zhou",
            "Yingxiu Zhao",
            "Yukang Shi",
            "Yunfang Xu",
            "Yuqing Zeng",
            "Yixun Zhang",
            "Zejia Weng",
            "Zhonghao Yan",
            "Zhiguo Huang",
            "Zhuoyu Wang",
            "Zheng Ge",
            "Jing Li",
            "Yibo Zhu",
            "Binxing Jiao",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "title": "Step-GUI Technical Report",
        "abstract": "arXiv:2512.15431v1 Announce Type: new  Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
        "arxiv_id": "2512.15431",
        "ARXIVID": "2512.15431",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (AndroidDaily) and methods for GUI automation, relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.15644": {
        "authors": [
            "Qirui Li",
            "Yizhe Tang",
            "Ran Yi",
            "Guangben Lu",
            "Fangyuan Zou",
            "Peng Shu",
            "Huan Yu",
            "Jie Jiang"
        ],
        "title": "InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization",
        "abstract": "arXiv:2512.15644v1 Announce Type: new  Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.",
        "arxiv_id": "2512.15644",
        "ARXIVID": "2512.15644",
        "COMMENT": "Matches criterion 1. Proposes a method to address spatial relationship hallucinations in foreground-conditioned inpainting, which relates to spatial reasoning in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.15508": {
        "authors": [
            "Arthur Moreau",
            "Richard Shaw",
            "Michal Nazarczuk",
            "Jisu Shin",
            "Thomas Tanay",
            "Zhensong Zhang",
            "Songcen Xu",
            "Eduardo P\\'erez-Pellitero"
        ],
        "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
        "abstract": "arXiv:2512.15508v1 Announce Type: new  Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
        "arxiv_id": "2512.15508",
        "ARXIVID": "2512.15508",
        "COMMENT": "Matches criterion 4 as it proposes a novel feed-forward architecture for 3D Gaussian splatting, improving efficiency and quality in 3D scene generation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.15524": {
        "authors": [
            "Yuxiang Shi",
            "Zhe Li",
            "Yanwen Wang",
            "Hao Zhu",
            "Xun Cao",
            "Ligang Liu"
        ],
        "title": "DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations",
        "abstract": "arXiv:2512.15524v1 Announce Type: new  Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.",
        "arxiv_id": "2512.15524",
        "ARXIVID": "2512.15524",
        "COMMENT": "Matches criterion 5 as it focuses on disentangled portrait animation using explicit and latent motion representations, combining image generation and multimodal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.15707": {
        "authors": [
            "Yu Wang",
            "Juhyung Ha",
            "Frangil M. Ramirez",
            "Yuchen Wang",
            "David J. Crandall"
        ],
        "title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection",
        "abstract": "arXiv:2512.15707v1 Announce Type: new  Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.",
        "arxiv_id": "2512.15707",
        "ARXIVID": "2512.15707",
        "COMMENT": "Matches criterion 2 as it explores multimodal fusion for active speaker detection, leveraging strong pretrained encoders and novel fusion techniques.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.15505": {
        "authors": [
            "Rohit Jena",
            "Pratik Chaudhari",
            "James C. Gee"
        ],
        "title": "The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge",
        "abstract": "arXiv:2512.15505v1 Announce Type: new  Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.",
        "arxiv_id": "2512.15505",
        "ARXIVID": "2512.15505",
        "COMMENT": "Matches criterion 3. Introduces a re-evaluation of a benchmark (LUMIR challenge) for deformable image registration, which is relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.15603": {
        "authors": [
            "Shengming Yin",
            "Zekai Zhang",
            "Zecheng Tang",
            "Kaiyuan Gao",
            "Xiao Xu",
            "Kun Yan",
            "Jiahao Li",
            "Yilei Chen",
            "Yuxiang Chen",
            "Heung-Yeung Shum",
            "Lionel M. Ni",
            "Jingren Zhou",
            "Junyang Lin",
            "Chenfei Wu"
        ],
        "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
        "abstract": "arXiv:2512.15603v1 Announce Type: new  Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \\href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}",
        "arxiv_id": "2512.15603",
        "ARXIVID": "2512.15603",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on layered image decomposition and inherent editability, combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.15423": {
        "authors": [
            "Hoang Nguyen",
            "Xiaohao Xu",
            "Xiaonan Huang"
        ],
        "title": "Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry",
        "abstract": "arXiv:2512.15423v1 Announce Type: new  Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.",
        "arxiv_id": "2512.15423",
        "ARXIVID": "2512.15423",
        "COMMENT": "Matches criterion 4 as it addresses robustness in monocular depth foundation models, focusing on structural and contextual evaluation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.15581": {
        "authors": [
            "Shashank Mishra",
            "Karan Patil",
            "Didier Stricker",
            "Jason Rambach"
        ],
        "title": "IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion",
        "abstract": "arXiv:2512.15581v1 Announce Type: new  Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.",
        "arxiv_id": "2512.15581",
        "ARXIVID": "2512.15581",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for radar-camera fusion in embodied AI, focusing on knowledge distillation and spatial reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2512.15311": {
        "authors": [
            "Wenke E",
            "Yixin Sun",
            "Jiaxu Liu",
            "Hubert P. H. Shum",
            "Amir Atapour-Abarghouei",
            "Toby P. Breckon"
        ],
        "title": "KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation",
        "abstract": "arXiv:2512.15311v1 Announce Type: new  Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.",
        "arxiv_id": "2512.15311",
        "ARXIVID": "2512.15311",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a cross-modality distillation framework for BEV segmentation, which is relevant to autonomous driving and embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.15153": {
        "authors": [
            "Mengshi Qi",
            "Yeteng Wu",
            "Xianlin Zhang",
            "Huadong Ma"
        ],
        "title": "Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning",
        "abstract": "arXiv:2512.15153v1 Announce Type: new  Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.",
        "arxiv_id": "2512.15153",
        "ARXIVID": "2512.15153",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a new dataset and method for action form assessment, which involves video-based tasks and multimodal reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.14938": {
        "authors": [
            "Zhenzhi Wang",
            "Jian Wang",
            "Ke Ma",
            "Dahua Lin",
            "Bing Zhou"
        ],
        "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
        "abstract": "arXiv:2512.14938v1 Announce Type: new  Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/",
        "arxiv_id": "2512.14938",
        "ARXIVID": "2512.14938",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a dataset and methods for audio-driven video generation, which involves video-based tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.15138": {
        "authors": [
            "Shengxiao Zhou",
            "Chenghua Li",
            "Jianhao Huang",
            "Qinghao Hu",
            "Yifan Zhang"
        ],
        "title": "Borrowing from anything: A generalizable framework for reference-guided instance editing",
        "abstract": "arXiv:2512.15138v1 Announce Type: new  Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.",
        "arxiv_id": "2512.15138",
        "ARXIVID": "2512.15138",
        "COMMENT": "This paper does not directly match any of the specific criteria. While it discusses spatial alignment and disentanglement in instance editing, it does not focus on embodied agents, vision-language models, or video understanding tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15340": {
        "authors": [
            "Junjie Chen",
            "Fei Wang",
            "Zhihao Huang",
            "Qing Zhou",
            "Kun Li",
            "Dan Guo",
            "Linfeng Zhang",
            "Xun Yang"
        ],
        "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
        "abstract": "arXiv:2512.15340v1 Announce Type: new  Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fr\\'echet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
        "arxiv_id": "2512.15340",
        "ARXIVID": "2512.15340",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal and interactive AI, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14228": {
        "authors": [
            "Aneesha Fernando",
            "Surangika Ranathunga",
            "Kristin Stock",
            "Raj Prasanna",
            "Christopher B. Jones"
        ],
        "title": "Georeferencing complex relative locality descriptions with large language models",
        "abstract": "arXiv:2512.14228v1 Announce Type: new  Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.",
        "arxiv_id": "2512.14228",
        "ARXIVID": "2512.14228",
        "COMMENT": "Does not match any specific criteria. Focuses on georeferencing text documents using LLMs, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.14755": {
        "authors": [
            "Paul Weinmann",
            "Ferdinand Schenck",
            "Martin \\v{S}iklar"
        ],
        "title": "SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation",
        "abstract": "arXiv:2512.14755v1 Announce Type: new  Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.",
        "arxiv_id": "2512.14755",
        "ARXIVID": "2512.14755",
        "COMMENT": "Does not match any specific criteria. Focuses on SAR and optical data for change detection, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15009": {
        "authors": [
            "Yunseong Nam",
            "Jiwon Jang",
            "Dongkyu Won",
            "Sang Hyun Park",
            "Soopil Kim"
        ],
        "title": "Model Agnostic Preference Optimization for Medical Image Segmentation",
        "abstract": "arXiv:2512.15009v1 Announce Type: new  Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.",
        "arxiv_id": "2512.15009",
        "ARXIVID": "2512.15009",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image segmentation with preference optimization, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15369": {
        "authors": [
            "Maximilian Kellner",
            "Mariana Ferrandon Cervantes",
            "Yuandong Pan",
            "Ruodan Lu",
            "Ioannis Brilakis",
            "Alexander Reiterer"
        ],
        "title": "SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis",
        "abstract": "arXiv:2512.15369v1 Announce Type: new  Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.",
        "arxiv_id": "2512.15369",
        "ARXIVID": "2512.15369",
        "COMMENT": "Does not match any specific criteria but introduces a dataset for 3D semantic segmentation of bridges, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15396": {
        "authors": [
            "Liang Peng",
            "Yixuan Ye",
            "Cheng Liu",
            "Hangjun Che",
            "Fei Wang",
            "Zhiwen Yu",
            "Si Wu",
            "Hau-San Wong"
        ],
        "title": "SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering",
        "abstract": "arXiv:2512.15396v1 Announce Type: new  Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.",
        "arxiv_id": "2512.15396",
        "ARXIVID": "2512.15396",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multi-view learning and clustering, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.15055": {
        "authors": [
            "Yifei Bian",
            "Banglei Guan",
            "Zibin Liu",
            "Ang Su",
            "Shiyao Zhu",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "title": "Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement",
        "abstract": "arXiv:2512.15055v1 Announce Type: new  Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.",
        "arxiv_id": "2512.15055",
        "ARXIVID": "2512.15055",
        "COMMENT": "Does not match any specific criteria but is related to event-based vision, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.15512": {
        "authors": [
            "Opeyemi Bamigbade",
            "Mark Scanlon",
            "John Sheppard"
        ],
        "title": "VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics",
        "abstract": "arXiv:2512.15512v1 Announce Type: new  Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.",
        "arxiv_id": "2512.15512",
        "ARXIVID": "2512.15512",
        "COMMENT": "Does not match any specific criteria but is related to image manipulation detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.15319": {
        "authors": [
            "Yuxin Jiang",
            "Yunkang Cao",
            "Weiming Shen"
        ],
        "title": "Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection",
        "abstract": "arXiv:2512.15319v1 Announce Type: new  Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.",
        "arxiv_id": "2512.15319",
        "ARXIVID": "2512.15319",
        "COMMENT": "Does not match any specific criteria but is related to few-shot anomaly detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.15326": {
        "authors": [
            "Yuxin Jiang",
            "Yunkang Can",
            "Weiming Shen"
        ],
        "title": "A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection",
        "abstract": "arXiv:2512.15326v1 Announce Type: new  Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.",
        "arxiv_id": "2512.15326",
        "ARXIVID": "2512.15326",
        "COMMENT": "Does not match any specific criteria but is related to image anomaly detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
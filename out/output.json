{
    "2601.16973": {
        "authors": [
            "Zirui Wang",
            "Junyi Zhang",
            "Jiaxin Ge",
            "Long Lian",
            "Letian Fu",
            "Lisa Dunlap",
            "Ken Goldberg",
            "XuDong Wang",
            "Ion Stoica",
            "David M. Chan",
            "Sewon Min",
            "Joseph E. Gonzalez"
        ],
        "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
        "abstract": "arXiv:2601.16973v1 Announce Type: new  Abstract: Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
        "arxiv_id": "2601.16973",
        "ARXIVID": "2601.16973",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) and criterion 2 (Visual and Multimodal Large Language Models) as it introduces VisGym, a benchmark for evaluating and training multimodal agents in interactive environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.16965": {
        "authors": [
            "Riyang Bao",
            "Cheng Yang",
            "Dazhou Yu",
            "Zhexiang Tang",
            "Gengchen Mai",
            "Liang Zhao"
        ],
        "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts",
        "abstract": "arXiv:2601.16965v1 Announce Type: new  Abstract: Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.",
        "arxiv_id": "2601.16965",
        "ARXIVID": "2601.16965",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces Spatial-Agent, a novel method for geospatial reasoning with interpretable workflows.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16964": {
        "authors": [
            "Mohamed Amine Ferrag",
            "Abderrahmane Lakas",
            "Merouane Debbah"
        ],
        "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems",
        "abstract": "arXiv:2601.16964v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive",
        "arxiv_id": "2601.16964",
        "ARXIVID": "2601.16964",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for agentic AI reasoning in autonomous systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.16449": {
        "authors": [
            "Xiaojiang Peng",
            "Jingyi Chen",
            "Zebang Cheng",
            "Bao Peng",
            "Fengyi Wu",
            "Yifei Dong",
            "Shuyuan Tu",
            "Qiyu Hu",
            "Huiting Huang",
            "Yuxiang Lin",
            "Jun-Yan He",
            "Kai Wang",
            "Zheng Lian",
            "Zhi-Qi Cheng"
        ],
        "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
        "abstract": "arXiv:2601.16449v1 Announce Type: new  Abstract: Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.",
        "arxiv_id": "2601.16449",
        "ARXIVID": "2601.16449",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models for emotion understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.16982": {
        "authors": [
            "Basile Van Hoorick",
            "Dian Chen",
            "Shun Iwase",
            "Pavel Tokmakov",
            "Muhammad Zubair Irshad",
            "Igor Vasiljevic",
            "Swati Gupta",
            "Fangzhou Cheng",
            "Sergey Zakharov",
            "Vitor Campagnolo Guizilini"
        ],
        "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
        "abstract": "arXiv:2601.16982v1 Announce Type: new  Abstract: Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/",
        "arxiv_id": "2601.16982",
        "ARXIVID": "2601.16982",
        "COMMENT": "Matches criterion 6 as it focuses on dynamic view synthesis for video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.16885": {
        "authors": [
            "Yangfan Xu",
            "Lilian Zhang",
            "Xiaofeng He",
            "Pengdong Wu",
            "Wenqi Wu",
            "Jun Mao"
        ],
        "title": "GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss",
        "abstract": "arXiv:2601.16885v1 Announce Type: new  Abstract: Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.",
        "arxiv_id": "2601.16885",
        "ARXIVID": "2601.16885",
        "COMMENT": "Matches criterion 3 as it introduces a novel self-supervised method for large-scale localization in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.16933": {
        "authors": [
            "Jingran Zhang",
            "Ning Li",
            "Yuanhao Ban",
            "Andrew Bai",
            "Justin Cui"
        ],
        "title": "Reward-Forcing: Autoregressive Video Generation with Reward Feedback",
        "abstract": "arXiv:2601.16933v1 Announce Type: new  Abstract: While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.",
        "arxiv_id": "2601.16933",
        "ARXIVID": "2601.16933",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on autoregressive video generation with reward feedback, which is a novel methodology for video-based tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.16381": {
        "authors": [
            "Yuxin Jiang",
            "Yunkang Cao",
            "Yuqi Cheng",
            "Yiheng Zhang",
            "Weiming Shen"
        ],
        "title": "VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection",
        "abstract": "arXiv:2601.16381v1 Announce Type: new  Abstract: Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.",
        "arxiv_id": "2601.16381",
        "ARXIVID": "2601.16381",
        "COMMENT": "Matches criterion 5 as it focuses on a vision-text multimodal fusion framework for anomaly detection.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.16451": {
        "authors": [
            "Peixian Liang",
            "Songhao Li",
            "Shunsuke Koga",
            "Yutong Li",
            "Zahra Alipour",
            "Yucheng Tang",
            "Daguang Xu",
            "Zhi Huang"
        ],
        "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
        "abstract": "arXiv:2601.16451v1 Announce Type: new  Abstract: Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.",
        "arxiv_id": "2601.16451",
        "ARXIVID": "2601.16451",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model for pathology image segmentation and its applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.16645": {
        "authors": [
            "Minsu Gong",
            "Nuri Ryu",
            "Jungseul Ok",
            "Sunghyun Cho"
        ],
        "title": "Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss",
        "abstract": "arXiv:2601.16645v1 Announce Type: new  Abstract: Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.",
        "arxiv_id": "2601.16645",
        "ARXIVID": "2601.16645",
        "COMMENT": "Does not closely match any specific criterion but is relevant to image manipulation and diffusion models, which are tangentially related to vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2601.16429": {
        "authors": [
            "Jongmin Yu",
            "Hyeontaek Oh",
            "Zhongtian Sun",
            "Angelica I Aviles-Rivero",
            "Moongu Jeon",
            "Jinhong Yang"
        ],
        "title": "AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose",
        "abstract": "arXiv:2601.16429v1 Announce Type: new  Abstract: Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'.",
        "arxiv_id": "2601.16429",
        "ARXIVID": "2601.16429",
        "COMMENT": "Does not closely match any specific criterion but is relevant to face-swapping and vision-language integration, which is tangentially related to multimodal learning.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.16694": {
        "authors": [
            "Hongda Liu",
            "Yunfan Liu",
            "Min Ren",
            "Lin Sui",
            "Yunlong Wang",
            "Zhenan Sun"
        ],
        "title": "Affinity Contrastive Learning for Skeleton-based Human Activity Understanding",
        "abstract": "arXiv:2601.16694v1 Announce Type: new  Abstract: In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.",
        "arxiv_id": "2601.16694",
        "ARXIVID": "2601.16694",
        "COMMENT": "Does not closely match any specific criterion but is relevant to skeleton-based human activity understanding, which is tangentially related to embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2601.16725": {
        "authors": [
            "Meituan LongCat Team",
            "Anchun Gui",
            "Bei Li",
            "Bingyang Tao",
            "Bole Zhou",
            "Borun Chen",
            "Chao Zhang",
            "Chao Zhang",
            "Chen Gao",
            "Chen Zhang",
            "Chengcheng Han",
            "Chenhui Yang",
            "Chuyu Zhang",
            "Cong Chen",
            "Cunguang Wang",
            "Daoru Pan",
            "Defei Bu",
            "Dengchang Zhao",
            "Di Xiu",
            "Dishan Liu",
            "Dongyu Ru",
            "Dunwei Tu",
            "Fan Wu",
            "Fengcheng Yuan",
            "Fengcun Li",
            "Gang Xu",
            "Guanyu Wu",
            "Guoyuan Lin",
            "Haibin Wang",
            "Hansi Yang",
            "Hao Yang",
            "Haonan Yan",
            "Haoxiang Ma",
            "Haoxing Wen",
            "Hongyan Hao",
            "Hongyin Tang",
            "Hongyu Zang",
            "Hongzhi Ni",
            "Hui Su",
            "Jiacheng Zhang",
            "Jiahong Zhou",
            "Jiahuan Li",
            "Jiaming Wang",
            "Jian Yang",
            "Jianfei Zhang",
            "Jianhao Xu",
            "Jianing Wang",
            "Jiapeng Zhu",
            "Jiaqi Sun",
            "Jiarong Shi",
            "Jiarui Zhao",
            "Jingang Wang",
            "Jinluan Yang",
            "Jinrui Ding",
            "Jinwei Xiao",
            "Jiyuan He",
            "Juncan Xu",
            "Kefeng Zhang",
            "Keheng Wang",
            "Li Wei",
            "Lianhui Ma",
            "Lin Qiu",
            "Lingbing Kong",
            "Lingchuan Liu",
            "Linsen Guo",
            "Mengshen Zhu",
            "Mengxia Shen",
            "Mingyang Zhu",
            "Peiguang Li",
            "Peng Pei",
            "Pengcheng Jia",
            "Pengtao Zhang",
            "Peng Zhao",
            "Qi Gu",
            "Qiong Huang",
            "Qiyuan Duan",
            "Quanchi Weng",
            "Rongxiang Weng",
            "Rongzhi Zhang",
            "Rumei Li",
            "Shanglin Lei",
            "Shengnan An",
            "Shijun Dai",
            "Shuaikang Liu",
            "Shuang Zhou",
            "Shuo Wang",
            "Songyuan Zhao",
            "Tao Liang",
            "Tianhao Hu",
            "Tianze Chen",
            "Wei Liu",
            "Wei Shi",
            "Wei Wang",
            "Weifeng Tang",
            "Wenjie Shi",
            "Wenlong Zhu",
            "Wentao Chen",
            "Wentao Shi",
            "Xi Su",
            "Xiangcheng Liu",
            "Xiandi Ma",
            "Xiangyu Xi",
            "Xiangyuan Liu",
            "Xiangzhou Huang",
            "Xiao Liu",
            "Xiaodong Cai",
            "Xiaolong Chen",
            "Xiaowei Shi",
            "Xiaoyu Li",
            "Xin Chen",
            "Xingchen Liu",
            "Xuan Huang",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yan Chen",
            "Yang Bai",
            "Yang Liu",
            "Yang Yang",
            "Yang Zheng",
            "Yaoming Wang",
            "Yaoming Zhu",
            "Yaqi Huo",
            "Yanyu Chen",
            "Yaorui Shi",
            "Yerui Sun",
            "Yi Zhang",
            "Yihao Chen",
            "Yi-Kai Zhang",
            "Yifan Lu",
            "Yifan Zhao",
            "Yitao Zhai",
            "Yongjing Yin",
            "Yongwei Zhou",
            "Youshao Xiao",
            "Yuchuan Dai",
            "Yuchen Xie",
            "Yuchen Yu",
            "Yufei Zhang",
            "Yuhuai Wei",
            "Yulei Qian",
            "Yunfan Liang",
            "Yunke Zhao",
            "Yuwei Jiang",
            "Yuxin Bian",
            "Yuxin Chen",
            "Yuxin Liu",
            "Yue Xu",
            "Yueqing Sun",
            "Zeyang Yu",
            "Zhao Yang",
            "Zhengsheng Huang",
            "Zhengyu Chen",
            "Zhijian Liu",
            "Zhikang Xia",
            "Zhimin Lin",
            "Zhiyuan Yao",
            "Zhuofan Chen",
            "Zhuowen Han",
            "Zijian Zhang",
            "Ziran Li",
            "Ziwen Wang",
            "Ziyuan Zhuang"
        ],
        "title": "LongCat-Flash-Thinking-2601 Technical Report",
        "abstract": "arXiv:2601.16725v1 Announce Type: new  Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
        "arxiv_id": "2601.16725",
        "ARXIVID": "2601.16725",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of reasoning and large-scale models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.16773": {
        "authors": [
            "Shuai Huang",
            "Xuhan Lin",
            "Yuwu Lu"
        ],
        "title": "CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts",
        "abstract": "arXiv:2601.16773v1 Announce Type: new  Abstract: Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.",
        "arxiv_id": "2601.16773",
        "ARXIVID": "2601.16773",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.16811": {
        "authors": [
            "Chen-Ying Chien",
            "Po-Chih Kuo"
        ],
        "title": "Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors",
        "abstract": "arXiv:2601.16811v1 Announce Type: new  Abstract: Understanding how people perceive and evaluate interior spaces is essential for designing environments that promote well-being. However, predicting aesthetic experiences remains difficult due to the subjective nature of perception and the complexity of visual responses. This study introduces a dual-branch CNN-LSTM framework that fuses visual features with eye-tracking signals to predict aesthetic evaluations of residential interiors. We collected a dataset of 224 interior design videos paired with synchronized gaze data from 28 participants who rated 15 aesthetic dimensions. The proposed model attains 72.2% accuracy on objective dimensions (e.g., light) and 66.8% on subjective dimensions (e.g., relaxation), outperforming state-of-the-art video baselines and showing clear gains on subjective evaluation tasks. Notably, models trained with eye-tracking retain comparable performance when deployed with visual input alone. Ablation experiments further reveal that pupil responses contribute most to objective assessments, while the combination of gaze and visual cues enhances subjective evaluations. These findings highlight the value of incorporating eye-tracking as privileged information during training, enabling more practical tools for aesthetic assessment in interior design.",
        "arxiv_id": "2601.16811",
        "ARXIVID": "2601.16811",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multimodal learning and aesthetic evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.16479": {
        "authors": [
            "Hongjia Wu",
            "Shuai Zhou",
            "Hongxin Zhang",
            "Wei Chen"
        ],
        "title": "Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs",
        "abstract": "arXiv:2601.16479v1 Announce Type: new  Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an \"expert bottleneck\" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.",
        "arxiv_id": "2601.16479",
        "ARXIVID": "2601.16479",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of decision-making and structured reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.16771": {
        "authors": [
            "Jiahao Li",
            "Yunpeng Bai",
            "Yongkang Dai",
            "Hao Guo",
            "Hongping Gan",
            "Yilei Shi"
        ],
        "title": "AutoRegressive Generation with B-rep Holistic Token Sequence Representation",
        "abstract": "arXiv:2601.16771v1 Announce Type: new  Abstract: Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.",
        "arxiv_id": "2601.16771",
        "ARXIVID": "2601.16771",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
{
    "2601.19228": {
        "authors": [
            "Tianhui Song",
            "Haoyu Lu",
            "Hao Yang",
            "Lin Sui",
            "Haoning Wu",
            "Zaida Zhou",
            "Zhiqi Huang",
            "Yiping Bao",
            "Y. Charles",
            "Xinyu Zhou",
            "Limin Wang"
        ],
        "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
        "abstract": "arXiv:2601.19228v1 Announce Type: new  Abstract: We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/",
        "arxiv_id": "2601.19228",
        "ARXIVID": "2601.19228",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it explores pixel-level perception in multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.19247": {
        "authors": [
            "Jiarun Liu",
            "Qifeng Chen",
            "Yiru Zhao",
            "Minghua Liu",
            "Baorui Ma",
            "Sheng Yang"
        ],
        "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
        "abstract": "arXiv:2601.19247v1 Announce Type: new  Abstract: While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.",
        "arxiv_id": "2601.19247",
        "ARXIVID": "2601.19247",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on aligning text, image, and 3D modalities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.19060": {
        "authors": [
            "Jeonghwan Kim",
            "Renjie Tao",
            "Sanat Sharma",
            "Jiaqi Wang",
            "Kai Sun",
            "Zhaojiang Lin",
            "Seungwhan Moon",
            "Lambert Mathias",
            "Anuj Kumar",
            "Heng Ji",
            "Xin Luna Dong"
        ],
        "title": "Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models",
        "abstract": "arXiv:2601.19060v1 Announce Type: new  Abstract: Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits  tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.",
        "arxiv_id": "2601.19060",
        "ARXIVID": "2601.19060",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its exploration of a multimodal large language model with pixel-level retrieval and reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2601.19484": {
        "authors": [
            "Yin Wang",
            "Zhiying Leng",
            "Haitian Liu",
            "Frederick W. B. Li",
            "Mu Li",
            "Xiaohui Liang"
        ],
        "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
        "abstract": "arXiv:2601.19484v1 Announce Type: new  Abstract: Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.",
        "arxiv_id": "2601.19484",
        "ARXIVID": "2601.19484",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on dynamic human-scene interaction and the introduction of a new benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.19557": {
        "authors": [
            "Riccardo Giubilato",
            "Marcus Gerhard M\\\"uller",
            "Marco Sewtz",
            "Laura Alejandra Encinar Gonzalez",
            "John Folkesson",
            "Rudolph Triebel"
        ],
        "title": "The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments",
        "abstract": "arXiv:2601.19557v1 Announce Type: new  Abstract: We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.",
        "arxiv_id": "2601.19557",
        "ARXIVID": "2601.19557",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark dataset for multi-modal SLAM in unstructured planetary environments, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.19717": {
        "authors": [
            "Yitong Yang",
            "Xuexin Liu",
            "Yinglin Wang",
            "Jing Wang",
            "Hao Dou",
            "Changshuo Wang",
            "Shuting He"
        ],
        "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization",
        "abstract": "arXiv:2601.19717v1 Announce Type: new  Abstract: 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.",
        "arxiv_id": "2601.19717",
        "ARXIVID": "2601.19717",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on 3D style transfer with multi-view consistency, integrating geometric and visual features.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.19325": {
        "authors": [
            "Zichen Wen",
            "Boxue Yang",
            "Shuang Chen",
            "Yaojie Zhang",
            "Yuhang Han",
            "Junlong Ke",
            "Cong Wang",
            "Yicheng Fu",
            "Jiawang Zhao",
            "Jiangchao Yao",
            "Xi Fang",
            "Zhen Wang",
            "Henxing Cai",
            "Lin Yao",
            "Zhifeng Gao",
            "Yanhui Hong",
            "Nang Yuan",
            "Yixuan Li",
            "Guojiang Zhao",
            "Haoyi Tao",
            "Nan Wang",
            "Han Lyu",
            "Guolin Ke",
            "Ning Liao",
            "Xiaoxing Wang",
            "Kai Chen",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Sihan Hu",
            "Kun Chen",
            "Yanfeng Wang",
            "Weinan E",
            "Linfeng Zhang",
            "Linfeng Zhang"
        ],
        "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
        "abstract": "arXiv:2601.19325v1 Announce Type: new  Abstract: We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.",
        "arxiv_id": "2601.19325",
        "ARXIVID": "2601.19325",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it presents a multimodal large language model with a focus on vision and scientific reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.19204": {
        "authors": [
            "Zhixi Cai",
            "Fucai Ke",
            "Kevin Leo",
            "Sukai Huang",
            "Maria Garcia de la Banda",
            "Peter J. Stuckey",
            "Hamid Rezatofighi"
        ],
        "title": "MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning",
        "abstract": "arXiv:2601.19204v1 Announce Type: new  Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.",
        "arxiv_id": "2601.19204",
        "ARXIVID": "2601.19204",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on multi-agent visual reasoning with a hierarchical automaton system.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.19849": {
        "authors": [
            "Haya Alyoussef",
            "Ahmad Bdeir",
            "Diego Coello de Portugal Mecke",
            "Tom Hanika",
            "Niels Landwehr",
            "Lars Schmidt-Thieme"
        ],
        "title": "HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation",
        "abstract": "arXiv:2601.19849v1 Announce Type: new  Abstract: Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.",
        "arxiv_id": "2601.19849",
        "ARXIVID": "2601.19849",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a hyperbolic vision transformer for image classification.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.19582": {
        "authors": [
            "Yujin Wang",
            "Yutong Zheng",
            "Wenxian Fan",
            "Tianyi Wang",
            "Hongqing Chu",
            "Daxin Tian",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "title": "ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving",
        "abstract": "arXiv:2601.19582v1 Announce Type: new  Abstract: In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.",
        "arxiv_id": "2601.19582",
        "ARXIVID": "2601.19582",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for vision-language models in autonomous driving scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.19785": {
        "authors": [
            "Haozhi Zhu",
            "Miaomiao Zhao",
            "Dingyao Liu",
            "Runze Tian",
            "Yan Zhang",
            "Jie Guo",
            "Fenggen Yu"
        ],
        "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
        "abstract": "arXiv:2601.19785v1 Announce Type: new  Abstract: 3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.",
        "arxiv_id": "2601.19785",
        "ARXIVID": "2601.19785",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on 3D scene generation using a novel self-supervised framework.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.19236": {
        "authors": [
            "Zhiyu Yin",
            "Zhipeng Liu",
            "Kehai Chen",
            "Lemao Liu",
            "Jin Liu",
            "Hong-Dong Li",
            "Yang Xiang",
            "Min Zhang"
        ],
        "title": "VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics",
        "abstract": "arXiv:2601.19236v1 Announce Type: new  Abstract: While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.",
        "arxiv_id": "2601.19236",
        "ARXIVID": "2601.19236",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) and criterion 6 (Video Understanding) due to the introduction of a new benchmark for video connecting tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.19210": {
        "authors": [
            "Sen Nie",
            "Jie Zhang",
            "Zhuo Wang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP",
        "abstract": "arXiv:2601.19210v1 Announce Type: new  Abstract: Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.",
        "arxiv_id": "2601.19210",
        "ARXIVID": "2601.19210",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on improving robustness in vision-language models like CLIP.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.19519": {
        "authors": [
            "Ofir Abramovich",
            "Ariel Shamir",
            "Andreas Aristidou"
        ],
        "title": "Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)",
        "abstract": "arXiv:2601.19519v1 Announce Type: new  Abstract: We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.",
        "arxiv_id": "2601.19519",
        "ARXIVID": "2601.19519",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel motion capture system for real-world settings.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.19199": {
        "authors": [
            "Libo Sun",
            "Jiwen Zhang",
            "Siyuan Wang",
            "Zhongyu Wei"
        ],
        "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
        "abstract": "arXiv:2601.19199v1 Announce Type: new  Abstract: Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.",
        "arxiv_id": "2601.19199",
        "ARXIVID": "2601.19199",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on adaptive GUI agents and memory-driven knowledge evolution.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.19686": {
        "authors": [
            "Ziyue Wang",
            "Sheng Jin",
            "Zhongrong Zuo",
            "Jiawei Wu",
            "Han Qiu",
            "Qi She",
            "Hao Zhang",
            "Xudong Jiang"
        ],
        "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
        "abstract": "arXiv:2601.19686v1 Announce Type: new  Abstract: Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.",
        "arxiv_id": "2601.19686",
        "ARXIVID": "2601.19686",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on video reasoning and improving interpretability and accuracy in video tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.19821": {
        "authors": [
            "Kun Li",
            "Michael Ying Yang",
            "Sami Sebastian Brandt"
        ],
        "title": "Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering",
        "abstract": "arXiv:2601.19821v1 Announce Type: new  Abstract: Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.",
        "arxiv_id": "2601.19821",
        "ARXIVID": "2601.19821",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on audio-visual question answering and multimodal reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.19488": {
        "authors": [
            "Yizhao Han",
            "Tianxing Shi",
            "Zhao Wang",
            "Zifan Xu",
            "Zhiyuan Pu",
            "Mingxiao Li",
            "Qian Zhang",
            "Wei Yin",
            "Xiao-Xiao Long"
        ],
        "title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation",
        "abstract": "arXiv:2601.19488v1 Announce Type: new  Abstract: Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.",
        "arxiv_id": "2601.19488",
        "ARXIVID": "2601.19488",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on video generation and addressing challenges in long-horizon video quality.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.18849": {
        "authors": [
            "Yuhui Zhang",
            "Hui Yu",
            "Wei Liang",
            "Sunjie Zhang"
        ],
        "title": "Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding",
        "abstract": "arXiv:2601.18849v1 Announce Type: new  Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.",
        "arxiv_id": "2601.18849",
        "ARXIVID": "2601.18849",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling and computer vision through its focus on audio-driven talking face generation, which may align with your friend's general interest in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.18924": {
        "authors": [
            "Andrew Jaffe",
            "Noah Reicin",
            "Jinho D. Choi"
        ],
        "title": "RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures",
        "abstract": "arXiv:2601.18924v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.",
        "arxiv_id": "2601.18924",
        "ARXIVID": "2601.18924",
        "COMMENT": "Does not match any specific criterion but is related to instruction-following in large language models, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.18851": {
        "authors": [
            "Wei Liang",
            "Hui Yu",
            "Derui Ding",
            "Rachael E. Jack",
            "Philippe G. Schyns"
        ],
        "title": "SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video",
        "abstract": "arXiv:2601.18851v1 Announce Type: new  Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.",
        "arxiv_id": "2601.18851",
        "ARXIVID": "2601.18851",
        "COMMENT": "Does not match any specific criterion but is related to avatar reenactment and generative modeling, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.19365": {
        "authors": [
            "Jinming Zhang",
            "Xi Yang",
            "Youpeng Yang",
            "Haosen Shi",
            "Yuyao Yan",
            "Qiufeng Wang",
            "Guangliang Cheng",
            "Kaizhu Huang"
        ],
        "title": "Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation",
        "abstract": "arXiv:2601.19365v1 Announce Type: new  Abstract: Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.",
        "arxiv_id": "2601.19365",
        "ARXIVID": "2601.19365",
        "COMMENT": "Does not match any specific criterion but is related to medical image segmentation, which is outside the specific criteria but within the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.19157": {
        "authors": [
            "Yongsong Huang",
            "Tzu-Hsuan Peng",
            "Tomo Miyazaki",
            "Xiaofeng Liu",
            "Chun-Ting Chou",
            "Ai-Chun Pang",
            "Shinichiro Omachi"
        ],
        "title": "GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution",
        "abstract": "arXiv:2601.19157v1 Announce Type: new  Abstract: Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.",
        "arxiv_id": "2601.19157",
        "ARXIVID": "2601.19157",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and image enhancement, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.19133": {
        "authors": [
            "Yuxiang Wang",
            "Kunming Jiang",
            "Tianxiang Zhang",
            "Ke Tian",
            "Gaozhe Jiang"
        ],
        "title": "QA-ReID: Quality-Aware Query-Adaptive Convolution Leveraging Fused Global and Structural Cues for Clothes-Changing ReID",
        "abstract": "arXiv:2601.19133v1 Announce Type: new  Abstract: Unlike conventional person re-identification (ReID), clothes-changing ReID (CC-ReID) presents severe challenges due to substantial appearance variations introduced by clothing changes. In this work, we propose the Quality-Aware Dual-Branch Matching (QA-ReID), which jointly leverages RGB-based features and parsing-based representations to model both global appearance and clothing-invariant structural cues. These heterogeneous features are adaptively fused through a multi-modal attention module. At the matching stage, we further design the Quality-Aware Query Adaptive Convolution (QAConv-QA), which incorporates pixel-level importance weighting and bidirectional consistency constraints to enhance robustness against clothing variations. Extensive experiments demonstrate that QA-ReID achieves state-of-the-art performance on multiple benchmarks, including PRCC, LTCC, and VC-Clothes, and significantly outperforms existing approaches under cross-clothing scenarios.",
        "arxiv_id": "2601.19133",
        "ARXIVID": "2601.19133",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to computer vision and person re-identification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.19753": {
        "authors": [
            "Xinrui Zhang",
            "Yufeng Wang",
            "Shuangkang Fang",
            "Zesheng Wang",
            "Dacheng Qi",
            "Wenrui Ding"
        ],
        "title": "WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration",
        "abstract": "arXiv:2601.19753v1 Announce Type: new  Abstract: Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.",
        "arxiv_id": "2601.19753",
        "ARXIVID": "2601.19753",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.19309": {
        "authors": [
            "Tailong Luo",
            "Jiesong Bai",
            "Jinyang Huang",
            "Junyu Xia",
            "Wangyu Wu",
            "Xuhang Chen"
        ],
        "title": "Beyond Shadows: A Large-Scale Benchmark and Multi-Stage Framework for High-Fidelity Facial Shadow Removal",
        "abstract": "arXiv:2601.19309v1 Announce Type: new  Abstract: Facial shadows often degrade image quality and the performance of vision algorithms. Existing methods struggle to remove shadows while preserving texture, especially under complex lighting conditions, and they lack real-world paired datasets for training. We present the Augmented Shadow Face in the Wild (ASFW) dataset, the first large-scale real-world dataset for facial shadow removal, containing 1,081 paired shadow and shadow-free images created via a professional Photoshop workflow. ASFW offers photorealistic shadow variations and accurate ground truths, bridging the gap between synthetic and real domains. Deep models trained on ASFW demonstrate improved shadow removal in real-world conditions. We also introduce the Face Shadow Eraser (FSE) method to showcase the effectiveness of the dataset. Experiments demonstrate that ASFW enhances the performance of facial shadow removal models, setting new standards for this task.",
        "arxiv_id": "2601.19309",
        "ARXIVID": "2601.19309",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to computer vision through its focus on facial shadow removal, which may interest your friend in terms of vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.19014": {
        "authors": [
            "Lena Hark\\\"amper",
            "Leo Lebrat",
            "David Ahmedt-Aristizabal",
            "Olivier Salvado",
            "Mattias Heinrich",
            "Rodrigo Santa Cruz"
        ],
        "title": "Non-Invasive 3D Wound Measurement with RGB-D Imaging",
        "abstract": "arXiv:2601.19014v1 Announce Type: new  Abstract: Chronic wound monitoring and management require accurate and efficient wound measurement methods. This paper presents a fast, non-invasive 3D wound measurement algorithm based on RGB-D imaging. The method combines RGB-D odometry with B-spline surface reconstruction to generate detailed 3D wound meshes, enabling automatic computation of clinically relevant wound measurements such as perimeter, surface area, and dimensions. We evaluated our system on realistic silicone wound phantoms and measured sub-millimetre 3D reconstruction accuracy compared with high-resolution ground-truth scans. The extracted measurements demonstrated low variability across repeated captures and strong agreement with manual assessments. The proposed pipeline also outperformed a state-of-the-art object-centric RGB-D reconstruction method while maintaining runtimes suitable for real-time clinical deployment. Our approach offers a promising tool for automated wound assessment in both clinical and remote healthcare settings.",
        "arxiv_id": "2601.19014",
        "ARXIVID": "2601.19014",
        "COMMENT": "Does not match any specific criterion but is related to 3D imaging and healthcare applications, which is outside the specific criteria but within the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.19680": {
        "authors": [
            "Antonio Di Marino",
            "Vincenzo Bevilacqua",
            "Emanuel Di Nardo",
            "Angelo Ciaramella",
            "Ivanoe De Falco",
            "Giovanna Sannino"
        ],
        "title": "A new Image Similarity Metric for a Perceptual and Transparent Geometric and Chromatic Assessment",
        "abstract": "arXiv:2601.19680v1 Announce Type: new  Abstract: In the literature, several studies have shown that state-of-the-art image similarity metrics are not perceptual metrics; moreover, they have difficulty evaluating images, especially when texture distortion is also present. In this work, we propose a new perceptual metric composed of two terms. The first term evaluates the dissimilarity between the textures of two images using Earth Mover's Distance. The second term evaluates the chromatic dissimilarity between two images in the Oklab perceptual color space. We evaluated the performance of our metric on a non-traditional dataset, called Berkeley-Adobe Perceptual Patch Similarity, which contains a wide range of complex distortions in shapes and colors. We have shown that our metric outperforms the state of the art, especially when images contain shape distortions, confirming also its greater perceptiveness. Furthermore, although deep black-box metrics could be very accurate, they only provide similarity scores between two images, without explaining their main differences and similarities. Our metric, on the other hand, provides visual explanations to support the calculated score, making the similarity assessment transparent and justified.",
        "arxiv_id": "2601.19680",
        "ARXIVID": "2601.19680",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to computer vision metrics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
{
    "2602.18746": {
        "authors": [
            "Haoyu Zhang",
            "Yuwei Wu",
            "Pengxiang Li",
            "Xintong Zhang",
            "Zhi Gao",
            "Rui Gao",
            "Mingyang Gao",
            "Che Sun",
            "Yunde Jia"
        ],
        "title": "MIRROR: Multimodal Iterative Reasoning via Reflection on Visual Regions",
        "abstract": "arXiv:2602.18746v1 Announce Type: new  Abstract: In the era of Vision-Language Models (VLMs), enhancing multimodal reasoning capabilities remains a critical challenge, particularly in handling ambiguous or complex visual inputs, where initial inferences often lead to hallucinations or logic errors. Existing VLMs often produce plausible yet ungrounded answers, and even when prompted to \"reflect\", their corrections may remain detached from the image evidence. To address this, we propose the MIRROR framework for Multimodal Iterative Reasoning via Reflection On visual Regions. By embedding visual reflection as a core mechanism, MIRROR is formulated as a closed-loop process comprising draft, critique, region-based verification, and revision, which are repeated until the output is visually grounded. To facilitate training of this model, we construct **ReflectV**, a visual reflective dataset for multi-turn supervision that explicitly contains reflection triggers, region-based verification actions, and answer revision grounded in visual evidence. Experiments on both general vision-language benchmarks and representative vision-language reasoning benchmarks show that MIRROR improves correctness and reduces visual hallucinations, demonstrating the value of training reflection as an evidence-seeking, region-aware verification process rather than a purely textual revision step.",
        "arxiv_id": "2602.18746",
        "ARXIVID": "2602.18746",
        "COMMENT": "Matches criterion 2 as it proposes a novel multimodal reasoning framework (MIRROR) for vision-language models, focusing on reducing hallucinations and improving grounding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.19672": {
        "authors": [
            "Jiayu Wang",
            "Yifei Ming",
            "Zixuan Ke",
            "Shafiq Joty",
            "Aws Albarghouthi",
            "Frederic Sala"
        ],
        "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
        "abstract": "arXiv:2602.19672v1 Announce Type: new  Abstract: Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
        "arxiv_id": "2602.19672",
        "ARXIVID": "2602.19672",
        "COMMENT": "Matches criterion 3 as it introduces a novel orchestration framework (SkillOrchestra) for compound AI systems, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.18532": {
        "authors": [
            "Xiao-Ming Wu",
            "Bin Fan",
            "Kang Liao",
            "Jian-Jian Jiang",
            "Runze Yang",
            "Yihang Luo",
            "Zhonghua Wu",
            "Wei-Shi Zheng",
            "Chen Change Loy"
        ],
        "title": "VLANeXt: Recipes for Building Strong VLA Models",
        "abstract": "arXiv:2602.18532v1 Announce Type: new  Abstract: Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.",
        "arxiv_id": "2602.18532",
        "ARXIVID": "2602.18532",
        "COMMENT": "Matches criterion 3 as it systematically explores design choices for Vision-Language-Action models and proposes VLANeXt, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.19710": {
        "authors": [
            "Haitao Lin",
            "Hanyang Yu",
            "Jingshun Huang",
            "He Zhang",
            "Yonggen Ling",
            "Ping Tan",
            "Xiangyang Xue",
            "Yanwei Fu"
        ],
        "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
        "abstract": "arXiv:2602.19710v1 Announce Type: new  Abstract: Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.   To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.   Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
        "arxiv_id": "2602.19710",
        "ARXIVID": "2602.19710",
        "COMMENT": "Matches criterion 3 as it introduces a novel pretraining paradigm (Pose-VLA) for vision-language-action policies, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.19432": {
        "authors": [
            "Yifeng Huang",
            "Gia Khanh Nguyen",
            "Minh Hoai"
        ],
        "title": "CountEx: Fine-Grained Counting via Exemplars and Exclusion",
        "abstract": "arXiv:2602.19432v1 Announce Type: new  Abstract: This paper presents CountEx, a discriminative visual counting framework designed to address a key limitation of existing prompt-based methods: the inability to explicitly exclude visually similar distractors. While current approaches allow users to specify what to count via inclusion prompts, they often struggle in cluttered scenes with confusable object categories, leading to ambiguity and overcounting. CountEx enables users to express both inclusion and exclusion intent, specifying what to count and what to ignore, through multimodal prompts including natural language descriptions and optional visual exemplars. At the core of CountEx is a novel Discriminative Query Refinement module, which jointly reasons over inclusion and exclusion cues by first identifying shared visual features, then isolating exclusion-specific patterns, and finally applying selective suppression to refine the counting query. To support systematic evaluation of fine-grained counting methods, we introduce CoCount, a benchmark comprising 1,780 videos and 10,086 annotated frames across 97 category pairs. Experiments show that CountEx achieves substantial improvements over state-of-the-art methods for counting objects from both known and novel categories. The data and code are available at https://github.com/bbvisual/CountEx.",
        "arxiv_id": "2602.19432",
        "ARXIVID": "2602.19432",
        "COMMENT": "Matches criterion 2 as it explores multimodal prompts and reasoning for fine-grained visual counting, which aligns with vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.18873": {
        "authors": [
            "Miaowei Wang",
            "Qingxuan Yan",
            "Zhi Cao",
            "Yayuan Li",
            "Oisin Mac Aodha",
            "Jason J. Corso",
            "Amir Vaxman"
        ],
        "title": "BiMotion: B-spline Motion for Text-guided Dynamic 3D Character Generation",
        "abstract": "arXiv:2602.18873v1 Announce Type: new  Abstract: Text-guided dynamic 3D character generation has advanced rapidly, yet producing high-quality motion that faithfully reflects rich textual descriptions remains challenging. Existing methods tend to generate limited sub-actions or incoherent motion due to fixed-length temporal inputs and discrete frame-wise representations that fail to capture rich motion semantics. We address these limitations by representing motion with continuous differentiable B-spline curves, enabling more effective motion generation without modifying the capabilities of the underlying generative model. Specifically, our closed-form, Laplacian-regularized B-spline solver efficiently compresses variable-length motion sequences into compact representations with a fixed number of control points. Further, we introduce a normal-fusion strategy for input shape adherence along with correspondence-aware and local-rigidity losses for motion-restoration quality. To train our model, we collate BIMO, a new dataset containing diverse variable-length 3D motion sequences with rich, high-quality text annotations. Extensive evaluations show that our feed-forward framework BiMotion generates more expressive, higher-quality, and better prompt-aligned motions than existing state-of-the-art methods, while also achieving faster generation. Our project page is at: https://wangmiaowei.github.io/BiMotion.github.io/.",
        "arxiv_id": "2602.18873",
        "ARXIVID": "2602.18873",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset (BIMO) and novel methods for text-guided dynamic 3D character generation, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.19323": {
        "authors": [
            "Yiran Qiao",
            "Yiren Lu",
            "Yunlai Zhou",
            "Rui Yang",
            "Linlin Hou",
            "Yu Yin",
            "Jing Ma"
        ],
        "title": "DefenseSplat: Enhancing the Robustness of 3D Gaussian Splatting via Frequency-Aware Filtering",
        "abstract": "arXiv:2602.19323v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for real-time and high-fidelity 3D reconstruction from posed images. However, recent studies reveal its vulnerability to adversarial corruptions in input views, where imperceptible yet consistent perturbations can drastically degrade rendering quality, increase training and rendering time, and inflate memory usage, even leading to server denial-of-service. In our work, to mitigate this issue, we begin by analyzing the distinct behaviors of adversarial perturbations in the low- and high-frequency components of input images using wavelet transforms. Based on this observation, we design a simple yet effective frequency-aware defense strategy that reconstructs training views by filtering high-frequency noise while preserving low-frequency content. This approach effectively suppresses adversarial artifacts while maintaining the authenticity of the original scene. Notably, it does not significantly impair training on clean data, achieving a desirable trade-off between robustness and performance on clean inputs. Through extensive experiments under a wide range of attack intensities on multiple benchmarks, we demonstrate that our method substantially enhances the robustness of 3DGS without access to clean ground-truth supervision. By highlighting and addressing the overlooked vulnerabilities of 3D Gaussian Splatting, our work paves the way for more robust and secure 3D reconstructions.",
        "arxiv_id": "2602.19323",
        "ARXIVID": "2602.19323",
        "COMMENT": "Does not match any specific criteria but is related to robustness in 3D reconstruction, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
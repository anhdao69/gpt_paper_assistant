{
    "2601.10611": {
        "authors": [
            "Christopher Clark",
            "Jieyu Zhang",
            "Zixian Ma",
            "Jae Sung Park",
            "Mohammadreza Salehi",
            "Rohun Tripathi",
            "Sangho Lee",
            "Zhongzheng Ren",
            "Chris Dongjoo Kim",
            "Yinuo Yang",
            "Vincent Shao",
            "Yue Yang",
            "Weikai Huang",
            "Ziqi Gao",
            "Taira Anderson",
            "Jianrui Zhang",
            "Jitesh Jain",
            "George Stoica",
            "Winson Han",
            "Ali Farhadi",
            "Ranjay Krishna"
        ],
        "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
        "abstract": "arXiv:2601.10611v1 Announce Type: new  Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
        "arxiv_id": "2601.10611",
        "ARXIVID": "2601.10611",
        "COMMENT": "Matches criteria 2, 5, and 6 as it introduces a new family of video-language models with novel datasets and capabilities for video understanding and grounding.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2601.10332": {
        "authors": [
            "Siqi Kou",
            "Jiachun Jin",
            "Zetong Zhou",
            "Ye Ma",
            "Yugang Wang",
            "Quan Chen",
            "Peng Jiang",
            "Xiao Yang",
            "Jun Zhu",
            "Kai Yu",
            "Zhijie Deng"
        ],
        "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
        "abstract": "arXiv:2601.10332v1 Announce Type: new  Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
        "arxiv_id": "2601.10332",
        "ARXIVID": "2601.10332",
        "COMMENT": "Matches criteria 2 and 5 as it integrates reasoning-aware text-to-image diffusion with LLMs, focusing on vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2601.10710": {
        "authors": [
            "Cheng Chen",
            "Yuyu Guo",
            "Pengpeng Zeng",
            "Jingkuan Song",
            "Peng Di",
            "Hang Yu",
            "Lianli Gao"
        ],
        "title": "From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion",
        "abstract": "arXiv:2601.10710v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.",
        "arxiv_id": "2601.10710",
        "ARXIVID": "2601.10710",
        "COMMENT": "Matches criteria 2 and 5 as it proposes a novel framework for deep vision-language fusion, improving multimodal understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.10632": {
        "authors": [
            "Chengfeng Zhao",
            "Jiazhi Shu",
            "Yubo Zhao",
            "Tianyu Huang",
            "Jiahao Lu",
            "Zekai Gu",
            "Chengwei Ren",
            "Zhiyang Dou",
            "Qing Shuai",
            "Yuan Liu"
        ],
        "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
        "abstract": "arXiv:2601.10632v1 Announce Type: new  Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
        "arxiv_id": "2601.10632",
        "ARXIVID": "2601.10632",
        "COMMENT": "Matches criterion 5 as it integrates 3D human motion generation with video generation, aligning with vision and multimodal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.10649": {
        "authors": [
            "Darshan Singh",
            "Arsha Nagrani",
            "Kawshik Manikantan",
            "Harman Singh",
            "Dinesh Tewari",
            "Tobias Weyand",
            "Cordelia Schmid",
            "Anelia Angelova",
            "Shachi Dave"
        ],
        "title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning",
        "abstract": "arXiv:2601.10649v1 Announce Type: new  Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural",
        "arxiv_id": "2601.10649",
        "ARXIVID": "2601.10649",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark for video understanding with a focus on cultural and multilingual reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.10165": {
        "authors": [
            "Chao Huang",
            "Benfeng Wang",
            "Wei Wang",
            "Jie Wen",
            "Li Shen",
            "Wenqi Ren",
            "Yong Xu",
            "Xiaochun Cao"
        ],
        "title": "Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method",
        "abstract": "arXiv:2601.10165v1 Announce Type: new  Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.",
        "arxiv_id": "2601.10165",
        "ARXIVID": "2601.10165",
        "COMMENT": "Matches criteria 6 as it introduces a benchmark and method for video anomaly reasoning, advancing video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.09879": {
        "authors": [
            "Yang Xing",
            "Jiong Wu",
            "Savas Ozdemir",
            "Ying Zhang",
            "Yang Yang",
            "Wei Shao",
            "Kuang Gong"
        ],
        "title": "MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation",
        "abstract": "arXiv:2601.09879v1 Announce Type: new  Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.",
        "arxiv_id": "2601.09879",
        "ARXIVID": "2601.09879",
        "COMMENT": "Matches criteria 1 and 2 as it focuses on spatial reasoning and multimodal reasoning in 3D medical vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.09954": {
        "authors": [
            "Nahid Alam",
            "Leema Krishna Murali",
            "Siddhant Bharadwaj",
            "Patrick Liu",
            "Timothy Chung",
            "Drishti Sharma",
            "Akshata A",
            "Kranthi Kiran",
            "Wesley Tam",
            "Bala Krishna S Vegesna"
        ],
        "title": "The Spatial Blindspot of Vision-Language Models",
        "abstract": "arXiv:2601.09954v1 Announce Type: new  Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.",
        "arxiv_id": "2601.09954",
        "ARXIVID": "2601.09954",
        "COMMENT": "Matches criterion 1 as it addresses spatial reasoning in vision-language models, which is relevant to embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.10103": {
        "authors": [
            "Lizhen Wang",
            "Yongming Zhu",
            "Zhipeng Ge",
            "Youwei Zheng",
            "Longhao Zhang",
            "Tianshu Hu",
            "Shiyang Qin",
            "Mingshuang Luo",
            "Jiaxu Zhang",
            "Xin Chen",
            "Yulong Wang",
            "Zerong Zheng",
            "Jianwen Jiang",
            "Chao Liang",
            "Weifeng Chen",
            "Xing Wang",
            "Yuan Zhang",
            "Mingyuan Gao"
        ],
        "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
        "abstract": "arXiv:2601.10103v1 Announce Type: new  Abstract: Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
        "arxiv_id": "2601.10103",
        "ARXIVID": "2601.10103",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on interactive humanoid video generation with a novel architecture and training strategy for vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10714": {
        "authors": [
            "Tal Reiss",
            "Daniel Winter",
            "Matan Cohen",
            "Alex Rav-Acha",
            "Yael Pritch",
            "Ariel Shamir",
            "Yedid Hoshen"
        ],
        "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
        "abstract": "arXiv:2601.10714v1 Announce Type: new  Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.",
        "arxiv_id": "2601.10714",
        "ARXIVID": "2601.10714",
        "COMMENT": "Matches criterion 5 as it focuses on techniques combining image understanding and generation tasks with textual prompts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09883": {
        "authors": [
            "Xinxing Ren",
            "Quagmire Zang",
            "Caelum Forder",
            "Suman Deb",
            "Ahsen Tahir",
            "Roman J. Georgio",
            "Peter Carroll",
            "Zekun Guo"
        ],
        "title": "Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL",
        "abstract": "arXiv:2601.09883v1 Announce Type: new  Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows",
        "arxiv_id": "2601.09883",
        "ARXIVID": "2601.09883",
        "COMMENT": "Matches criterion 1 as it discusses spatial intelligence and embodied agents through a novel multi-agent paradigm.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10497": {
        "authors": [
            "Wenqing Wang",
            "Da Li",
            "Xiatian Zhu",
            "Josef Kittler"
        ],
        "title": "mergetune: Continued fine-tuning of vision-language models",
        "abstract": "arXiv:2601.10497v1 Announce Type: new  Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \\emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\\% on base-novel generalisation without adding parameters. % We show \\emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.",
        "arxiv_id": "2601.10497",
        "ARXIVID": "2601.10497",
        "COMMENT": "Matches criteria 2 as it focuses on fine-tuning vision-language models and mitigating catastrophic forgetting.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10117": {
        "authors": [
            "Wenwen Liao",
            "Jianbo Yu",
            "Yuansong Wang",
            "Shifu Yan",
            "Xiaofeng Yang"
        ],
        "title": "Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL",
        "abstract": "arXiv:2601.10117v1 Announce Type: new  Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.   We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.",
        "arxiv_id": "2601.10117",
        "ARXIVID": "2601.10117",
        "COMMENT": "Matches criteria 2 as it proposes a novel framework for vision in-context learning with synergistic fusion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.10107": {
        "authors": [
            "Wenwen Liao",
            "Jianbo Yu",
            "Yuansong Wang",
            "Qingchao Jiang",
            "Xiaofeng Yang"
        ],
        "title": "Enhancing Visual In-Context Learning by Multi-Faceted Fusion",
        "abstract": "arXiv:2601.10107v1 Announce Type: new  Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant \"retrieve-then-prompt\" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.",
        "arxiv_id": "2601.10107",
        "ARXIVID": "2601.10107",
        "COMMENT": "Matches criteria 2 as it explores visual in-context learning with a novel fusion approach for multimodal tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09881": {
        "authors": [
            "Weili Nie",
            "Julius Berner",
            "Nanye Ma",
            "Chao Liu",
            "Saining Xie",
            "Arash Vahdat"
        ],
        "title": "Transition Matching Distillation for Fast Video Generation",
        "abstract": "arXiv:2601.09881v1 Announce Type: new  Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
        "arxiv_id": "2601.09881",
        "ARXIVID": "2601.09881",
        "COMMENT": "Matches criteria 6 as it introduces a novel framework for fast video generation, which is a video-based task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.10416": {
        "authors": [
            "Tiesunlong Shen",
            "Rui Mao",
            "Jin Wang",
            "Heming Sun",
            "Jian Zhang",
            "Xuejie Zhang",
            "Erik Cambria"
        ],
        "title": "LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models",
        "abstract": "arXiv:2601.10416v1 Announce Type: new  Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.",
        "arxiv_id": "2601.10416",
        "ARXIVID": "2601.10416",
        "COMMENT": "Does not match any specific criterion but is generally relevant to optimization and alignment in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10306": {
        "authors": [
            "Xin Guan",
            "Zijian Li",
            "Shen Huang",
            "Pengjun Xie",
            "Jingren Zhou",
            "Jiuxin Cao"
        ],
        "title": "Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning",
        "abstract": "arXiv:2601.10306v1 Announce Type: new  Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.",
        "arxiv_id": "2601.10306",
        "ARXIVID": "2601.10306",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reinforcement learning and reasoning, which are tangentially related to the friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10098": {
        "authors": [
            "Wenwen Liao",
            "Hang Ruan",
            "Jianbo Yu",
            "Yuansong Wang",
            "Qingchao Jiang",
            "Xiaofeng Yang"
        ],
        "title": "InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery",
        "abstract": "arXiv:2601.10098v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.",
        "arxiv_id": "2601.10098",
        "ARXIVID": "2601.10098",
        "COMMENT": "Does not match any specific criteria but explores representation learning, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10075": {
        "authors": [
            "Zhendong Wang",
            "Lebin Zhou",
            "Jingchuan Xiao",
            "Rongduo Han",
            "Nam Ling",
            "Cihan Ruan"
        ],
        "title": "Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting",
        "abstract": "arXiv:2601.10075v1 Announce Type: new  Abstract: In 1888, Vincent van Gogh wrote, \"I am seeking exaggeration in the essential.\" This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.   We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.   Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.",
        "arxiv_id": "2601.10075",
        "ARXIVID": "2601.10075",
        "COMMENT": "Does not match any specific criteria but is tangentially related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10148": {
        "authors": [
            "Xiaowei Lv",
            "Zhilin Zhang",
            "Yijun Li",
            "Yusen Huo",
            "Siyuan Ju",
            "Xuyan Li",
            "Chunxiang Hong",
            "Tianyu Wang",
            "Yongcai Wang",
            "Peng Sun",
            "Chuan Yu",
            "Jian Xu",
            "Bo Zheng"
        ],
        "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration",
        "abstract": "arXiv:2601.10148v1 Announce Type: new  Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",
        "arxiv_id": "2601.10148",
        "ARXIVID": "2601.10148",
        "COMMENT": "Does not match any specific criteria but explores LLMs for decision-making tasks, which is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10373": {
        "authors": [
            "Yichong Xia",
            "Yimin Zhou",
            "Jinpeng Wang",
            "Bin Chen"
        ],
        "title": "Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement",
        "abstract": "arXiv:2601.10373v1 Announce Type: new  Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \\textbf{Diff}usion-based Image Compression via \\textbf{C}onsistency Prior \\textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $\\epsilon$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \\textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\\% BD-rate (LPIPS) and 65.1\\% BD-rate (PSNR)) and over $10\\times$ speed-up compared to SOTA diffusion-based compression baselines.",
        "arxiv_id": "2601.10373",
        "ARXIVID": "2601.10373",
        "COMMENT": "Does not match any specific criteria but is tangentially related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.10200": {
        "authors": [
            "Kim Youwang",
            "Lee Hyoseok",
            "Subin Park",
            "Gerard Pons-Moll",
            "Tae-Hyun Oh"
        ],
        "title": "ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation",
        "abstract": "arXiv:2601.10200v1 Announce Type: new  Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.",
        "arxiv_id": "2601.10200",
        "ARXIVID": "2601.10200",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10192": {
        "authors": [
            "Hu Gao",
            "Xiaoning Lei",
            "Xichen Xu",
            "Xingjian Wang",
            "Lizhuang Ma"
        ],
        "title": "From Physical Degradation Models to Task-Aware All-in-One Image Restoration",
        "abstract": "arXiv:2601.10192v1 Announce Type: new  Abstract: All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.",
        "arxiv_id": "2601.10192",
        "ARXIVID": "2601.10192",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10031": {
        "authors": [
            "Jianheng Tang",
            "Shilong Tao",
            "Zhe Feng",
            "Haonan Sun",
            "Menglu Wang",
            "Zhanxing Zhu",
            "Yunhuai Liu"
        ],
        "title": "FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data",
        "abstract": "arXiv:2601.10031v1 Announce Type: new  Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.",
        "arxiv_id": "2601.10031",
        "ARXIVID": "2601.10031",
        "COMMENT": "Does not match any specific criterion but is generally relevant to deep learning applications in scientific computation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10193": {
        "authors": [
            "Jiujiu Chen",
            "Weijun Zeng",
            "Shaofeng Hu",
            "Sihong Xie",
            "Hui Xiong"
        ],
        "title": "GFM4GA: Graph Foundation Model for Group Anomaly Detection",
        "abstract": "arXiv:2601.10193v1 Announce Type: new  Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.",
        "arxiv_id": "2601.10193",
        "ARXIVID": "2601.10193",
        "COMMENT": "Does not match any specific criterion but is generally relevant to graph foundation models, which are tangentially related to the friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.10132": {
        "authors": [
            "Yanan Cao",
            "Farnaz Fallahi",
            "Murali Mohana Krishna Dandu",
            "Lalitesh Morishetti",
            "Kai Zhao",
            "Luyi Ma",
            "Sinduja Subramaniam",
            "Jianpeng Xu",
            "Evren Korpeoglu",
            "Kaushiki Nag",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "title": "Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction",
        "abstract": "arXiv:2601.10132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that \"more context leads to better reasoning\". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.",
        "arxiv_id": "2601.10132",
        "ARXIVID": "2601.10132",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning and prediction, which are tangentially related to the friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
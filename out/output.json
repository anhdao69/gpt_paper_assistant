{
    "2601.09668": {
        "authors": [
            "Ailin Huang",
            "Chengyuan Yao",
            "Chunrui Han",
            "Fanqi Wan",
            "Hangyu Guo",
            "Haoran Lv",
            "Hongyu Zhou",
            "Jia Wang",
            "Jian Zhou",
            "Jianjian Sun",
            "Jingcheng Hu",
            "Kangheng Lin",
            "Liang Zhao",
            "Mitt Huang",
            "Song Yuan",
            "Wenwen Qu",
            "Xiangfeng Wang",
            "Yanlin Lai",
            "Yingxiu Zhao",
            "Yinmin Zhang",
            "Yukang Shi",
            "Yuyang Chen",
            "Zejia Weng",
            "Ziyang Meng",
            "Ang Li",
            "Aobo Kong",
            "Bo Dong",
            "Changyi Wan",
            "David Wang",
            "Di Qi",
            "Dingming Li",
            "En Yu",
            "Guopeng Li",
            "Haiquan Yin",
            "Han Zhou",
            "Hanshan Zhang",
            "Haolong Yan",
            "Hebin Zhou",
            "Hongbo Peng",
            "Jiaran Zhang",
            "Jiashu Lv",
            "Jiayi Fu",
            "Jie Cheng",
            "Jie Zhou",
            "Jisheng Yin",
            "Jingjing Xie",
            "Jingwei Wu",
            "Jun Zhang",
            "Junfeng Liu",
            "Kaijun Tan",
            "Kaiwen Yan",
            "Liangyu Chen",
            "Lina Chen",
            "Mingliang Li",
            "Qian Zhao",
            "Quan Sun",
            "Shaoliang Pang",
            "Shengjie Fan",
            "Shijie Shang",
            "Siyuan Zhang",
            "Tianhao You",
            "Wei Ji",
            "Wuxun Xie",
            "Xiaobo Yang",
            "Xiaojie Hou",
            "Xiaoran Jiao",
            "Xiaoxiao Ren",
            "Xiangwen Kong",
            "Xin Huang",
            "Xin Wu",
            "Xing Chen",
            "Xinran Wang",
            "Xuelin Zhang",
            "Yana Wei",
            "Yang Li",
            "Yanming Xu",
            "Yeqing Shen",
            "Yuang Peng",
            "Yue Peng",
            "Yu Zhou",
            "Yusheng Li",
            "Yuxiang Yang",
            "Yuyang Zhang",
            "Zhe Xie",
            "Zhewei Huang",
            "Zhenyi Lu",
            "Zhimin Fan",
            "Zihui Cheng",
            "Daxin Jiang",
            "Qi Han",
            "Xiangyu Zhang",
            "Yibo Zhu",
            "Zheng Ge"
        ],
        "title": "STEP3-VL-10B Technical Report",
        "abstract": "arXiv:2601.09668v1 Announce Type: new  Abstract: We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
        "arxiv_id": "2601.09668",
        "ARXIVID": "2601.09668",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel Visual Large Language Model (VLLM) architecture and integrates vision-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.09613": {
        "authors": [
            "Yonglin Tian",
            "Qiyao Zhang",
            "Wei Xu",
            "Yutong Wang",
            "Yihao Wu",
            "Xinyi Li",
            "Xingyuan Dai",
            "Hui Zhang",
            "Zhiyong Cui",
            "Baoqing Guo",
            "Zujun Yu",
            "Yisheng Lv"
        ],
        "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems",
        "abstract": "arXiv:2601.09613v1 Announce Type: new  Abstract: Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.",
        "arxiv_id": "2601.09613",
        "ARXIVID": "2601.09613",
        "COMMENT": "Matches criteria 2 and 3 as it introduces a benchmark (CogRail) for visual-language models in spatio-temporal reasoning and evaluates their performance in a safety-critical domain.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.08868": {
        "authors": [
            "Yi Wang",
            "Yinfeng Yu",
            "Bin Ren"
        ],
        "title": "Residual Cross-Modal Fusion Networks for Audio-Visual Navigation",
        "abstract": "arXiv:2601.08868v1 Announce Type: new  Abstract: Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.",
        "arxiv_id": "2601.08868",
        "ARXIVID": "2601.08868",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for audio-visual embodied navigation, addressing cross-modal challenges.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.08323": {
        "authors": [
            "Yupeng Huo",
            "Yaxi Lu",
            "Zhong Zhang",
            "Haotian Chen",
            "Yankai Lin"
        ],
        "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
        "abstract": "arXiv:2601.08323v1 Announce Type: new  Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.",
        "arxiv_id": "2601.08323",
        "ARXIVID": "2601.08323",
        "COMMENT": "Matches criteria 1 as it introduces a novel memory mechanism for embodied agents, improving spatial reasoning and task alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.08173": {
        "authors": [
            "Daocheng Fu",
            "Jianbiao Mei",
            "Rong Wu",
            "Xuemeng Yang",
            "Jia Xu",
            "Ding Wang",
            "Pinlong Cai",
            "Yong Liu",
            "Licheng Wen",
            "Botian Shi"
        ],
        "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
        "abstract": "arXiv:2601.08173v1 Announce Type: new  Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
        "arxiv_id": "2601.08173",
        "ARXIVID": "2601.08173",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark for embodied agents focusing on dynamic task scheduling, active exploration, and continuous learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09228": {
        "authors": [
            "Fan Liu",
            "Ting Wu",
            "Chuanyi Zhang",
            "Liang Yao",
            "Xing Ma",
            "Yuhui Zheng"
        ],
        "title": "Disentangle Object and Non-object Infrared Features via Language Guidance",
        "abstract": "arXiv:2601.09228v1 Announce Type: new  Abstract: Infrared object detection focuses on identifying and locating objects in complex environments (\\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\\textsuperscript{3}FD (83.7\\% mAP), FLIR (86.1\\% mAP). Our code will be publicly available once the paper is accepted.",
        "arxiv_id": "2601.09228",
        "ARXIVID": "2601.09228",
        "COMMENT": "Matches criteria 5 as it combines vision tasks with language guidance for infrared object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09230": {
        "authors": [
            "Haodi Yao",
            "Fenghua He",
            "Ning Hao",
            "Yao Su"
        ],
        "title": "CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation",
        "abstract": "arXiv:2601.09230v1 Announce Type: new  Abstract: Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.",
        "arxiv_id": "2601.09230",
        "ARXIVID": "2601.09230",
        "COMMENT": "Matches criteria 1 as it focuses on spatial intelligence tasks and introduces a novel method for local feature representation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.09528": {
        "authors": [
            "Alfio Spoto",
            "Rosario Leonardi",
            "Francesco Ragusa",
            "Giovanni Maria Farinella"
        ],
        "title": "GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection",
        "abstract": "arXiv:2601.09528v1 Announce Type: new  Abstract: Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.",
        "arxiv_id": "2601.09528",
        "ARXIVID": "2601.09528",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark (GlovEgo-HOI) and a novel method (GlovEgo-Net) for egocentric human-object interaction detection in industrial settings.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.09136": {
        "authors": [
            "Lijun Liu",
            "Linwei Chen",
            "Zhishou Zhang",
            "Meng Tian",
            "Hengfu Cui",
            "Ruiyang Li",
            "Zhaocheng Liu",
            "Qiang Ju",
            "Qianxi Li",
            "Hong-Yu Zhou"
        ],
        "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
        "abstract": "arXiv:2601.09136v1 Announce Type: new  Abstract: General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
        "arxiv_id": "2601.09136",
        "ARXIVID": "2601.09136",
        "COMMENT": "Matches criteria 2 as it explores a novel framework for visual-language models in a medical domain, focusing on optimization of visual information transmission.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.08235": {
        "authors": [
            "Shouju Wang",
            "Haopeng Zhang"
        ],
        "title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents",
        "abstract": "arXiv:2601.08235v2 Announce Type: new  Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.",
        "arxiv_id": "2601.08235",
        "ARXIVID": "2601.08235",
        "COMMENT": "Matches criteria 5 as it evaluates multimodal privacy behavior in LLMs, integrating visual and textual modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.08731": {
        "authors": [
            "Yuanlin Duan",
            "Yuning Wang",
            "Wenjie Qiu",
            "He Zhu"
        ],
        "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
        "abstract": "arXiv:2601.08731v1 Announce Type: new  Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.",
        "arxiv_id": "2601.08731",
        "ARXIVID": "2601.08731",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for learning from demonstrations in goal-conditioned tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.08876": {
        "authors": [
            "Shuai Chen",
            "Hao Chen",
            "Yuanchen Bei",
            "Tianyang Zhao",
            "Zhibo Zhou",
            "Feiran Huang"
        ],
        "title": "The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models",
        "abstract": "arXiv:2601.08876v1 Announce Type: new  Abstract: Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.",
        "arxiv_id": "2601.08876",
        "ARXIVID": "2601.08876",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it discusses semantic processing in embodied AI using foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.09116": {
        "authors": [
            "Haoyan Gong",
            "Hongbin Liu"
        ],
        "title": "LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models",
        "abstract": "arXiv:2601.09116v1 Announce Type: new  Abstract: Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing \"restoration-then-recognition\" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.",
        "arxiv_id": "2601.09116",
        "ARXIVID": "2601.09116",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates structured reasoning into large multimodal models for text recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.09658": {
        "authors": [
            "Selim Emir Can",
            "Jan Ackermann",
            "Kiyohiro Nakayama",
            "Ruofan Liu",
            "Tong Wu",
            "Yang Zheng",
            "Hugo Bertiche",
            "Menglei Chai",
            "Thabo Beeler",
            "Gordon Wetzstein"
        ],
        "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
        "abstract": "arXiv:2601.09658v1 Announce Type: new  Abstract: Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.",
        "arxiv_id": "2601.09658",
        "ARXIVID": "2601.09658",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it involves spatial reasoning for garment simulation and criterion 2 (Visual and Multimodal Large Language Models) due to the use of vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.08881": {
        "authors": [
            "Yu Xu",
            "Hongbin Yan",
            "Juan Cao",
            "Yiji Cheng",
            "Tiankai Hang",
            "Runze He",
            "Zijin Yin",
            "Shiyi Zhang",
            "Yuxin Zhang",
            "Jintao Li",
            "Chunyu Wang",
            "Qinglin Lu",
            "Tong-Yee Lee",
            "Fan Tang"
        ],
        "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
        "abstract": "arXiv:2601.08881v1 Announce Type: new  Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.",
        "arxiv_id": "2601.08881",
        "ARXIVID": "2601.08881",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on integrating semantic intent into image generation and editing tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.09298": {
        "authors": [
            "Lianying Chao",
            "Haoran Cai",
            "Xubin Li",
            "Kai Zhang",
            "Sijie Wu",
            "Rui Xu"
        ],
        "title": "Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain",
        "abstract": "arXiv:2601.09298v1 Announce Type: new  Abstract: In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.",
        "arxiv_id": "2601.09298",
        "ARXIVID": "2601.09298",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on training a domain-specific multimodal LLM for image captioning.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2601.09572": {
        "authors": [
            "Tianli Tao",
            "Ziyang Wang",
            "Delong Yang",
            "Han Zhang",
            "Le Zhang"
        ],
        "title": "Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model",
        "abstract": "arXiv:2601.09572v1 Announce Type: new  Abstract: Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.",
        "arxiv_id": "2601.09572",
        "ARXIVID": "2601.09572",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in medical imaging, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08462": {
        "authors": [
            "Sixiong Xie",
            "Zhuofan Shi",
            "Haiyang Shen",
            "Gang Huang",
            "Yun Ma",
            "Xiang Jing"
        ],
        "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games",
        "abstract": "arXiv:2601.08462v1 Announce Type: new  Abstract: As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.",
        "arxiv_id": "2601.08462",
        "ARXIVID": "2601.08462",
        "COMMENT": "Does not match any specific criteria but is related to evaluating social behaviors in LLMs, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.09213": {
        "authors": [
            "Jialu Li",
            "Taiyan Zhou"
        ],
        "title": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion",
        "abstract": "arXiv:2601.09213v1 Announce Type: new  Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.   We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.",
        "arxiv_id": "2601.09213",
        "ARXIVID": "2601.09213",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision and neuroscience applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08673": {
        "authors": [
            "Didier Sornette",
            "Sandro Claudio Lera",
            "Ke Wu"
        ],
        "title": "Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock",
        "abstract": "arXiv:2601.08673v1 Announce Type: new  Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.",
        "arxiv_id": "2601.08673",
        "ARXIVID": "2601.08673",
        "COMMENT": "Does not match any specific criteria but is related to AI alignment and societal impacts.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.08834": {
        "authors": [
            "Yufeng Zhong",
            "Lei Chen",
            "Zhixiong Zeng",
            "Xuanle Zhao",
            "Deyang Jiang",
            "Liming Zheng",
            "Jing Huang",
            "Haibo Qiu",
            "Peng Shi",
            "Siqi Yang",
            "Lin Ma"
        ],
        "title": "Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR",
        "abstract": "arXiv:2601.08834v1 Announce Type: new  Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.",
        "arxiv_id": "2601.08834",
        "ARXIVID": "2601.08834",
        "COMMENT": "Does not match any specific criteria but is related to OCR and reinforcement learning, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09263": {
        "authors": [
            "Yucheng Li",
            "Xiaofan Wang",
            "Junyi Wang",
            "Yijie Li",
            "Xi Zhu",
            "Mubai Du",
            "Dian Sheng",
            "Wei Zhang",
            "Fan Zhang"
        ],
        "title": "BrainSegNet: A Novel Framework for Whole-Brain MRI Parcellation Enhanced by Large Models",
        "abstract": "arXiv:2601.09263v1 Announce Type: new  Abstract: Whole-brain parcellation from MRI is a critical yet challenging task due to the complexity of subdividing the brain into numerous small, irregular shaped regions. Traditionally, template-registration methods were used, but recent advances have shifted to deep learning for faster workflows. While large models like the Segment Anything Model (SAM) offer transferable feature representations, they are not tailored for the high precision required in brain parcellation. To address this, we propose BrainSegNet, a novel framework that adapts SAM for accurate whole-brain parcellation into 95 regions. We enhance SAM by integrating U-Net skip connections and specialized modules into its encoder and decoder, enabling fine-grained anatomical precision. Key components include a hybrid encoder combining U-Net skip connections with SAM's transformer blocks, a multi-scale attention decoder with pyramid pooling for varying-sized structures, and a boundary refinement module to sharpen edges. Experimental results on the Human Connectome Project (HCP) dataset demonstrate that BrainSegNet outperforms several state-of-the-art methods, achieving higher accuracy and robustness in complex, multi-label parcellation.",
        "arxiv_id": "2601.09263",
        "ARXIVID": "2601.09263",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09433": {
        "authors": [
            "David Reid",
            "Ognjen Arandjelovic"
        ],
        "title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?",
        "abstract": "arXiv:2601.09433v1 Announce Type: new  Abstract: Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.",
        "arxiv_id": "2601.09433",
        "ARXIVID": "2601.09433",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and the application of Vision Transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09606": {
        "authors": [
            "Manning Gao",
            "Leheng Zhang",
            "Shiqin Han",
            "Haifeng Hu",
            "Yuncheng Jiang",
            "Sijie Mai"
        ],
        "title": "GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis",
        "abstract": "arXiv:2601.09606v1 Announce Type: new  Abstract: Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.",
        "arxiv_id": "2601.09606",
        "ARXIVID": "2601.09606",
        "COMMENT": "Does not match any specific criteria but is related to multimodal sentiment analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.08000": {
        "authors": [
            "Can Jin",
            "Rui Wu",
            "Tong Che",
            "Qixin Zhang",
            "Hongwu Peng",
            "Jiahui Zhao",
            "Zhenting Wang",
            "Wenqi Wei",
            "Ligong Han",
            "Zhao Zhang",
            "Yuan Cao",
            "Ruixiang Tang",
            "Dimitris N. Metaxas"
        ],
        "title": "Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety",
        "abstract": "arXiv:2601.08000v1 Announce Type: new  Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.",
        "arxiv_id": "2601.08000",
        "ARXIVID": "2601.08000",
        "COMMENT": "Does not match any specific criteria but is related to safety in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.08867": {
        "authors": [
            "Qingyu Liu",
            "Zhongjie Ba",
            "Jianmin Guo",
            "Qiu Wang",
            "Zhibo Wang",
            "Jie Shi",
            "Kui Ren"
        ],
        "title": "R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images",
        "abstract": "arXiv:2601.08867v1 Announce Type: new  Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.   In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.   Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.",
        "arxiv_id": "2601.08867",
        "ARXIVID": "2601.08867",
        "COMMENT": "Does not match any specific criteria but is related to fake image detection and generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09121": {
        "authors": [
            "Xin Yuan",
            "Meiqi Wan",
            "Wei Liu",
            "Xin Xu",
            "Zheng Wang"
        ],
        "title": "Beyond Seen Bounds: Class-Centric Polarization for Single-Domain Generalized Deep Metric Learning",
        "abstract": "arXiv:2601.09121v1 Announce Type: new  Abstract: Single-domain generalized deep metric learning (SDG-DML) faces the dual challenge of both category and domain shifts during testing, limiting real-world applications. Therefore, aiming to learn better generalization ability on both unseen categories and domains is a realistic goal for the SDG-DML task. To deliver the aspiration, existing SDG-DML methods employ the domain expansion-equalization strategy to expand the source data and generate out-of-distribution samples. However, these methods rely on proxy-based expansion, which tends to generate samples clustered near class proxies, failing to simulate the broad and distant domain shifts encountered in practice. To alleviate the problem, we propose CenterPolar, a novel SDG-DML framework that dynamically expands and constrains domain distributions to learn a generalizable DML model for wider target domain distributions. Specifically, \\textbf{CenterPolar} contains two collaborative class-centric polarization phases: (1) Class-Centric Centrifugal Expansion ($C^3E$) and (2) Class-Centric Centripetal Constraint ($C^4$). In the first phase, $C^3E$ drives the source domain distribution by shifting the source data away from class centroids using centrifugal expansion to generalize to more unseen domains. In the second phase, to consolidate domain-invariant class information for the generalization ability to unseen categories, $C^4$ pulls all seen and unseen samples toward their class centroids while enforcing inter-class separation via centripetal constraint. Extensive experimental results on widely used CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, and Office-Home datasets demonstrate the superiority and effectiveness of our CenterPolar over existing state-of-the-art methods. The code will be released after acceptance.",
        "arxiv_id": "2601.09121",
        "ARXIVID": "2601.09121",
        "COMMENT": "Does not match any specific criteria but is related to metric learning and domain generalization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09316": {
        "authors": [
            "Xinming Fang",
            "Chaoyan Huang",
            "Juncheng Li",
            "Jun Wang",
            "Jun Shi",
            "Guixu Zhang"
        ],
        "title": "Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction",
        "abstract": "arXiv:2601.09316v1 Announce Type: new  Abstract: Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.",
        "arxiv_id": "2601.09316",
        "ARXIVID": "2601.09316",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in medical imaging and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09209": {
        "authors": [
            "Qiang Hu",
            "Qimei Wang",
            "Yingjie Guo",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy",
        "abstract": "arXiv:2601.09209v1 Announce Type: new  Abstract: White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.",
        "arxiv_id": "2601.09209",
        "ARXIVID": "2601.09209",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in cross-modal learning and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.08276": {
        "authors": [
            "Zhiyuan Yao",
            "Zishan Xu",
            "Yifu Guo",
            "Zhiguang Han",
            "Cheng Yang",
            "Shuo Zhang",
            "Weinan Zhang",
            "Xingshan Zeng",
            "Weiwen Liu"
        ],
        "title": "ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web",
        "abstract": "arXiv:2601.08276v1 Announce Type: new  Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.",
        "arxiv_id": "2601.08276",
        "ARXIVID": "2601.08276",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in agent ecosystems and routing in collaborative networks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.09531": {
        "authors": [
            "Yue Yao",
            "Ruining Yang",
            "Tom Gedeon"
        ],
        "title": "Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server",
        "abstract": "arXiv:2601.09531v1 Announce Type: new  Abstract: We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.",
        "arxiv_id": "2601.09531",
        "ARXIVID": "2601.09531",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in domain adaptation and data-centric methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.08187": {
        "authors": [
            "Zijun Di",
            "Bin Lu",
            "Huquan Kang",
            "Luoyi Fu",
            "Jiaxin Ding",
            "Xiaoying Gan",
            "Lei Zhou",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "title": "Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression",
        "abstract": "arXiv:2601.08187v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.",
        "arxiv_id": "2601.08187",
        "ARXIVID": "2601.08187",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in machine learning and reasoning improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.08684": {
        "authors": [
            "Paolo Italiani",
            "David Gimeno-Gomez",
            "Luca Ragazzi",
            "Gianluca Moro",
            "Paolo Rosso"
        ],
        "title": "MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection",
        "abstract": "arXiv:2601.08684v1 Announce Type: new  Abstract: Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.",
        "arxiv_id": "2601.08684",
        "ARXIVID": "2601.08684",
        "COMMENT": "Does not match any specific criteria but is related to multimodal reasoning and hate detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2601.09191": {
        "authors": [
            "Qizhen Lan",
            "Aaron Choi",
            "Jun Ma",
            "Bo Wang",
            "Zhaogming Zhao",
            "Xiaoqian Jiang",
            "Yu-Chun Hsu"
        ],
        "title": "From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows",
        "abstract": "arXiv:2601.09191v1 Announce Type: new  Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.",
        "arxiv_id": "2601.09191",
        "ARXIVID": "2601.09191",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in machine learning applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
{
    "2601.05172": {
        "authors": [
            "Haoyu Zhao",
            "Akide Liu",
            "Zeyu Zhang",
            "Weijie Wang",
            "Feng Chen",
            "Ruihan Zhu",
            "Gholamreza Haffari",
            "Bohan Zhuang"
        ],
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "abstract": "arXiv:2601.05172v1 Announce Type: new  Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "arxiv_id": "2601.05172",
        "ARXIVID": "2601.05172",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on spatial reasoning improvements in embodied question answering and novel test-time reasoning framework.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.05201": {
        "authors": [
            "William Rudman",
            "Michal Golovanevsky",
            "Dana Arad",
            "Yonatan Belinkov",
            "Ritambhara Singh",
            "Carsten Eickhoff",
            "Kyle Mahowald"
        ],
        "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models",
        "abstract": "arXiv:2601.05201v1 Announce Type: new  Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.",
        "arxiv_id": "2601.05201",
        "ARXIVID": "2601.05201",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores prompt-induced hallucinations in vision-language models, providing insights into their mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.04824": {
        "authors": [
            "Oriol Rabasseda",
            "Zenjie Li",
            "Kamal Nasrollahi",
            "Sergio Escalera"
        ],
        "title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models",
        "abstract": "arXiv:2601.04824v1 Announce Type: new  Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.   Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.",
        "arxiv_id": "2601.04824",
        "ARXIVID": "2601.04824",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for vehicle surveillance action retrieval and evaluates multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.04035": {
        "authors": [
            "Yilin Cao",
            "Yufeng Zhong",
            "Zhixiong Zeng",
            "Liming Zheng",
            "Jing Huang",
            "Haibo Qiu",
            "Peng Shi",
            "Wenji Mao",
            "Wan Guanglu"
        ],
        "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
        "abstract": "arXiv:2601.04035v1 Announce Type: new  Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.",
        "arxiv_id": "2601.04035",
        "ARXIVID": "2601.04035",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel world-model-based framework for GUI agents, addressing long-horizon tasks with spatial awareness.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.04359": {
        "authors": [
            "Kunyang Li",
            "Mubarak Shah",
            "Yuzhang Shang"
        ],
        "title": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache",
        "abstract": "arXiv:2601.04359v1 Announce Type: new  Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.",
        "arxiv_id": "2601.04359",
        "ARXIVID": "2601.04359",
        "COMMENT": "Matches criterion 5. The paper introduces a method for efficient unified autoregressive video generation, integrating video understanding and generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.04777": {
        "authors": [
            "Shurong Zheng",
            "Yousong Zhu",
            "Hongyin Zhao",
            "Fan Yang",
            "Yufei Zhan",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models",
        "abstract": "arXiv:2601.04777v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.",
        "arxiv_id": "2601.04777",
        "ARXIVID": "2601.04777",
        "COMMENT": "Matches criterion 2. The paper focuses on multimodal large language models (MLLMs) and their application to multi-image visual grounding, which aligns with vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.05116": {
        "authors": [
            "Zirui Wu",
            "Zeren Jiang",
            "Martin R. Oswald",
            "Jie Song"
        ],
        "title": "From Rays to Projections: Better Inputs for Feed-Forward View Synthesis",
        "abstract": "arXiv:2601.05116v1 Announce Type: new  Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Pl\\\"ucker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.",
        "arxiv_id": "2601.05116",
        "ARXIVID": "2601.05116",
        "COMMENT": "Matches criterion 6 (Video Understanding) and partially criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on improving view synthesis with projective conditioning and pretraining strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.05251": {
        "authors": [
            "Zeren Jiang",
            "Chuanxia Zheng",
            "Iro Laina",
            "Diane Larlus",
            "Andrea Vedaldi"
        ],
        "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
        "abstract": "arXiv:2601.05251v1 Announce Type: new  Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
        "arxiv_id": "2601.05251",
        "ARXIVID": "2601.05251",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on 4D mesh reconstruction and tracking from monocular video, which involves video-based tasks and novel methodologies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.05175": {
        "authors": [
            "Shuming Liu",
            "Mingchen Zhuge",
            "Changsheng Zhao",
            "Jun Chen",
            "Lemeng Wu",
            "Zechun Liu",
            "Chenchen Zhu",
            "Zhipeng Cai",
            "Chong Zhou",
            "Haozhe Liu",
            "Ernie Chang",
            "Saksham Suri",
            "Hongyu Xu",
            "Qi Qian",
            "Wei Wen",
            "Balakrishnan Varadarajan",
            "Zhuang Liu",
            "Hu Xu",
            "Florian Bordes",
            "Raghuraman Krishnamoorthi",
            "Bernard Ghanem",
            "Vikas Chandra",
            "Yunyang Xiong"
        ],
        "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
        "abstract": "arXiv:2601.05175v1 Announce Type: new  Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
        "arxiv_id": "2601.05175",
        "ARXIVID": "2601.05175",
        "COMMENT": "Matches criterion 6. The paper proposes a novel framework for video understanding tasks, focusing on reasoning and efficiency improvements.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.05083": {
        "authors": [
            "Ellington Kirby",
            "Alexandre Boulch",
            "Yihong Xu",
            "Yuan Yin",
            "Gilles Puy",
            "\\'Eloi Zablocki",
            "Andrei Bursuc",
            "Spyros Gidaris",
            "Renaud Marlet",
            "Florent Bartoccioni",
            "Anh-Quan Cao",
            "Nermin Samet",
            "Tuan-Hung VU",
            "Matthieu Cord"
        ],
        "title": "Driving on Registers",
        "abstract": "arXiv:2601.05083v1 Announce Type: new  Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.",
        "arxiv_id": "2601.05083",
        "ARXIVID": "2601.05083",
        "COMMENT": "Matches criterion 3. The paper introduces a novel transformer-based architecture for autonomous driving, which involves embodied AI and spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.04946": {
        "authors": [
            "Subhadeep Roy",
            "Gagan Bhatia",
            "Steffen Eger"
        ],
        "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics",
        "abstract": "arXiv:2601.04946v1 Announce Type: new  Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.",
        "arxiv_id": "2601.04946",
        "ARXIVID": "2601.04946",
        "COMMENT": "Matches criterion 5. The paper explores multimodal evaluation metrics and introduces a new metric for text-to-image models, which aligns with integrating image understanding and LLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03595": {
        "authors": [
            "Yi Fang",
            "Wenjie Wang",
            "Mingfeng Xue",
            "Boyi Deng",
            "Fengli Xu",
            "Dayiheng Liu",
            "Fuli Feng"
        ],
        "title": "Controllable LLM Reasoning via Sparse Autoencoder-Based Steering",
        "abstract": "arXiv:2601.03595v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\\% absolute accuracy improvement.",
        "arxiv_id": "2601.03595",
        "ARXIVID": "2601.03595",
        "COMMENT": "Does not match any specific criteria. Focuses on controlling reasoning strategies in LRMs, which is tangential to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.03672": {
        "authors": [
            "Chen Zhang",
            "Kepu Zhang",
            "Jiatong Zhang",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "title": "Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction",
        "abstract": "arXiv:2601.03672v1 Announce Type: new  Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.",
        "arxiv_id": "2601.03672",
        "ARXIVID": "2601.03672",
        "COMMENT": "Does not match any specific criteria. Focuses on query correction and reasoning, which is tangential to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.04899": {
        "authors": [
            "Hongyi Li",
            "William Ward Armstrong",
            "Jun Xu"
        ],
        "title": "Rotation-Robust Regression with Convolutional Model Trees",
        "abstract": "arXiv:2601.04899v1 Announce Type: new  Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.",
        "arxiv_id": "2601.04899",
        "ARXIVID": "2601.04899",
        "COMMENT": "Does not match any specific criterion. Focuses on rotation-robust regression for image inputs, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.04428": {
        "authors": [
            "Donghang Lyu",
            "Marius Staring",
            "Hildo Lamb",
            "Mariya Doneva"
        ],
        "title": "CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction",
        "abstract": "arXiv:2601.04428v1 Announce Type: new  Abstract: In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.",
        "arxiv_id": "2601.04428",
        "ARXIVID": "2601.04428",
        "COMMENT": "Does not match any specific criterion. Focuses on cardiac MRI reconstruction, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.04588": {
        "authors": [
            "Yusri Al-Sanaani",
            "Rebecca Thornhill",
            "Sreeraman Rajan"
        ],
        "title": "3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks",
        "abstract": "arXiv:2601.04588v1 Announce Type: new  Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.",
        "arxiv_id": "2601.04588",
        "ARXIVID": "2601.04588",
        "COMMENT": "Does not match any specific criterion. Focuses on 3D conditional image synthesis for medical imaging, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
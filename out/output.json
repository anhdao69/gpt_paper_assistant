{
    "2512.13604": {
        "authors": [
            "Jianxiong Gao",
            "Zhaoxi Chen",
            "Xian Liu",
            "Junhao Zhuang",
            "Chengming Xu",
            "Jianfeng Feng",
            "Yu Qiao",
            "Yanwei Fu",
            "Chenyang Si",
            "Ziwei Liu"
        ],
        "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
        "abstract": "arXiv:2512.13604v1 Announce Type: new  Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
        "arxiv_id": "2512.13604",
        "ARXIVID": "2512.13604",
        "COMMENT": "Matches criteria 5 and 6 as it focuses on multimodal controllable video generation and long-term video understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2512.13684": {
        "authors": [
            "Daniel Zoran",
            "Nikhil Parthasarathy",
            "Yi Yang",
            "Drew A Hudson",
            "Joao Carreira",
            "Andrew Zisserman"
        ],
        "title": "Recurrent Video Masked Autoencoders",
        "abstract": "arXiv:2512.13684v1 Announce Type: new  Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.",
        "arxiv_id": "2512.13684",
        "ARXIVID": "2512.13684",
        "COMMENT": "Matches criteria 6 as it introduces a novel video representation learning approach for video-based tasks with strong spatio-temporal modeling.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.12822": {
        "authors": [
            "Yongyuan Liang",
            "Xiyao Wang",
            "Yuanchen Ju",
            "Jianwei Yang",
            "Furong Huang"
        ],
        "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding",
        "abstract": "arXiv:2512.12822v1 Announce Type: new  Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.",
        "arxiv_id": "2512.12822",
        "ARXIVID": "2512.12822",
        "COMMENT": "Matches criteria 1 and 2 as it presents a novel unified transformer architecture for 3D spatial understanding and integrates spatial-linguistic fusion.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.12799": {
        "authors": [
            "Zhe Liu",
            "Runhui Huang",
            "Rui Yang",
            "Siming Yan",
            "Zining Wang",
            "Lu Hou",
            "Di Lin",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
        "abstract": "arXiv:2512.12799v1 Announce Type: new  Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
        "arxiv_id": "2512.12799",
        "ARXIVID": "2512.12799",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a 4D MLLM for autonomous driving tasks, integrating vision, language, and action.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2512.13122": {
        "authors": [
            "Vivek Alumootil",
            "Tuan-Anh Vu",
            "M. Khalid Jawed"
        ],
        "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass",
        "abstract": "arXiv:2512.13122v1 Announce Type: new  Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R",
        "arxiv_id": "2512.13122",
        "ARXIVID": "2512.13122",
        "COMMENT": "Matches criteria 3 as it introduces a novel framework for joint dense point tracking and 3D reconstruction in dynamic scenes.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.12751": {
        "authors": [
            "Zhenya Yang",
            "Zhe Liu",
            "Yuxiang Lu",
            "Liping Hou",
            "Chenxuan Miao",
            "Siyi Peng",
            "Bailan Feng",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
        "abstract": "arXiv:2512.12751v1 Announce Type: new  Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
        "arxiv_id": "2512.12751",
        "ARXIVID": "2512.12751",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on physics-aware driving video generation with novel methodologies for 4D occupancy and video generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.12756": {
        "authors": [
            "Yue Jiang",
            "Dingkang Yang",
            "Minghao Han",
            "Jinghang Han",
            "Zizhi Chen",
            "Yizhou Liu",
            "Mingcheng Li",
            "Peng Zhai",
            "Lihua Zhang"
        ],
        "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning",
        "abstract": "arXiv:2512.12756v1 Announce Type: new  Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.",
        "arxiv_id": "2512.12756",
        "ARXIVID": "2512.12756",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a full-modality benchmark for multimodal large language models, enabling evaluation across image, video, audio, and text, with a focus on reasoning and generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.13560": {
        "authors": [
            "Shun Maeda",
            "Chunzhi Gu",
            "Koichiro Kamide",
            "Katsuya Hotta",
            "Shangce Gao",
            "Chao Zhang"
        ],
        "title": "3D Human-Human Interaction Anomaly Detection",
        "abstract": "arXiv:2512.13560v1 Announce Type: new  Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.",
        "arxiv_id": "2512.13560",
        "ARXIVID": "2512.13560",
        "COMMENT": "Matches criteria 3 as it introduces a novel task and method for anomaly detection in human-human interactions, focusing on spatial and temporal dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.12372": {
        "authors": [
            "Peixuan Zhang",
            "Zijian Jia",
            "Kaiqi Liu",
            "Shuchen Weng",
            "Si Li",
            "Boxin Shi"
        ],
        "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative",
        "abstract": "arXiv:2512.12372v1 Announce Type: new  Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.",
        "arxiv_id": "2512.12372",
        "ARXIVID": "2512.12372",
        "COMMENT": "Matches criteria 5 as it proposes a novel method for multi-shot video generation using storyboard-anchored generation, integrating video understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.13238": {
        "authors": [
            "Francesco Ragusa",
            "Michele Mazzamuto",
            "Rosario Forte",
            "Irene D'Ambra",
            "James Fort",
            "Jakob Engel",
            "Antonino Furnari",
            "Giovanni Maria Farinella"
        ],
        "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance",
        "abstract": "arXiv:2512.13238v1 Announce Type: new  Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.",
        "arxiv_id": "2512.13238",
        "ARXIVID": "2512.13238",
        "COMMENT": "Matches criteria 2 and 6 as it introduces a novel egocentric video-language dataset and benchmark for evaluating multimodal large language models, focusing on video understanding and vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2512.13608": {
        "authors": [
            "Felix J. Dorfner",
            "Manon A. Dorster",
            "Ryan Connolly",
            "Oscar Gentilhomme",
            "Edward Gibbs",
            "Steven Graham",
            "Seth Wander",
            "Thomas Schultz",
            "Manisha Bahl",
            "Dania Daye",
            "Albert E. Kim",
            "Christopher P. Bridge"
        ],
        "title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis",
        "abstract": "arXiv:2512.13608v1 Announce Type: new  Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.   To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.   Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.   For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%.   Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.",
        "arxiv_id": "2512.13608",
        "ARXIVID": "2512.13608",
        "COMMENT": "Matches criteria 4 as it focuses on a foundation model for medical imaging, specifically Digital Breast Tomosynthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13297": {
        "authors": [
            "Zhenghao Zhu",
            "Chuxue Cao",
            "Sirui Han",
            "Yuanfeng Song",
            "Xing Chen",
            "Caleb Chen Cao",
            "Yike Guo"
        ],
        "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
        "abstract": "arXiv:2512.13297v1 Announce Type: new  Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
        "arxiv_id": "2512.13297",
        "ARXIVID": "2512.13297",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for evaluating LMMs in medical data analysis, focusing on multi-modal insight discovery.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13192": {
        "authors": [
            "Zhuo Chen",
            "Chengqun Yang",
            "Zhuo Su",
            "Zheng Lv",
            "Jingnan Gao",
            "Xiaoyuan Zhang",
            "Xiaokang Yang",
            "Yichao Yan"
        ],
        "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling",
        "abstract": "arXiv:2512.13192v1 Announce Type: new  Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.",
        "arxiv_id": "2512.13192",
        "ARXIVID": "2512.13192",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a generative framework for illumination-aware face modeling, integrating image understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13303": {
        "authors": [
            "Zhihang Liu",
            "Xiaoyi Bao",
            "Pandeng Li",
            "Junjie Zhou",
            "Zhaohe Liao",
            "Yefei He",
            "Kaixun Jiang",
            "Chen-Wei Xie",
            "Yun Zheng",
            "Hongtao Xie"
        ],
        "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
        "abstract": "arXiv:2512.13303v1 Announce Type: new  Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.",
        "arxiv_id": "2512.13303",
        "ARXIVID": "2512.13303",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a pipeline combining MLLMs and diffusion models for creative table visualization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.12360": {
        "authors": [
            "Yufei Yin",
            "Qianke Meng",
            "Minghao Chen",
            "Jiajun Ding",
            "Zhenwei Shao",
            "Zhou Yu"
        ],
        "title": "VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding",
        "abstract": "arXiv:2512.12360v1 Announce Type: new  Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.",
        "arxiv_id": "2512.12360",
        "ARXIVID": "2512.12360",
        "COMMENT": "Matches criterion 6 as it introduces a novel framework for long-form video understanding, focusing on adaptive reasoning and memory construction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13276": {
        "authors": [
            "Yan Li",
            "Lin Liu",
            "Xiaopeng Zhang",
            "Wei Xue",
            "Wenhan Luo",
            "Yike Guo",
            "Qi Tian"
        ],
        "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing",
        "abstract": "arXiv:2512.13276v1 Announce Type: new  Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation",
        "arxiv_id": "2512.13276",
        "ARXIVID": "2512.13276",
        "COMMENT": "Matches criterion 5 as it proposes a fine-grained image editing framework combining multi-modal reasoning and dense gradient flow optimization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13636": {
        "authors": [
            "Haoyu Fu",
            "Diankun Zhang",
            "Zongchuang Zhao",
            "Jianfeng Cui",
            "Hongwei Xie",
            "Bing Wang",
            "Guang Chen",
            "Dingkang Liang",
            "Xiang Bai"
        ],
        "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
        "abstract": "arXiv:2512.13636v1 Announce Type: new  Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
        "arxiv_id": "2512.13636",
        "ARXIVID": "2512.13636",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for vision-language-action models in autonomous driving, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.12089": {
        "authors": [
            "Zihu Wang",
            "Boxun Xu",
            "Yuxuan Xia",
            "Peng Li"
        ],
        "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering",
        "abstract": "arXiv:2512.12089v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.",
        "arxiv_id": "2512.12089",
        "ARXIVID": "2512.12089",
        "COMMENT": "Matches criterion 2 as it addresses hallucination mitigation in large vision-language models, which is a key challenge in VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2512.13290": {
        "authors": [
            "Shu Yu",
            "Chaochao Lu"
        ],
        "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models",
        "abstract": "arXiv:2512.13290v1 Announce Type: new  Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.",
        "arxiv_id": "2512.13290",
        "ARXIVID": "2512.13290",
        "COMMENT": "Matches criterion 5 as it explores techniques for integrating image understanding and generation tasks with diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13507": {
        "authors": [
            "Siyan Chen",
            "Yanfei Chen",
            "Ying Chen",
            "Zhuo Chen",
            "Feng Cheng",
            "Xuyan Chi",
            "Jian Cong",
            "Qinpeng Cui",
            "Qide Dong",
            "Junliang Fan",
            "Jing Fang",
            "Zetao Fang",
            "Chengjian Feng",
            "Han Feng",
            "Mingyuan Gao",
            "Yu Gao",
            "Qiushan Guo",
            "Boyang Hao",
            "Qingkai Hao",
            "Bibo He",
            "Qian He",
            "Tuyen Hoang",
            "Ruoqing Hu",
            "Xi Hu",
            "Weilin Huang",
            "Zhaoyang Huang",
            "Zhongyi Huang",
            "Siqi Jiang",
            "Wei Jiang",
            "Yunpu Jiang",
            "Zhuo Jiang",
            "Ashley Kim",
            "Jianan Kong",
            "Zhichao Lai",
            "Shanshan Lao",
            "Ai Li",
            "Feiya Li",
            "Gen Li",
            "Huixia Li",
            "JiaShi Li",
            "Liang Li",
            "Ming Li",
            "Tao Li",
            "Xian Li",
            "Xiaojie Li",
            "Xiaoyang Li",
            "Xingxing Li",
            "Yameng Li",
            "Yifu Li",
            "Yiying Li",
            "Chao Liang",
            "Ying Liang",
            "Zhiqiang Liang",
            "Wang Liao",
            "Yalin Liao",
            "Heng Lin",
            "Kengyu Lin",
            "Shanchuan Lin",
            "Xi Lin",
            "Zhijie Lin",
            "Feng Ling",
            "Fangfang Liu",
            "Gaohong Liu",
            "Jiawei Liu",
            "Jie Liu",
            "Shouda Liu",
            "Shu Liu",
            "Sichao Liu",
            "Songwei Liu",
            "Xin Liu",
            "Xue Liu",
            "Yibo Liu",
            "Zikun Liu",
            "Zuxi Liu",
            "Junlin Lyu",
            "Lecheng Lyu",
            "Qian Lyu",
            "Han Mu",
            "Xiaonan Nie",
            "Jingzhe Ning",
            "Xitong Pan",
            "Yanghua Peng",
            "Lianke Qin",
            "Xueqiong Qu",
            "Yuxi Ren",
            "Yuchen Shen",
            "Guang Shi",
            "Lei Shi",
            "Yan Song",
            "Yinglong Song",
            "Fan Sun",
            "Li Sun",
            "Renfei Sun",
            "Zeyu Sun",
            "Wenjing Tang",
            "Zirui Tao",
            "Feng Wang",
            "Furui Wang",
            "Jinran Wang",
            "Junkai Wang",
            "Ke Wang",
            "Kexin Wang",
            "Qingyi Wang",
            "Rui Wang",
            "Sen Wang",
            "Shuai Wang",
            "Tingru Wang",
            "Weichen Wang",
            "Xin Wang",
            "Yanhui Wang",
            "Yue Wang",
            "Yuping Wang",
            "Yuxuan Wang",
            "Ziyu Wang",
            "Guoqiang Wei",
            "Wanru Wei",
            "Di Wu",
            "Guohong Wu",
            "Hanjie Wu",
            "Jian Wu",
            "Jie Wu",
            "Ruolan Wu",
            "Xinglong Wu",
            "Yonghui Wu",
            "Ruiqi Xia",
            "Liang Xiang",
            "Fei Xiao",
            "XueFeng Xiao",
            "Pan Xie",
            "Shuangyi Xie",
            "Shuang Xu",
            "Jinlan Xue",
            "Bangbang Yang",
            "Ceyuan Yang",
            "Jiaqi Yang",
            "Runkai Yang",
            "Tao Yang",
            "Yang Yang",
            "Yihang Yang",
            "ZhiXian Yang",
            "Ziyan Yang",
            "Yifan Yao",
            "Zilyu Ye",
            "Bowen Yu",
            "Chujie Yuan",
            "Linxiao Yuan",
            "Sichun Zeng",
            "Weihong Zeng",
            "Xuejiao Zeng",
            "Yan Zeng",
            "Chuntao Zhang",
            "Heng Zhang",
            "Jingjie Zhang",
            "Kuo Zhang",
            "Liang Zhang",
            "Liying Zhang",
            "Manlin Zhang",
            "Ting Zhang",
            "Weida Zhang",
            "Xiaohe Zhang",
            "Xinyan Zhang",
            "Yan Zhang",
            "Yuan Zhang",
            "Zixiang Zhang",
            "Fengxuan Zhao",
            "Huating Zhao",
            "Yang Zhao",
            "Hao Zheng",
            "Jianbin Zheng",
            "Xiaozheng Zheng",
            "Yangyang Zheng",
            "Yijie Zheng",
            "Jiexin Zhou",
            "Kuan Zhu",
            "Shenhan Zhu",
            "Wenjia Zhu",
            "Benhui Zou",
            "Feilong Zuo"
        ],
        "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
        "abstract": "arXiv:2512.13507v1 Announce Type: new  Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
        "arxiv_id": "2512.13507",
        "ARXIVID": "2512.13507",
        "COMMENT": "Matches criterion 2 as it presents a novel architecture for joint audio-video generation, which is a multimodal large language model application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13674": {
        "authors": [
            "Yiyi Cai",
            "Xuangeng Chu",
            "Xiwei Gao",
            "Sitong Gong",
            "Yifei Huang",
            "Caixin Kang",
            "Kunhang Li",
            "Haiyang Liu",
            "Ruicong Liu",
            "Yun Liu",
            "Dianwen Ng",
            "Zixiong Su",
            "Erwin Wu",
            "Yuhan Wu",
            "Dingkun Yan",
            "Tianyu Yan",
            "Chang Zeng",
            "Bo Zheng",
            "You Zhou"
        ],
        "title": "Towards Interactive Intelligence for Digital Humans",
        "abstract": "arXiv:2512.13674v1 Announce Type: new  Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
        "arxiv_id": "2512.13674",
        "ARXIVID": "2512.13674",
        "COMMENT": "Matches criteria 2 as it introduces a multimodal framework for interactive digital humans, integrating reasoning, speaking, and acting, which aligns with vision and multimodal LLM interests.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.12703": {
        "authors": [
            "Boyuan Li",
            "Sipeng Zheng",
            "Bin Cao",
            "Ruihua Song",
            "Zongqing Lu"
        ],
        "title": "Robust Motion Generation using Part-level Reliable Data from Videos",
        "abstract": "arXiv:2512.12703v1 Announce Type: new  Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.   To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as \"credible\". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.   In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/",
        "arxiv_id": "2512.12703",
        "ARXIVID": "2512.12703",
        "COMMENT": "Matches criteria 6 as it proposes a robust method for motion generation from videos, addressing challenges in video understanding and motion quality.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.12177": {
        "authors": [
            "Aydin Ayanzadeh",
            "Tim Oates"
        ],
        "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation",
        "abstract": "arXiv:2512.12177v1 Announce Type: new  Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.",
        "arxiv_id": "2512.12177",
        "ARXIVID": "2512.12177",
        "COMMENT": "Matches criteria 1 as it focuses on spatial intelligence by transforming floor plans into navigable knowledge graphs for indoor navigation, which is relevant to embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.12692": {
        "authors": [
            "Mahir Labib Dihan",
            "Tanzima Hashem",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
        "abstract": "arXiv:2512.12692v1 Announce Type: new  Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
        "arxiv_id": "2512.12692",
        "ARXIVID": "2512.12692",
        "COMMENT": "Matches criteria 3 as it introduces a novel tree-search framework for autonomous agents in web environments, addressing challenges in embodied AI with strategic exploration and backtracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.13639": {
        "authors": [
            "Michal Nazarczuk",
            "Thomas Tanay",
            "Arthur Moreau",
            "Zhensong Zhang",
            "Eduardo P\\'erez-Pellitero"
        ],
        "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All",
        "abstract": "arXiv:2512.13639v1 Announce Type: new  Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.",
        "arxiv_id": "2512.13639",
        "ARXIVID": "2512.13639",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it introduces a comprehensive benchmark and dataset for novel view synthesis, which is relevant to 3D vision.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.12675": {
        "authors": [
            "Yuran Wang",
            "Bohan Zeng",
            "Chengzhuo Tong",
            "Wenxuan Liu",
            "Yang Shi",
            "Xiaochen Ma",
            "Hao Liang",
            "Yuanxing Zhang",
            "Wentao Zhang"
        ],
        "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
        "abstract": "arXiv:2512.12675v1 Announce Type: new  Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.",
        "arxiv_id": "2512.12675",
        "ARXIVID": "2512.12675",
        "COMMENT": "Matches criterion 5 as it focuses on subject-driven image generation with a unified understanding-generation model, integrating image understanding and generation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.12963": {
        "authors": [
            "Luan Thanh Trinh",
            "Kenji Doi",
            "Atsuki Osanai"
        ],
        "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer",
        "abstract": "arXiv:2512.12963v1 Announce Type: new  Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.",
        "arxiv_id": "2512.12963",
        "ARXIVID": "2512.12963",
        "COMMENT": "Matches criterion 5 as it focuses on content-style disentanglement in diffusion models, which involves integrating image understanding and generation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.12492": {
        "authors": [
            "Shengkai Xu",
            "Hsiang Lun Kao",
            "Tianxiang Xu",
            "Honghui Zhang",
            "Junqiao Wang",
            "Runmeng Ding",
            "Guanyu Liu",
            "Tianyu Shi",
            "Zhenyu Yu",
            "Guofeng Pan",
            "Ziqian Bi",
            "Yuqi Ouyang"
        ],
        "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings",
        "abstract": "arXiv:2512.12492v1 Announce Type: new  Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.",
        "arxiv_id": "2512.12492",
        "ARXIVID": "2512.12492",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for zero-shot polyp detection in challenging conditions, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.13159": {
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Jie Hao",
            "Joo Hyuk Jeon",
            "Heng Ji",
            "Dilek Hakkani-T\\\"ur",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
        "abstract": "arXiv:2512.13159v1 Announce Type: new  Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
        "arxiv_id": "2512.13159",
        "ARXIVID": "2512.13159",
        "COMMENT": "Matches criteria 2 as it explores reinforcement learning to enhance conversational capabilities in language models, focusing on reasoning and interaction, which aligns with vision and multimodal LLM interests.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.13313": {
        "authors": [
            "Kling Team",
            "Jialu Chen",
            "Yikang Ding",
            "Zhixue Fang",
            "Kun Gai",
            "Yuan Gao",
            "Kang He",
            "Jingyun Hua",
            "Boyuan Jiang",
            "Mingming Lao",
            "Xiaohan Li",
            "Hui Liu",
            "Jiwen Liu",
            "Xiaoqiang Liu",
            "Yuan Liu",
            "Shun Lu",
            "Yongsen Mao",
            "Yingchao Shao",
            "Huafeng Shi",
            "Xiaoyu Shi",
            "Peiqin Sun",
            "Songlin Tang",
            "Pengfei Wan",
            "Chao Wang",
            "Xuebo Wang",
            "Haoxian Zhang",
            "Yuanxing Zhang",
            "Yan Zhou"
        ],
        "title": "KlingAvatar 2.0 Technical Report",
        "abstract": "arXiv:2512.13313v1 Announce Type: new  Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
        "arxiv_id": "2512.13313",
        "ARXIVID": "2512.13313",
        "COMMENT": "Matches criterion 5 as it focuses on multimodal instruction fusion for long-form video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.12534": {
        "authors": [
            "Qi Sun",
            "Can Wang",
            "Jiaxiang Shang",
            "Wensen Feng",
            "Jing Liao"
        ],
        "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation",
        "abstract": "arXiv:2512.12534v1 Announce Type: new  Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.",
        "arxiv_id": "2512.12534",
        "ARXIVID": "2512.12534",
        "COMMENT": "Matches criterion 5 as it combines text-driven generation with 3D animation, involving multimodal integration.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.13177": {
        "authors": [
            "Minghui Hou",
            "Wei-Hsing Huang",
            "Shaofeng Liang",
            "Daizong Liu",
            "Tai-Hao Wen",
            "Gang Wang",
            "Runwei Guan",
            "Weiping Ding"
        ],
        "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
        "abstract": "arXiv:2512.13177v1 Announce Type: new  Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
        "arxiv_id": "2512.13177",
        "ARXIVID": "2512.13177",
        "COMMENT": "Matches criterion 2 as it explores multimodal vision-language models for autonomous driving, integrating multiple modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.12977": {
        "authors": [
            "Shengling Qin",
            "Hao Yu",
            "Chenxin Wu",
            "Zheng Li",
            "Yizhong Cao",
            "Zhengyang Zhuge",
            "Yuxin Zhou",
            "Wentao Yao",
            "Yi Zhang",
            "Zhengheng Wang",
            "Shuai Bai",
            "Jianwei Zhang",
            "Junyang Lin"
        ],
        "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference",
        "abstract": "arXiv:2512.12977v1 Announce Type: new  Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.",
        "arxiv_id": "2512.12977",
        "ARXIVID": "2512.12977",
        "COMMENT": "Matches criterion 2 as it focuses on improving vision-language inference efficiency, relevant to multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.13072": {
        "authors": [
            "Zizhi Chen",
            "Yizhen Gao",
            "Minghao Han",
            "Yizhou Liu",
            "Zhaoyu Chen",
            "Dingkang Yang",
            "Lihua Zhang"
        ],
        "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models",
        "abstract": "arXiv:2512.13072v1 Announce Type: new  Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.",
        "arxiv_id": "2512.13072",
        "ARXIVID": "2512.13072",
        "COMMENT": "Matches criterion 2 as it explores multimodal vision-language models in the medical domain, with novel continual learning strategies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.13680": {
        "authors": [
            "Tianye Ding",
            "Yiming Xie",
            "Yiqing Liang",
            "Moitreya Chatterjee",
            "Pedro Miraldo",
            "Huaizu Jiang"
        ],
        "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
        "abstract": "arXiv:2512.13680v1 Announce Type: new  Abstract: Recent feed-forward reconstruction models like VGGT and $\\pi^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$",
        "arxiv_id": "2512.13680",
        "ARXIVID": "2512.13680",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for streaming 4D reconstruction, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.12205": {
        "authors": [
            "Peizheng Li",
            "Ioannis Mavromatis",
            "Ajith Sahadevan",
            "Tim Farnham",
            "Adnan Aijaz",
            "Aftab Khan"
        ],
        "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection",
        "abstract": "arXiv:2512.12205v1 Announce Type: new  Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.",
        "arxiv_id": "2512.12205",
        "ARXIVID": "2512.12205",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset for visual monitoring and spatio-temporal drift detection, relevant to benchmarks in embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2512.13427": {
        "authors": [
            "Noa Cohen",
            "Nurit Spingarn-Eliezer",
            "Inbar Huberman-Spiegelglas",
            "Tomer Michaeli"
        ],
        "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
        "abstract": "arXiv:2512.13427v1 Announce Type: new  Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
        "arxiv_id": "2512.13427",
        "ARXIVID": "2512.13427",
        "COMMENT": "Does not match any specific criteria but is relevant to bias detection in text-to-image models, which is tangentially related to vision-language models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2512.13095": {
        "authors": [
            "Feng Zhang",
            "Zezhong Tan",
            "Xinhong Ma",
            "Ziqiang Dong",
            "Xi Leng",
            "Jianfei Zhao",
            "Xin Sun",
            "Yang Yang"
        ],
        "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
        "abstract": "arXiv:2512.13095v1 Announce Type: new  Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
        "arxiv_id": "2512.13095",
        "ARXIVID": "2512.13095",
        "COMMENT": "Does not match any specific criteria. Focuses on reinforcement learning with adaptive hints, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.13689": {
        "authors": [
            "Yuanwen Yue",
            "Damien Robert",
            "Jianyuan Wang",
            "Sunghwan Hong",
            "Jan Dirk Wegner",
            "Christian Rupprecht",
            "Konrad Schindler"
        ],
        "title": "LitePT: Lighter Yet Stronger Point Transformer",
        "abstract": "arXiv:2512.13689v1 Announce Type: new  Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
        "arxiv_id": "2512.13689",
        "ARXIVID": "2512.13689",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and 3D point cloud processing, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.12906": {
        "authors": [
            "Zhimao Peng",
            "Enguang Wang",
            "Xialei Liu",
            "Ming-Ming Cheng"
        ],
        "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection",
        "abstract": "arXiv:2512.12906v1 Announce Type: new  Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.",
        "arxiv_id": "2512.12906",
        "ARXIVID": "2512.12906",
        "COMMENT": "Does not closely match any specific criterion but is related to out-of-distribution detection, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.12303": {
        "authors": [
            "Yang Ou",
            "Xiongwei Zhao",
            "Xinye Yang",
            "Yihan Wang",
            "Yicheng Di",
            "Rong Yuan",
            "Xieyuanli Chen",
            "Xu Zhu"
        ],
        "title": "OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation",
        "abstract": "arXiv:2512.12303v1 Announce Type: new  Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.",
        "arxiv_id": "2512.12303",
        "ARXIVID": "2512.12303",
        "COMMENT": "Does not closely match any specific criterion but is related to domain adaptation in semantic segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.12108": {
        "authors": [
            "Dashti A. Ali",
            "Aras T. Asaad",
            "Jacob J. Peoples",
            "Mohammad Hamghalam",
            "Alex Robins",
            "Mane Piliposyan",
            "Richard K. G. Do",
            "Natalie Gangai",
            "Yun S. Chun",
            "Ahmad Bashir Barekzai",
            "Jayasree Chakraborty",
            "Hala Khasawneh",
            "Camila Vilela",
            "Natally Horvat",
            "Jo\\~ao Miranda",
            "Alice C. Wei",
            "Amber L. Simpson"
        ],
        "title": "A Novel Patch-Based TDA Approach for Computed Tomography",
        "abstract": "arXiv:2512.12108v1 Announce Type: new  Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.",
        "arxiv_id": "2512.12108",
        "ARXIVID": "2512.12108",
        "COMMENT": "Does not match any specific criteria but is relevant to medical imaging and machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.13154": {
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Joo Hyuk Jeon",
            "Jie Hao",
            "Heng Ji",
            "Dilek Hakkani-T\\\"ur",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
        "abstract": "arXiv:2512.13154v1 Announce Type: new  Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
        "arxiv_id": "2512.13154",
        "ARXIVID": "2512.13154",
        "COMMENT": "Does not match any specific criteria but is generally relevant to multi-agent systems and conversational AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.12175": {
        "authors": [
            "Haoyang Chen",
            "Richong Zhang",
            "Junfan Chen"
        ],
        "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective",
        "abstract": "arXiv:2512.12175v1 Announce Type: new  Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.",
        "arxiv_id": "2512.12175",
        "ARXIVID": "2512.12175",
        "COMMENT": "Does not match any specific criteria but is generally relevant to machine learning and in-context learning, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.12357": {
        "authors": [
            "Zishen Song",
            "Yongjian Zhu",
            "Dong Wang",
            "Hongzhan Liu",
            "Lingyu Jiang",
            "Yongxing Duan",
            "Zehua Zhang",
            "Sihan Li",
            "Jiarui Li"
        ],
        "title": "TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection",
        "abstract": "arXiv:2512.12357v1 Announce Type: new  Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.",
        "arxiv_id": "2512.12357",
        "ARXIVID": "2512.12357",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in agriculture.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
{
    "2601.03781": {
        "authors": [
            "Xiaokun Sun",
            "Zezhong Wu",
            "Zewen Ding",
            "Linli Xu"
        ],
        "title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction",
        "abstract": "arXiv:2601.03781v1 Announce Type: new  Abstract: Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.",
        "arxiv_id": "2601.03781",
        "ARXIVID": "2601.03781",
        "COMMENT": "Matches criterion 6 as it enhances Video Large Language Models (VideoLLMs) with a novel training objective for video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.03579": {
        "authors": [
            "Tianyi Shang",
            "Pengjie Xu",
            "Zhaojun Deng",
            "Zhenyu Li",
            "Zhicong Chen",
            "Lijun Wu"
        ],
        "title": "SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization",
        "abstract": "arXiv:2601.03579v1 Announce Type: new  Abstract: Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.",
        "arxiv_id": "2601.03579",
        "ARXIVID": "2601.03579",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for cross-modal localization, relevant to embodied AI and spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.03590": {
        "authors": [
            "Zhongbin Guo",
            "Zhen Yang",
            "Yushan Li",
            "Xinyue Zhang",
            "Wenyu Gao",
            "Jiacheng Wang",
            "Chengzhi Li",
            "Xiangrui Liu",
            "Ping Jian"
        ],
        "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
        "abstract": "arXiv:2601.03590v1 Announce Type: new  Abstract: Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
        "arxiv_id": "2601.03590",
        "ARXIVID": "2601.03590",
        "COMMENT": "Matches criterion 1 as it introduces a benchmark for spatial intelligence in LLMs, focusing on symbolic reasoning for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.03309": {
        "authors": [
            "Jianke Zhang",
            "Xiaoyu Chen",
            "Qiuyue Wang",
            "Mingsheng Li",
            "Yanjiang Guo",
            "Yucheng Hu",
            "Jiajun Zhang",
            "Shuai Bai",
            "Junyang Lin",
            "Jianyu Chen"
        ],
        "title": "VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models",
        "abstract": "arXiv:2601.03309v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.",
        "arxiv_id": "2601.03309",
        "ARXIVID": "2601.03309",
        "COMMENT": "Matches criterion 3 as it investigates Vision-Language-Action models and their performance in embodied AI tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.04073": {
        "authors": [
            "Zhihao Zhu",
            "Jiafeng Liang",
            "Shixin Jiang",
            "Jinlan Fu",
            "Ming Liu",
            "Guanglu Sun",
            "See-Kiong Ng",
            "Bing Qin"
        ],
        "title": "Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts",
        "abstract": "arXiv:2601.04073v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.",
        "arxiv_id": "2601.04073",
        "ARXIVID": "2601.04073",
        "COMMENT": "Matches criterion 2 as it explores reasoning consistency in Large Multimodal Models (LMMs), focusing on vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2601.04185": {
        "authors": [
            "Xudong Jiang",
            "Fangjinhua Wang",
            "Silvano Galliani",
            "Christoph Vogel",
            "Marc Pollefeys"
        ],
        "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
        "abstract": "arXiv:2601.04185v1 Announce Type: new  Abstract: Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
        "arxiv_id": "2601.04185",
        "ARXIVID": "2601.04185",
        "COMMENT": "Matches criterion 4 as it proposes a novel image-based representation for visual localization, achieving state-of-the-art results.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.03467": {
        "authors": [
            "Hengjia Li",
            "Liming Jiang",
            "Qing Yan",
            "Yizhi Song",
            "Hao Kang",
            "Zichuan Liu",
            "Xin Lu",
            "Boxi Wu",
            "Deng Cai"
        ],
        "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
        "abstract": "arXiv:2601.03467v1 Announce Type: new  Abstract: Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.",
        "arxiv_id": "2601.03467",
        "ARXIVID": "2601.03467",
        "COMMENT": "Matches criterion 5 as it focuses on integrating image editing tasks with reasoning-centric reinforcement learning and multimodal generative models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.03955": {
        "authors": [
            "Xu Zhang",
            "Cheng Da",
            "Huan Yang",
            "Kun Gai",
            "Ming Lu",
            "Zhan Ma"
        ],
        "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
        "abstract": "arXiv:2601.03955v1 Announce Type: new  Abstract: Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
        "arxiv_id": "2601.03955",
        "ARXIVID": "2601.03955",
        "COMMENT": "Matches criterion 4 as it focuses on hierarchical residuals in visual tokenizers, which are foundational for autoregressive image generation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2601.03655": {
        "authors": [
            "Jinsong Zhou",
            "Yihua Du",
            "Xinli Xu",
            "Luozhou Wang",
            "Zijie Zhuang",
            "Yehang Zhang",
            "Shuaibo Li",
            "Xiaojun Hu",
            "Bolan Su",
            "Ying-cong Chen"
        ],
        "title": "VideoMemory: Toward Consistent Video Generation via Memory Integration",
        "abstract": "arXiv:2601.03655v1 Announce Type: new  Abstract: Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.",
        "arxiv_id": "2601.03655",
        "ARXIVID": "2601.03655",
        "COMMENT": "Matches criterion 6 as it focuses on consistent video generation with a novel memory integration framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03500": {
        "authors": [
            "Yuxuan Xia",
            "Siheng Wang",
            "Peng Li"
        ],
        "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
        "abstract": "arXiv:2601.03500v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
        "arxiv_id": "2601.03500",
        "ARXIVID": "2601.03500",
        "COMMENT": "Matches criterion 2 as it addresses object hallucination in large vision-language models with a novel decoding algorithm.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.04035": {
        "authors": [
            "Yilin Cao",
            "Yufeng Zhong",
            "Zhixiong Zeng",
            "Liming Zheng",
            "Jing Huang",
            "Haibo Qiu",
            "Peng Shi",
            "Wenji Mao",
            "Wan Guanglu"
        ],
        "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
        "abstract": "arXiv:2601.04035v1 Announce Type: new  Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.",
        "arxiv_id": "2601.04035",
        "ARXIVID": "2601.04035",
        "COMMENT": "Matches criterion 3 as it introduces a world-model-based framework for GUI agents, focusing on long-horizon tasks and spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03905": {
        "authors": [
            "Cheng Qian",
            "Emre Can Acikgoz",
            "Bingxuan Li",
            "Xiusi Chen",
            "Yuji Zhang",
            "Bingxiang He",
            "Qinyu Luo",
            "Dilek Hakkani-T\\\"ur",
            "Gokhan Tur",
            "Yunzhu Li",
            "Heng Ji",
            "Heng Ji"
        ],
        "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
        "abstract": "arXiv:2601.03905v1 Announce Type: new  Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
        "arxiv_id": "2601.03905",
        "ARXIVID": "2601.03905",
        "COMMENT": "Matches criterion 3 as it discusses the use of world models for foresight in embodied agents, addressing challenges in anticipatory cognition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03733": {
        "authors": [
            "Xiaoxian Shen",
            "Yuhui Zhang",
            "Sahithi Ankireddy",
            "Xiaohan Wang",
            "Maya Varma",
            "Henry Guo",
            "Curtis Langlotz",
            "Serena Yeung-Levy"
        ],
        "title": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
        "abstract": "arXiv:2601.03733v1 Announce Type: new  Abstract: Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
        "arxiv_id": "2601.03733",
        "ARXIVID": "2601.03733",
        "COMMENT": "Matches criterion 2 as it explores a multimodal vision-language model for radiology image comparison.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03362": {
        "authors": [
            "Xiang Zhang",
            "Yang Zhang",
            "Lukas Mehl",
            "Markus Gross",
            "Christopher Schroers"
        ],
        "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
        "abstract": "arXiv:2601.03362v1 Announce Type: new  Abstract: Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
        "arxiv_id": "2601.03362",
        "ARXIVID": "2601.03362",
        "COMMENT": "Matches criterion 4 as it focuses on improving depth and stereo vision, which are foundational aspects of computer vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.03595": {
        "authors": [
            "Yi Fang",
            "Wenjie Wang",
            "Mingfeng Xue",
            "Boyi Deng",
            "Fengli Xu",
            "Dayiheng Liu",
            "Fuli Feng"
        ],
        "title": "Controllable LLM Reasoning via Sparse Autoencoder-Based Steering",
        "abstract": "arXiv:2601.03595v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\\% absolute accuracy improvement.",
        "arxiv_id": "2601.03595",
        "ARXIVID": "2601.03595",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reasoning and control in large models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.03662": {
        "authors": [
            "Su-Hyeon Kim",
            "Hyundong Jin",
            "Yejin Lee",
            "Yo-Sub Han"
        ],
        "title": "How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs",
        "abstract": "arXiv:2601.03662v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.",
        "arxiv_id": "2601.03662",
        "ARXIVID": "2601.03662",
        "COMMENT": "Does not match any specific criteria but may be of general interest due to its focus on safety in reasoning models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.03718": {
        "authors": [
            "Wenyong Lia",
            "Qi Jiang",
            "Weijian Hu",
            "Kailun Yang",
            "Zhanjun Zhang",
            "Wenjun Tian",
            "Kaiwei Wang",
            "Jian Bai"
        ],
        "title": "Towards Real-world Lens Active Alignment with Unlabeled Data via Domain Adaptation",
        "abstract": "arXiv:2601.03718v1 Announce Type: new  Abstract: Active Alignment (AA) is a key technology for the large-scale automated assembly of high-precision optical systems. Compared with labor-intensive per-model on-device calibration, a digital-twin pipeline built on optical simulation offers a substantial advantage in generating large-scale labeled data. However, complex imaging conditions induce a domain gap between simulation and real-world images, limiting the generalization of simulation-trained models. To address this, we propose augmenting a simulation baseline with minimal unlabeled real-world images captured at random misalignment positions, mitigating the gap from a domain adaptation perspective. We introduce Domain Adaptive Active Alignment (DA3), which utilizes an autoregressive domain transformation generator and an adversarial-based feature alignment strategy to distill real-world domain information via self-supervised learning. This enables the extraction of domain-invariant image degradation features to facilitate robust misalignment prediction. Experiments on two lens types reveal that DA3 improves accuracy by 46% over a purely simulation pipeline. Notably, it approaches the performance achieved with precisely labeled real-world data collected on 3 lens samples, while reducing on-device data collection time by 98.7%. The results demonstrate that domain adaptation effectively endows simulation-trained models with robust real-world performance, validating the digital-twin pipeline as a practical solution to significantly enhance the efficiency of large-scale optical assembly.",
        "arxiv_id": "2601.03718",
        "ARXIVID": "2601.03718",
        "COMMENT": "Does not match any specific criteria but may be of general interest due to its focus on domain adaptation for real-world applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.03507": {
        "authors": [
            "Qiang Zhang",
            "Tong Xiao",
            "Haroun Habeeb",
            "Larissa Laich",
            "Sofien Bouaziz",
            "Patrick Snape",
            "Wenjing Zhang",
            "Matthew Cioffi",
            "Peizhao Zhang",
            "Pavel Pidlypenskyi",
            "Winnie Lin",
            "Luming Ma",
            "Mengjiao Wang",
            "Kunpeng Li",
            "Chengjiang Long",
            "Steven Song",
            "Martin Prazak",
            "Alexander Sjoholm",
            "Ajinkya Deogade",
            "Jaebong Lee",
            "Julio Delgado Mangas",
            "Amaury Aubel"
        ],
        "title": "REFA: Real-time Egocentric Facial Animations for Virtual Reality",
        "abstract": "arXiv:2601.03507v1 Announce Type: new  Abstract: We present a novel system for real-time tracking of facial expressions using egocentric views captured from a set of infrared cameras embedded in a virtual reality (VR) headset. Our technology facilitates any user to accurately drive the facial expressions of virtual characters in a non-intrusive manner and without the need of a lengthy calibration step. At the core of our system is a distillation based approach to train a machine learning model on heterogeneous data and labels coming form multiple sources, \\eg synthetic and real images. As part of our dataset, we collected 18k diverse subjects using a lightweight capture setup consisting of a mobile phone and a custom VR headset with extra cameras. To process this data, we developed a robust differentiable rendering pipeline enabling us to automatically extract facial expression labels. Our system opens up new avenues for communication and expression in virtual environments, with applications in video conferencing, gaming, entertainment, and remote collaboration.",
        "arxiv_id": "2601.03507",
        "ARXIVID": "2601.03507",
        "COMMENT": "Does not match any specific criteria but may be of general interest due to its focus on embodied agents and VR applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.03822": {
        "authors": [
            "Muyang Zhao",
            "Qi Qi",
            "Hao Sun"
        ],
        "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
        "abstract": "arXiv:2601.03822v1 Announce Type: new  Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.",
        "arxiv_id": "2601.03822",
        "ARXIVID": "2601.03822",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reasoning and optimization in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.03808": {
        "authors": [
            "Usha Shrestha",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "title": "From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs",
        "abstract": "arXiv:2601.03808v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.",
        "arxiv_id": "2601.03808",
        "ARXIVID": "2601.03808",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to LLMs and task-level reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
{
    "2512.05513": {
        "authors": [
            "Chinthani Sugandhika",
            "Chen Li",
            "Deepu Rajan",
            "Basura Fernando"
        ],
        "title": "Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning",
        "abstract": "arXiv:2512.05513v1 Announce Type: new  Abstract: Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to \"show what they know\" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.",
        "arxiv_id": "2512.05513",
        "ARXIVID": "2512.05513",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for spatio-temporal grounded reasoning in video-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.05965": {
        "authors": [
            "Hongyu Li",
            "Manyuan Zhang",
            "Dian Zheng",
            "Ziyu Guo",
            "Yimeng Jia",
            "Kaituo Feng",
            "Hao Yu",
            "Yexin Liu",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Linjiang Huang",
            "Hongsheng Li",
            "Si Liu"
        ],
        "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
        "abstract": "arXiv:2512.05965v1 Announce Type: new  Abstract: Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.",
        "arxiv_id": "2512.05965",
        "ARXIVID": "2512.05965",
        "COMMENT": "Matches criterion 2 as it explores a Multi-modal Large Language Model (MLLM) for iterative reasoning in image editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.05391": {
        "authors": [
            "Qingqiao Hu",
            "Weimin Lyu",
            "Meilong Xu",
            "Kehan Qi",
            "Xiaoling Hu",
            "Saumya Gupta",
            "Jiawei Zhou",
            "Chao Chen"
        ],
        "title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models",
        "abstract": "arXiv:2512.05391v1 Announce Type: new  Abstract: Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.",
        "arxiv_id": "2512.05391",
        "ARXIVID": "2512.05391",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with multimodal large language models for pathology.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2512.05272": {
        "authors": [
            "Ahmet Berke Gokmen",
            "Ajad Chhatkuli",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
        "abstract": "arXiv:2512.05272v1 Announce Type: new  Abstract: Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.",
        "arxiv_id": "2512.05272",
        "ARXIVID": "2512.05272",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding by reconstructing 4D scenes from monocular videos.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2512.05597": {
        "authors": [
            "Ruihong Yin",
            "Xuepeng Shi",
            "Oleksandr Bailo",
            "Marco Manfredi",
            "Theo Gevers"
        ],
        "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction",
        "abstract": "arXiv:2512.05597v1 Announce Type: new  Abstract: Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.",
        "arxiv_id": "2512.05597",
        "ARXIVID": "2512.05597",
        "COMMENT": "Matches criterion 4 as it focuses on efficient structured language models for 3D scene layout estimation, which is relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.05394": {
        "authors": [
            "Shizhan Liu",
            "Xinran Deng",
            "Zhuoyi Yang",
            "Jiayan Teng",
            "Xiaotao Gu",
            "Jie Tang"
        ],
        "title": "Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability",
        "abstract": "arXiv:2512.05394v1 Announce Type: new  Abstract: Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\\times$ speedup in text-to-video generation convergence and a 10\\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.",
        "arxiv_id": "2512.05394",
        "ARXIVID": "2512.05394",
        "COMMENT": "Matches criterion 6 as it focuses on improving video VAEs for text-to-video generation, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.05941": {
        "authors": [
            "Zhiyuan Jiang",
            "Shenghao Xie",
            "Wenyi Li",
            "Wenqiang Zu",
            "Peihang Li",
            "Jiahao Qiu",
            "Siqi Pei",
            "Lei Ma",
            "Tiejun Huang",
            "Mengdi Wang",
            "Shilong Liu"
        ],
        "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding",
        "abstract": "arXiv:2512.05941v1 Announce Type: new  Abstract: Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.",
        "arxiv_id": "2512.05941",
        "ARXIVID": "2512.05941",
        "COMMENT": "Matches criterion 1 as it introduces a novel spatial reasoning method for GUI agents using zooming, which is relevant to spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.05571": {
        "authors": [
            "Xingyu Zhang",
            "Anna Reithmeir",
            "Fryderyk K\\\"ogl",
            "Rickmer Braren",
            "Julia A. Schnabel",
            "Daniel M. Lang"
        ],
        "title": "MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging",
        "abstract": "arXiv:2512.05571v1 Announce Type: new  Abstract: Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.",
        "arxiv_id": "2512.05571",
        "ARXIVID": "2512.05571",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D medical image correspondence, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.05524": {
        "authors": [
            "Chinthani Sugandhika",
            "Chen Li",
            "Deepu Rajan",
            "Basura Fernando"
        ],
        "title": "VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation",
        "abstract": "arXiv:2512.05524v1 Announce Type: new  Abstract: Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.",
        "arxiv_id": "2512.05524",
        "ARXIVID": "2512.05524",
        "COMMENT": "Matches criterion 2 as it explores vision-language models for spatio-temporal scene graph generation, integrating multimodal features.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2512.05529": {
        "authors": [
            "Kunyi Yang",
            "Qingyu Wang",
            "Cheng Yuan",
            "Yutong Ban"
        ],
        "title": "See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors",
        "abstract": "arXiv:2512.05529v1 Announce Type: new  Abstract: Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.",
        "arxiv_id": "2512.05529",
        "ARXIVID": "2512.05529",
        "COMMENT": "Matches criterion 4. The paper uses vision foundation models for surgical scene segmentation, which aligns with the focus on foundation models in computer vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05137": {
        "authors": [
            "Yunfei Zhang",
            "Yizhuo He",
            "Yuanxun Shao",
            "Zhengtao Yao",
            "Haoyan Xu",
            "Junhao Dong",
            "Zhen Yao",
            "Zhikang Dong"
        ],
        "title": "ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images",
        "abstract": "arXiv:2512.05137v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.",
        "arxiv_id": "2512.05137",
        "ARXIVID": "2512.05137",
        "COMMENT": "Matches criterion 7 as it introduces a benchmark for evaluating vision-language models under challenging conditions, providing insights into their limitations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05824": {
        "authors": [
            "Hafsa Akebli (University of Udine",
            "Udine",
            "Italy)",
            "Adam Shephard (University of Warwick",
            "Coventry",
            "UK)",
            "Vincenzo Della Mea (University of Udine",
            "Udine",
            "Italy)",
            "Nasir Rajpoot (University of Warwick",
            "Coventry",
            "UK",
            "Histofy Ltd",
            "Coventry",
            "UK)"
        ],
        "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma",
        "abstract": "arXiv:2512.05824v1 Announce Type: new  Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.",
        "arxiv_id": "2512.05824",
        "ARXIVID": "2512.05824",
        "COMMENT": "Matches criterion 2 as it involves a multimodal oncology agent integrating vision-language models for mutation prediction.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05557": {
        "authors": [
            "Xingxi Yin",
            "Yicheng Li",
            "Gong Yan",
            "Chenglin Li",
            "Jian Zhao",
            "Cong Huang",
            "Yue Deng",
            "Yin Zhang"
        ],
        "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency",
        "abstract": "arXiv:2512.05557v1 Announce Type: new  Abstract: Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.",
        "arxiv_id": "2512.05557",
        "ARXIVID": "2512.05557",
        "COMMENT": "Matches criterion 5 as it involves multimodal datasets and disentangled control for visual storytelling, which integrates image understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05905": {
        "authors": [
            "Wenhao Yan",
            "Sheng Ye",
            "Zhuoyi Yang",
            "Jiayan Teng",
            "ZhenHui Dong",
            "Kairui Wen",
            "Xiaotao Gu",
            "Yong-Jin Liu",
            "Jie Tang"
        ],
        "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
        "abstract": "arXiv:2512.05905v1 Announce Type: new  Abstract: Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.",
        "arxiv_id": "2512.05905",
        "ARXIVID": "2512.05905",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and novel methods for character animation, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05492": {
        "authors": [
            "Qi Zhu",
            "Jingyi Zhang",
            "Naishan Zheng",
            "Wei Yu",
            "Jinghao Zhang",
            "Deyi Ji",
            "Feng Zhao"
        ],
        "title": "WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field",
        "abstract": "arXiv:2512.05492v1 Announce Type: new  Abstract: Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.",
        "arxiv_id": "2512.05492",
        "ARXIVID": "2512.05492",
        "COMMENT": "Matches criterion 6 as it focuses on video enhancement and temporal consistency, which is relevant to video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2512.05371": {
        "authors": [
            "Changwen Xing",
            "SamZaak Wong",
            "Xinlai Wan",
            "Yanfeng Lu",
            "Mengli Zhang",
            "Zebin Ma",
            "Lei Qi",
            "Zhengxiong Li",
            "Nan Guan",
            "Zhe Jiang",
            "Xi Wang",
            "Jun Yang"
        ],
        "title": "ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications",
        "abstract": "arXiv:2512.05371v1 Announce Type: new  Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).",
        "arxiv_id": "2512.05371",
        "ARXIVID": "2512.05371",
        "COMMENT": "Does not match any specific criteria. Focuses on retrieval-augmented reasoning for circuit design specifications, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.05139": {
        "authors": [
            "Yang Xiang",
            "Jingwen Zhong",
            "Yige Yan",
            "Petros Koutrakis",
            "Eric Garshick",
            "Meredith Franklin"
        ],
        "title": "Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models",
        "abstract": "arXiv:2512.05139v1 Announce Type: new  Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.",
        "arxiv_id": "2512.05139",
        "ARXIVID": "2512.05139",
        "COMMENT": "Does not match any specific criteria. Focuses on satellite image downscaling using generative models, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.05651": {
        "authors": [
            "Nan Zhong",
            "Mian Zou",
            "Yiran Xu",
            "Zhenxing Qian",
            "Xinpeng Zhang",
            "Baoyuan Wu",
            "Kede Ma"
        ],
        "title": "Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective",
        "abstract": "arXiv:2512.05651v1 Announce Type: new  Abstract: The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.",
        "arxiv_id": "2512.05651",
        "ARXIVID": "2512.05651",
        "COMMENT": "Does not match any specific criteria. Focuses on AI-generated image detection using camera metadata, which is tangentially related to computer vision but not directly to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.05925": {
        "authors": [
            "Federico Bianchi",
            "Yongchan Kwon",
            "Zachary Izzo",
            "Linjun Zhang",
            "James Zou"
        ],
        "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis",
        "abstract": "arXiv:2512.05925v1 Announce Type: new  Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.",
        "arxiv_id": "2512.05925",
        "ARXIVID": "2512.05925",
        "COMMENT": "Does not match any specific criteria. Focuses on error detection in AI papers using LLMs, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05859": {
        "authors": [
            "Abhijith Punnappurath",
            "Luxi Zhao",
            "Ke Zhao",
            "Hue Nguyen",
            "Radek Grzeszczuk",
            "Michael S. Brown"
        ],
        "title": "Edit-aware RAW Reconstruction",
        "abstract": "arXiv:2512.05859v1 Announce Type: new  Abstract: Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.",
        "arxiv_id": "2512.05859",
        "ARXIVID": "2512.05859",
        "COMMENT": "Does not match any specific criteria. Focuses on RAW image reconstruction and edit-aware loss functions, which are not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05152": {
        "authors": [
            "Kun Wang",
            "Donglin Di",
            "Tonghua Su",
            "Lei Fan"
        ],
        "title": "EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models",
        "abstract": "arXiv:2512.05152v1 Announce Type: new  Abstract: Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.",
        "arxiv_id": "2512.05152",
        "ARXIVID": "2512.05152",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05960": {
        "authors": [
            "Munsif Ali",
            "Najmul Hassan",
            "Lucia Ventura",
            "Davide Di Bari",
            "Simonepietro Canese"
        ],
        "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement",
        "abstract": "arXiv:2512.05960v1 Announce Type: new  Abstract: Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.",
        "arxiv_id": "2512.05960",
        "ARXIVID": "2512.05960",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and image enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05593": {
        "authors": [
            "Rong Wang",
            "Wei Mao",
            "Changsheng Lu",
            "Hongdong Li"
        ],
        "title": "Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer",
        "abstract": "arXiv:2512.05593v1 Announce Type: new  Abstract: We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.",
        "arxiv_id": "2512.05593",
        "ARXIVID": "2512.05593",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and animation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05937": {
        "authors": [
            "Anne Sielemann",
            "Valentin Barner",
            "Stefan Wolf",
            "Masoud Roschani",
            "Jens Ziehn",
            "Juergen Beyerer"
        ],
        "title": "Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception",
        "abstract": "arXiv:2512.05937v1 Announce Type: new  Abstract: Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].   Download: synset.de/datasets/synset-signset-ger/background-effect",
        "arxiv_id": "2512.05937",
        "ARXIVID": "2512.05937",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to explainable AI and classification in vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.05759": {
        "authors": [
            "Johannes Meyer",
            "Jasper Hoffmann",
            "Felix Schulz",
            "Dominik Merkle",
            "Daniel Buescher",
            "Alexander Reiterer",
            "Joschka Boedecker",
            "Wolfram Burgard"
        ],
        "title": "Label-Efficient Point Cloud Segmentation with Active Learning",
        "abstract": "arXiv:2512.05759v1 Announce Type: new  Abstract: Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.",
        "arxiv_id": "2512.05759",
        "ARXIVID": "2512.05759",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to active learning and 3D point cloud segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2512.05415": {
        "authors": [
            "Masato Shibukawa",
            "Fumi Yoshida",
            "Toshifumi Yanagisawa",
            "Takashi Ito",
            "Hirohisa Kurosaki",
            "Makoto Yoshikawa",
            "Kohki Kamiya",
            "Ji-an Jiang",
            "Wesley Fraser",
            "JJ Kavelaars",
            "Susan Benecchi",
            "Anne Verbiscer",
            "Akira Hatakeyama",
            "Hosei O",
            "Naoya Ozaki"
        ],
        "title": "Moving object detection from multi-depth images with an attention-enhanced CNN",
        "abstract": "arXiv:2512.05415v1 Announce Type: new  Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.",
        "arxiv_id": "2512.05415",
        "ARXIVID": "2512.05415",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
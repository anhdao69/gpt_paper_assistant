{
    "2602.15112": {
        "authors": [
            "Aniketh Garikaparthi",
            "Manasi Patwardhan",
            "Arman Cohan"
        ],
        "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
        "abstract": "arXiv:2602.15112v1 Announce Type: new  Abstract: We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
        "arxiv_id": "2602.15112",
        "ARXIVID": "2602.15112",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark (ResearchGym) for evaluating AI agents on research tasks, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2602.15294": {
        "authors": [
            "Ming Du",
            "Yanqi Luo",
            "Srutarshi Banerjee",
            "Michael Wojcik",
            "Jelena Popovic",
            "Mathew J. Cherukara"
        ],
        "title": "EAA: Automating materials characterization with vision language model agents",
        "abstract": "arXiv:2602.15294v1 Announce Type: new  Abstract: We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.",
        "arxiv_id": "2602.15294",
        "ARXIVID": "2602.15294",
        "COMMENT": "Matches criteria 1 and 2 closely as it presents a vision-language-model-driven agentic system for spatial reasoning and multimodal reasoning in experimental workflows.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.15539": {
        "authors": [
            "Qinglong Cao",
            "Yuntian Chen",
            "Chao Ma",
            "Xiaokang Yang"
        ],
        "title": "Dynamic Training-Free Fusion of Subject and Style LoRAs",
        "abstract": "arXiv:2602.15539v1 Announce Type: new  Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.",
        "arxiv_id": "2602.15539",
        "ARXIVID": "2602.15539",
        "COMMENT": "Matches criteria 5 as it showcases a technique for combining subject and style generation tasks using LoRAs, which aligns with integrating image generation and large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.15318": {
        "authors": [
            "Libo Zhang",
            "Zhaoning Zhang",
            "Wangyang Hong",
            "Peng Qiao",
            "Dongsheng Li"
        ],
        "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
        "abstract": "arXiv:2602.15318v1 Announce Type: new  Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.",
        "arxiv_id": "2602.15318",
        "ARXIVID": "2602.15318",
        "COMMENT": "Matches criterion 5 as it proposes a framework (Sparrow) for speculative decoding in video large language models, focusing on integrating video and language tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.15329": {
        "authors": [
            "Siwei Wen",
            "Zhangcheng Wang",
            "Xingjian Zhang",
            "Lei Huang",
            "Wenjun Wu"
        ],
        "title": "EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use",
        "abstract": "arXiv:2602.15329v1 Announce Type: new  Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.",
        "arxiv_id": "2602.15329",
        "ARXIVID": "2602.15329",
        "COMMENT": "Matches criterion 6 as it introduces a framework for online video understanding with hierarchical memory and adaptive tool use, addressing long-range reasoning in video tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.15580": {
        "authors": [
            "Hongxuan Wu",
            "Yukun Zhang",
            "Xueqing Zhou"
        ],
        "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
        "abstract": "arXiv:2602.15580v1 Announce Type: new  Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.",
        "arxiv_id": "2602.15580",
        "ARXIVID": "2602.15580",
        "COMMENT": "Matches criterion 2 as it provides an information-theoretic analysis of multimodal reasoning in vision-language models, offering insights into their internal mechanisms.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2602.15724": {
        "authors": [
            "Shutian Gu",
            "Chengkai Huang",
            "Ruoyu Wang",
            "Lina Yao"
        ],
        "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
        "abstract": "arXiv:2602.15724v1 Announce Type: new  Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
        "arxiv_id": "2602.15724",
        "ARXIVID": "2602.15724",
        "COMMENT": "Matches criterion 1 as it focuses on improving vision-and-language navigation through retrieval-augmented frameworks, which is relevant to spatial intelligence and embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2602.15727": {
        "authors": [
            "Hila Manor",
            "Rinon Gal",
            "Haggai Maron",
            "Tomer Michaeli",
            "Gal Chechik"
        ],
        "title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs",
        "abstract": "arXiv:2602.15727v1 Announce Type: new  Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\\{\\mathbf{a}$, $\\mathbf{a}'$, $\\mathbf{b}\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb",
        "arxiv_id": "2602.15727",
        "ARXIVID": "2602.15727",
        "COMMENT": "Matches criterion 5 as it combines image manipulation tasks with large language models using LoRA-based techniques.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.15819": {
        "authors": [
            "Hui Ren",
            "Yuval Alaluf",
            "Omer Bar Tal",
            "Alexander Schwing",
            "Antonio Torralba",
            "Yael Vinker"
        ],
        "title": "VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation",
        "abstract": "arXiv:2602.15819v1 Announce Type: new  Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.",
        "arxiv_id": "2602.15819",
        "ARXIVID": "2602.15819",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically sequential sketch generation using video models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.15124": {
        "authors": [
            "Shiyu Xuan",
            "Dongkai Wang",
            "Zechao Li",
            "Jinhui Tang"
        ],
        "title": "Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition",
        "abstract": "arXiv:2602.15124v1 Announce Type: new  Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.",
        "arxiv_id": "2602.15124",
        "ARXIVID": "2602.15124",
        "COMMENT": "Matches criterion 2 as it explores multi-modal large language models for zero-shot human-object interaction detection.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.15645": {
        "authors": [
            "Lucas Elbert Suryana",
            "Farah Bierenga",
            "Sanne van Buuren",
            "Pepijn Kooij",
            "Elsefien Tulleners",
            "Federico Scari",
            "Simeon Calvert",
            "Bart van Arem",
            "Arkady Zgonnikov"
        ],
        "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
        "abstract": "arXiv:2602.15645v1 Announce Type: new  Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.",
        "arxiv_id": "2602.15645",
        "ARXIVID": "2602.15645",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models in the context of automated driving, focusing on their reasoning and decision-making capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15584": {
        "authors": [
            "Flavien Armangeon",
            "Thibaud Ehret",
            "Enric Meinhardt-Llopis",
            "Rafael Grompone von Gioi",
            "Guillaume Thibault",
            "Marc Petit",
            "Gabriele Facciolo"
        ],
        "title": "An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment",
        "abstract": "arXiv:2602.15584v1 Announce Type: new  Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.",
        "arxiv_id": "2602.15584",
        "ARXIVID": "2602.15584",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset (IRIS-v2) for alignment tasks in industrial settings, which could be relevant for embodied/robotic AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15274": {
        "authors": [
            "Omid Madani",
            "J. Brian Burns",
            "Reza Eghbali",
            "Thomas L. Dean"
        ],
        "title": "When Remembering and Planning are Worth it: Navigating under Change",
        "abstract": "arXiv:2602.15274v1 Announce Type: new  Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.",
        "arxiv_id": "2602.15274",
        "ARXIVID": "2602.15274",
        "COMMENT": "Matches criterion 1 as it explores spatial reasoning and navigation in changing environments for embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15650": {
        "authors": [
            "Marco Salm\\`e",
            "Federico Siciliano",
            "Fabrizio Silvestri",
            "Paolo Soda",
            "Rosa Sicilia",
            "Valerio Guarrasi"
        ],
        "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
        "abstract": "arXiv:2602.15650v1 Announce Type: new  Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.",
        "arxiv_id": "2602.15650",
        "ARXIVID": "2602.15650",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with large language models for radiology report generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15720": {
        "authors": [
            "Hyunchan Moon",
            "Cheonjun Park",
            "Steven L. Waslander"
        ],
        "title": "ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT",
        "abstract": "arXiv:2602.15720v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\\% accuracy (+1.64 \\%) with 39.4\\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.",
        "arxiv_id": "2602.15720",
        "ARXIVID": "2602.15720",
        "COMMENT": "Matches criterion 4 as it focuses on improving Vision Transformers, a type of vision foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15315": {
        "authors": [
            "Tai Le-Gia",
            "Jaehyun Ahn"
        ],
        "title": "Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models",
        "abstract": "arXiv:2602.15315v1 Announce Type: new  Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.",
        "arxiv_id": "2602.15315",
        "ARXIVID": "2602.15315",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models in computer vision and their application to 3D medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.15355": {
        "authors": [
            "Rong Fu",
            "Jiekai Wu",
            "Haiyun Wei",
            "Yee Tan Jia",
            "Wenxin Zhang",
            "Yang Li",
            "Xiaowen Ma",
            "Wangyu Wu",
            "Simon Fong"
        ],
        "title": "DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles",
        "abstract": "arXiv:2602.15355v1 Announce Type: new  Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.",
        "arxiv_id": "2602.15355",
        "ARXIVID": "2602.15355",
        "COMMENT": "Does not match any specific criteria but discusses a novel method for 3D neural rendering, which is tangentially related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.15816": {
        "authors": [
            "Xiaoran Liu",
            "Istvan David"
        ],
        "title": "Developing AI Agents with Simulated Data: Why, what, and how?",
        "abstract": "arXiv:2602.15816v1 Announce Type: new  Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.",
        "arxiv_id": "2602.15816",
        "ARXIVID": "2602.15816",
        "COMMENT": "Does not match any specific criteria but discusses simulation-based synthetic data generation, which is tangentially related to embodied AI and benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15669": {
        "authors": [
            "Xiachong Feng",
            "Liang Zhao",
            "Weihong Zhong",
            "Yichong Huang",
            "Yuxuan Gu",
            "Lingpeng Kong",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra",
        "abstract": "arXiv:2602.15669v1 Announce Type: new  Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.",
        "arxiv_id": "2602.15669",
        "ARXIVID": "2602.15669",
        "COMMENT": "Does not match any specific criterion but is related to personality control in large language models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15270": {
        "authors": [
            "Farbod Abbasi",
            "Zachary Patterson",
            "Bilal Farooq"
        ],
        "title": "Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models",
        "abstract": "arXiv:2602.15270v1 Announce Type: new  Abstract: Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\\% and precision by 15\\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\\% increase in recall and 1\\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.",
        "arxiv_id": "2602.15270",
        "ARXIVID": "2602.15270",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for synthetic population generation, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15776": {
        "authors": [
            "Yiqin Yang",
            "Xu Yang",
            "Yuhua Jiang",
            "Ni Mu",
            "Hao Hu",
            "Runpeng Xie",
            "Ziyou Zhang",
            "Siyuan Li",
            "Yuan-Hua Ni",
            "Qianchuan Zhao",
            "Bo Xu"
        ],
        "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems",
        "abstract": "arXiv:2602.15776v1 Announce Type: new  Abstract: In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.",
        "arxiv_id": "2602.15776",
        "ARXIVID": "2602.15776",
        "COMMENT": "Does not match any specific criterion but is related to multi-agent systems and decision-making under partial observability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15785": {
        "authors": [
            "Jessica Hullman",
            "David Broska",
            "Huaman Sun",
            "Aaron Shaw"
        ],
        "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence",
        "abstract": "arXiv:2602.15785v1 Announce Type: new  Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.",
        "arxiv_id": "2602.15785",
        "ARXIVID": "2602.15785",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15553": {
        "authors": [
            "Gabriele Conte",
            "Alessio Mattiace",
            "Gianni Carmosino",
            "Potito Aghilar",
            "Giovanni Servedio",
            "Francesco Musicco",
            "Vito Walter Anelli",
            "Tommaso Di Noia",
            "Francesco Maria Donini"
        ],
        "title": "RUVA: Personalized Transparent On-Device Graph Reasoning",
        "abstract": "arXiv:2602.15553v1 Announce Type: new  Abstract: The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.",
        "arxiv_id": "2602.15553",
        "ARXIVID": "2602.15553",
        "COMMENT": "Does not closely match any specific criterion but is relevant to AI and reasoning systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15383": {
        "authors": [
            "Shuwei Li",
            "Lei Tan",
            "Robby T. Tan"
        ],
        "title": "Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation",
        "abstract": "arXiv:2602.15383v1 Announce Type: new  Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.",
        "arxiv_id": "2602.15383",
        "ARXIVID": "2602.15383",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.15490": {
        "authors": [
            "Youngwan Jin",
            "Incheol Park",
            "Yagiz Nalcakan",
            "Hyeongjin Ju",
            "Sanghyeop Yeo",
            "Shiho Kim"
        ],
        "title": "RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution",
        "abstract": "arXiv:2602.15490v1 Announce Type: new  Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra",
        "arxiv_id": "2602.15490",
        "ARXIVID": "2602.15490",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning through its focus on super-resolution for infrared images.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
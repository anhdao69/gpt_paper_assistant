{
    "2602.16918": {
        "authors": [
            "Shlok Mishra",
            "Tsung-Yu Lin",
            "Linda Wang",
            "Hongli Xu",
            "Yimin Liu",
            "Michael Hsu",
            "Chaitanya Ahuja",
            "Hao Yuan",
            "Jianpeng Cheng",
            "Hong-You Chen",
            "Haoyuan Xu",
            "Chao Li",
            "Abhijeet Awasthi",
            "Jihye Moon",
            "Don Husa",
            "Michael Ge",
            "Sumedha Singla",
            "Arkabandhu Chowdhury",
            "Phong Dingh",
            "Satya Narayan Shukla",
            "Yonghuan Yang",
            "David Jacobs",
            "Qi Guo",
            "Jun Xiao",
            "Xiangjun Fan",
            "Aashu Singh"
        ],
        "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
        "abstract": "arXiv:2602.16918v1 Announce Type: new  Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.",
        "arxiv_id": "2602.16918",
        "ARXIVID": "2602.16918",
        "COMMENT": "Matches criterion 2 as it explores a large-scale vision-language model with multimodal capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2602.17594": {
        "authors": [
            "Lance Ying",
            "Ryan Truong",
            "Prafull Sharma",
            "Kaiya Ivy Zhao",
            "Nathan Cloos",
            "Kelsey R. Allen",
            "Thomas L. Griffiths",
            "Katherine M. Collins",
            "Jos\\'e Hern\\'andez-Orallo",
            "Phillip Isola",
            "Samuel J. Gershman",
            "Joshua B. Tenenbaum"
        ],
        "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
        "abstract": "arXiv:2602.17594v1 Announce Type: new  Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
        "arxiv_id": "2602.17594",
        "ARXIVID": "2602.17594",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating general intelligence in AI systems, relevant to embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.17555": {
        "authors": [
            "Zixu Cheng",
            "Da Li",
            "Jian Hu",
            "Ziquan Liu",
            "Wei Li",
            "Shaogang Gong"
        ],
        "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
        "abstract": "arXiv:2602.17555v1 Announce Type: new  Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.",
        "arxiv_id": "2602.17555",
        "ARXIVID": "2602.17555",
        "COMMENT": "Matches criterion 6 as it focuses on video reasoning and introduces a novel methodology (GraphThinker) for video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.17605": {
        "authors": [
            "Jowaria Khan",
            "Anindya Sarkar",
            "Yevgeniy Vorobeychik",
            "Elizabeth Bondi-Kelly"
        ],
        "title": "Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery",
        "abstract": "arXiv:2602.17605v1 Announce Type: new  Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.",
        "arxiv_id": "2602.17605",
        "ARXIVID": "2602.17605",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for geospatial discovery, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.17182": {
        "authors": [
            "Jiwei Shan",
            "Zeyu Cai",
            "Yirui Li",
            "Yongbo Chen",
            "Lijun Han",
            "Yun-hui Liu",
            "Hesheng Wang",
            "Shing Shin Cheng"
        ],
        "title": "NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting",
        "abstract": "arXiv:2602.17182v1 Announce Type: new  Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.",
        "arxiv_id": "2602.17182",
        "ARXIVID": "2602.17182",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied AI in the context of non-rigid SLAM for endoscopy.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.17047": {
        "authors": [
            "Chaojie Yang",
            "Tian Li",
            "Yue Zhang",
            "Jun Gao"
        ],
        "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
        "abstract": "arXiv:2602.17047v1 Announce Type: new  Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.",
        "arxiv_id": "2602.17047",
        "ARXIVID": "2602.17047",
        "COMMENT": "Matches criterion 2 as it focuses on compressing diffusion transformers for text-to-image generation, which is relevant to multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16856": {
        "authors": [
            "Boda Lin",
            "Yongjie Zhu",
            "Wenyu Qin",
            "Meng Wang",
            "Pengfei Wan"
        ],
        "title": "Analytic Score Optimization for Multi Dimension Video Quality Assessment",
        "abstract": "arXiv:2602.16856v1 Announce Type: new  Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.",
        "arxiv_id": "2602.16856",
        "ARXIVID": "2602.16856",
        "COMMENT": "Matches criterion 6 as it introduces a new dataset (UltraVQA) and a novel method (ASO) for video quality assessment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.16915": {
        "authors": [
            "Zeyu Ren",
            "Xiang Li",
            "Yiran Wang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
        "abstract": "arXiv:2602.16915v1 Announce Type: new  Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
        "arxiv_id": "2602.16915",
        "ARXIVID": "2602.16915",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UW-StereoDepth-80K) and novel methods for underwater stereo depth estimation in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.17473": {
        "authors": [
            "Jiwei Shan",
            "Zeyu Cai",
            "Cheng-Tai Hsieh",
            "Yirui Li",
            "Hao Liu",
            "Lijun Han",
            "Hesheng Wang",
            "Shing Shin Cheng"
        ],
        "title": "4D Monocular Surgical Reconstruction under Arbitrary Camera Motions",
        "abstract": "arXiv:2602.17473v1 Announce Type: new  Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.",
        "arxiv_id": "2602.17473",
        "ARXIVID": "2602.17473",
        "COMMENT": "Matches criterion 3 as it proposes a new method for 4D reconstruction in surgical settings, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2602.17517": {
        "authors": [
            "Hanyuan Zhang",
            "Lucas He",
            "Runlong He",
            "Abdolrahim Kadkhodamohammadi",
            "Danail Stoyanov",
            "Brian R. Davidson",
            "Evangelos B. Mazomenos",
            "Matthew J. Clarkson"
        ],
        "title": "FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality",
        "abstract": "arXiv:2602.17517v1 Announce Type: new  Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.",
        "arxiv_id": "2602.17517",
        "ARXIVID": "2602.17517",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for surgical augmented reality, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.16902": {
        "authors": [
            "Juliusz Ziomek",
            "William Bankes",
            "Lorenz Wolf",
            "Shyam Sundhar Ramesh",
            "Xiaohang Tang",
            "Ilija Bogunovic"
        ],
        "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
        "abstract": "arXiv:2602.16902v1 Announce Type: new  Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.",
        "arxiv_id": "2602.16902",
        "ARXIVID": "2602.16902",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (LLM-Wikirace) for evaluating planning and reasoning in large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.16763": {
        "authors": [
            "Mubashara Akhtar",
            "Anka Reuel",
            "Prajna Soni",
            "Sanchit Ahuja",
            "Pawan Sasanka Ammanamanchi",
            "Ruchit Rawal",
            "Vil\\'em Zouhar",
            "Srishti Yadav",
            "Chenxi Whitehouse",
            "Dayeon Ki",
            "Jennifer Mickel",
            "Leshem Choshen",
            "Marek \\v{S}uppa",
            "Jan Batzner",
            "Jenny Chim",
            "Jeba Sania",
            "Yanan Long",
            "Hossein A. Rahmani",
            "Christina Knight",
            "Yiyang Nan",
            "Jyoutir Raj",
            "Yu Fan",
            "Shubham Singh",
            "Subramanyam Sahoo",
            "Eliya Habba",
            "Usman Gohar",
            "Siddhesh Pawar",
            "Robert Scholz",
            "Arjun Subramonian",
            "Jingwei Ni",
            "Mykel Kochenderfer",
            "Sanmi Koyejo",
            "Mrinmaya Sachan",
            "Stella Biderman",
            "Zeerak Talat",
            "Avijit Ghosh",
            "Irene Solaiman"
        ],
        "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
        "abstract": "arXiv:2602.16763v1 Announce Type: new  Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.",
        "arxiv_id": "2602.16763",
        "ARXIVID": "2602.16763",
        "COMMENT": "Matches criterion 3 as it systematically studies benchmark saturation and provides insights into designing more durable benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.17385": {
        "authors": [
            "Angelo Porrello",
            "Pietro Buzzega",
            "Felix Dangel",
            "Thomas Sommariva",
            "Riccardo Salami",
            "Lorenzo Bonicelli",
            "Simone Calderara"
        ],
        "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
        "abstract": "arXiv:2602.17385v1 Announce Type: new  Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.",
        "arxiv_id": "2602.17385",
        "ARXIVID": "2602.17385",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in machine learning methods.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.17308": {
        "authors": [
            "Hui Min Wong",
            "Philip Heesen",
            "Pascal Janetzky",
            "Martin Bendszus",
            "Stefan Feuerriegel"
        ],
        "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
        "abstract": "arXiv:2602.17308v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.",
        "arxiv_id": "2602.17308",
        "ARXIVID": "2602.17308",
        "COMMENT": "Does not match any specific criterion but is generally relevant to medical AI and diagnostic reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.16950": {
        "authors": [
            "Kibon Ku",
            "Talukder Z. Jubery",
            "Adarsh Krishnamurthy",
            "Baskar Ganapathysubramanian"
        ],
        "title": "HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs",
        "abstract": "arXiv:2602.16950v1 Announce Type: new  Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.",
        "arxiv_id": "2602.16950",
        "ARXIVID": "2602.16950",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and 3D reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.17418": {
        "authors": [
            "Diana Addae",
            "Diana Rogachova",
            "Nafiseh Kahani",
            "Masoud Barati",
            "Michael Christensen",
            "Chen Zhou"
        ],
        "title": "A Privacy by Design Framework for Large Language Model-Based Applications for Children",
        "abstract": "arXiv:2602.17418v1 Announce Type: new  Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.",
        "arxiv_id": "2602.17418",
        "ARXIVID": "2602.17418",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in applications of large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.16953": {
        "authors": [
            "Hejia Zhang",
            "Zhongming Yu",
            "Chia-Tung Ho",
            "Haoxing Ren",
            "Brucek Khailany",
            "Jishen Zhao"
        ],
        "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
        "abstract": "arXiv:2602.16953v1 Announce Type: new  Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.",
        "arxiv_id": "2602.16953",
        "ARXIVID": "2602.16953",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and agentic learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.17048": {
        "authors": [
            "Joongwon Chae",
            "Lihui Luo",
            "Yang Liu",
            "Runming Wang",
            "Dongmei Yu",
            "Zeming Liang",
            "Xi Yuan",
            "Dayan Zhang",
            "Zhenglin Chen",
            "Peiwu Qin",
            "Ilmoon Chae"
        ],
        "title": "StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection",
        "abstract": "arXiv:2602.17048v1 Announce Type: new  Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.   We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.   StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.",
        "arxiv_id": "2602.17048",
        "ARXIVID": "2602.17048",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
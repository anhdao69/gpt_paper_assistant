{
    "2601.17737": {
        "authors": [
            "Chenyu Mu",
            "Xin He",
            "Qu Yang",
            "Wanshun Chen",
            "Jiadi Yao",
            "Huang Liu",
            "Zihao Yi",
            "Bo Zhao",
            "Xingyu Chen",
            "Ruotian Ma",
            "Fanghua Ye",
            "Erkun Yang",
            "Cheng Deng",
            "Zhaopeng Tu",
            "Xiaolong Li",
            "Linus"
        ],
        "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
        "abstract": "arXiv:2601.17737v1 Announce Type: new  Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
        "arxiv_id": "2601.17737",
        "ARXIVID": "2601.17737",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on dialogue-to-cinematic video generation using multimodal techniques.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.18100": {
        "authors": [
            "James Tribble",
            "Hao Wang",
            "Si-En Hong",
            "Chaoyi Zhou",
            "Ashish Bastola",
            "Siyu Huang",
            "Abolfazl Razi"
        ],
        "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos",
        "abstract": "arXiv:2601.18100v1 Announce Type: new  Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.",
        "arxiv_id": "2601.18100",
        "ARXIVID": "2601.18100",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 6 (Video Understanding) due to its focus on spatial reasoning in long egocentric videos and video-based tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2601.17857": {
        "authors": [
            "Lan Yang",
            "Minghan Yang",
            "Ke Li",
            "Honggang Zhang",
            "Kaiyue Pang",
            "Yi-Zhe Song"
        ],
        "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
        "abstract": "arXiv:2601.17857v1 Announce Type: new  Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
        "arxiv_id": "2601.17857",
        "ARXIVID": "2601.17857",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates semantic reasoning with visual priors for fMRI-based image reconstruction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2601.18493": {
        "authors": [
            "Sara Tehrani",
            "Yonghao Xu",
            "Leif Haglund",
            "Amanda Berg",
            "Michael Felsberg"
        ],
        "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment",
        "abstract": "arXiv:2601.18493v1 Announce Type: new  Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.   To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.",
        "arxiv_id": "2601.18493",
        "ARXIVID": "2601.18493",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 5 (Integration of Image/Video and Large Language Models) due to its multimodal benchmark for disaster assessment and grounded reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.17895": {
        "authors": [
            "Bin Tan",
            "Changjiang Sun",
            "Xiage Qin",
            "Hanat Adai",
            "Zelin Fu",
            "Tianxiang Zhou",
            "Han Zhang",
            "Yinghao Xu",
            "Xing Zhu",
            "Yujun Shen",
            "Nan Xue"
        ],
        "title": "Masked Depth Modeling for Spatial Perception",
        "abstract": "arXiv:2601.17895v1 Announce Type: new  Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.",
        "arxiv_id": "2601.17895",
        "ARXIVID": "2601.17895",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on spatial perception and depth modeling for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.17323": {
        "authors": [
            "Debang Li",
            "Zhengcong Fei",
            "Tuanhui Li",
            "Yikun Dou",
            "Zheng Chen",
            "Jiangping Yang",
            "Mingyuan Fan",
            "Jingtao Xu",
            "Jiahua Wang",
            "Baoxuan Gu",
            "Mingshan Chang",
            "Yuqiang Xie",
            "Binjie Mao",
            "Youqiang Zhang",
            "Nuo Pang",
            "Hao Zhang",
            "Yuzhe Jin",
            "Zhiheng Xu",
            "Dixuan Lin",
            "Guibin Chen",
            "Yahui Zhou"
        ],
        "title": "SkyReels-V3 Technique Report",
        "abstract": "arXiv:2601.17323v1 Announce Type: new  Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.   Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.",
        "arxiv_id": "2601.17323",
        "ARXIVID": "2601.17323",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on multimodal video generation with diffusion transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2601.18340": {
        "authors": [
            "Bingzheng Qu",
            "Kehai Chen",
            "Xuefeng Bai",
            "Jun Yu",
            "Min Zhang"
        ],
        "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing",
        "abstract": "arXiv:2601.18340v1 Announce Type: new  Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.",
        "arxiv_id": "2601.18340",
        "ARXIVID": "2601.18340",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for non-rigid video editing and evaluates video-based tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.18157": {
        "authors": [
            "Aniket Rege",
            "Arka Sadhu",
            "Yuliang Li",
            "Kejie Li",
            "Ramya Korlakai Vinayak",
            "Yuning Chai",
            "Yong Jae Lee",
            "Hyo Jin Kim"
        ],
        "title": "Agentic Very Long Video Understanding",
        "abstract": "arXiv:2601.18157v1 Announce Type: new  Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.",
        "arxiv_id": "2601.18157",
        "ARXIVID": "2601.18157",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on long-horizon video understanding with novel methodologies for egocentric video tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2601.17383": {
        "authors": [
            "Chen Ling",
            "Kai Hu",
            "Hangcheng Liu",
            "Xingshuo Han",
            "Tianwei Zhang",
            "Changhai Ou"
        ],
        "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
        "abstract": "arXiv:2601.17383v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.",
        "arxiv_id": "2601.17383",
        "ARXIVID": "2601.17383",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores vulnerabilities in LVLMs and their behavior in real-world scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2601.18589": {
        "authors": [
            "KV Karthikeya",
            "Ashok Kumar Das",
            "Shantanu Pal",
            "Vivekananda Bhat K",
            "Arun Sekar Rajasekaran"
        ],
        "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment",
        "abstract": "arXiv:2601.18589v1 Announce Type: new  Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.",
        "arxiv_id": "2601.18589",
        "ARXIVID": "2601.18589",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on multimodal fusion with text, audio, and images.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2601.18407": {
        "authors": [
            "Jon Sporring",
            "David Stansby"
        ],
        "title": "Larger than memory image processing",
        "abstract": "arXiv:2601.18407v1 Announce Type: new  Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.",
        "arxiv_id": "2601.18407",
        "ARXIVID": "2601.18407",
        "COMMENT": "Does not match any specific criterion but is relevant to large-scale image processing and computational efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.17905": {
        "authors": [
            "Jack Foster",
            "Kirill Paramonov",
            "Mete Ozay",
            "Umberto Michieli"
        ],
        "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning",
        "abstract": "arXiv:2601.17905v1 Announce Type: new  Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.",
        "arxiv_id": "2601.17905",
        "ARXIVID": "2601.17905",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and few-shot learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.17733": {
        "authors": [
            "Junran Lu",
            "Yuanqi Li",
            "Hengji Li",
            "Jie Guo",
            "Yanwen Guo"
        ],
        "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles",
        "abstract": "arXiv:2601.17733v1 Announce Type: new  Abstract: Boundary Representation (B-Rep) is the widely adopted standard   in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.   We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.",
        "arxiv_id": "2601.17733",
        "ARXIVID": "2601.17733",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for CAD applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.18222": {
        "authors": [
            "Mengfan He",
            "Liangzheng Sun",
            "Chunyu Li",
            "Ziyang Meng"
        ],
        "title": "HomoFM: Deep Homography Estimation with Flow Matching",
        "abstract": "arXiv:2601.18222v1 Announce Type: new  Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.",
        "arxiv_id": "2601.18222",
        "ARXIVID": "2601.18222",
        "COMMENT": "Does not match any specific criteria but is related to homography estimation in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.17331": {
        "authors": [
            "Fabian Vazquez",
            "Jose A. Nu\\~nez",
            "Diego Adame",
            "Alissen Moreno",
            "Augustin Zhan",
            "Huimin Li",
            "Jinghao Yang",
            "Haoteng Tang",
            "Bin Fu",
            "Pengfei Gu"
        ],
        "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation",
        "abstract": "arXiv:2601.17331v1 Announce Type: new  Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg",
        "arxiv_id": "2601.17331",
        "ARXIVID": "2601.17331",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.17740": {
        "authors": [
            "Cong Cao",
            "Ren Li",
            "Corentin Dumery",
            "Hao Li"
        ],
        "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields",
        "abstract": "arXiv:2601.17740v1 Announce Type: new  Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.",
        "arxiv_id": "2601.17740",
        "ARXIVID": "2601.17740",
        "COMMENT": "Does not match any specific criteria but is related to sewing pattern modeling using implicit representations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.18098": {
        "authors": [
            "Chuang Yang",
            "Haozhao Ma",
            "Xu Han",
            "Yuan Yuan",
            "Qi Wang"
        ],
        "title": "Text-Pass Filter: An Efficient Scene Text Detector",
        "abstract": "arXiv:2601.18098v1 Announce Type: new  Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.",
        "arxiv_id": "2601.18098",
        "ARXIVID": "2601.18098",
        "COMMENT": "Does not match any specific criteria but is related to text detection in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.18386": {
        "authors": [
            "Gabriel Lee Jun Rong",
            "Christos Korgialas",
            "Dion Jia Xu Ho",
            "Pai Chet Ng",
            "Xiaoxiao Miao",
            "Konstantinos N. Plataniotis"
        ],
        "title": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks",
        "abstract": "arXiv:2601.18386v1 Announce Type: new  Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.",
        "arxiv_id": "2601.18386",
        "ARXIVID": "2601.18386",
        "COMMENT": "Does not match any specific criteria but explores adversarial attacks with Vision Language Models, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2601.17228": {
        "authors": [
            "Tengyue Zhang",
            "Ruiwen Ding",
            "Luoting Zhuang",
            "Yuxiao Wu",
            "Erika F. Rodriguez",
            "William Hsu"
        ],
        "title": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification",
        "abstract": "arXiv:2601.17228v1 Announce Type: new  Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.",
        "arxiv_id": "2601.17228",
        "ARXIVID": "2601.17228",
        "COMMENT": "Does not match any specific criteria but is related to domain adaptation in computational pathology.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
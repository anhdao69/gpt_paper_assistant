{
    "2511.16567": {
        "authors": [
            "Ye Mao",
            "Weixun Luo",
            "Ranran Huang",
            "Junpeng Jing",
            "Krystian Mikolajczyk"
        ],
        "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
        "abstract": "arXiv:2511.16567v1 Announce Type: new  Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
        "arxiv_id": "2511.16567",
        "ARXIVID": "2511.16567",
        "COMMENT": "Matches criteria 1 (Spatial Intelligence and Embodied Agents) and criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of POMA-3D, a self-supervised 3D representation model for spatial understanding and its application to embodied tasks like navigation and localization.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16175": {
        "authors": [
            "Yi Yang",
            "Xueqi Li",
            "Yiyang Chen",
            "Jin Song",
            "Yihan Wang",
            "Zipeng Xiao",
            "Jiadi Su",
            "You Qiaoben",
            "Pengfei Liu",
            "Zhijie Deng"
        ],
        "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
        "abstract": "arXiv:2511.16175v1 Announce Type: new  Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\\pi_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
        "arxiv_id": "2511.16175",
        "ARXIVID": "2511.16175",
        "COMMENT": "Matches criterion 2 as it introduces a Vision-Language-Action model with disentangled visual foresight, focusing on multimodal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.15351": {
        "authors": [
            "Yifu Guo",
            "Zishan Xu",
            "Zhiyuan Yao",
            "Yuquan Lu",
            "Jiaye Lin",
            "Sen Hu",
            "Zhenheng Tang",
            "Yingchao Li",
            "Huacan Wang",
            "Ronghao Chen"
        ],
        "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
        "abstract": "arXiv:2511.15351v1 Announce Type: new  Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
        "arxiv_id": "2511.15351",
        "ARXIVID": "2511.15351",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal reasoning framework with a novel architecture for vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.16049": {
        "authors": [
            "Pei Liu",
            "Songtao Wang",
            "Lang Zhang",
            "Xingyue Peng",
            "Yuandong Lyu",
            "Jiaxin Deng",
            "Songxin Lu",
            "Weiliang Ma",
            "Xueyang Zhang",
            "Yifei Zhan",
            "XianPeng Lang",
            "Jun Ma"
        ],
        "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving",
        "abstract": "arXiv:2511.16049v1 Announce Type: new  Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.",
        "arxiv_id": "2511.16049",
        "ARXIVID": "2511.16049",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of LiSTAR, a novel generative world model for 4D LiDAR sequences, which is relevant for autonomous driving simulations.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2511.16618": {
        "authors": [
            "Haofeng Liu",
            "Ziyue Wang",
            "Sudhanshu Mishra",
            "Mingqi Gao",
            "Guanyi Qin",
            "Chang Han Low",
            "Alex Y. W. Kong",
            "Yueming Jin"
        ],
        "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
        "abstract": "arXiv:2511.16618v1 Announce Type: new  Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
        "arxiv_id": "2511.16618",
        "ARXIVID": "2511.16618",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of the SA-SV benchmark for surgical video segmentation and novel methods like DiveMem for long-term tracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16091": {
        "authors": [
            "Renxiang Xiao",
            "Wei Liu",
            "Yuanfan Zhang",
            "Yushuai Chen",
            "Jinming Chen",
            "Zilu Wang",
            "Liang Hu"
        ],
        "title": "Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
        "abstract": "arXiv:2511.16091v1 Announce Type: new  Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.",
        "arxiv_id": "2511.16091",
        "ARXIVID": "2511.16091",
        "COMMENT": "Matches criterion 3 as it introduces a radar-vision integration system for large-scale outdoor SLAM, relevant to embodied AI and robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16666": {
        "authors": [
            "Zhenyuan Qin",
            "Xincheng Shuai",
            "Henghui Ding"
        ],
        "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
        "abstract": "arXiv:2511.16666v1 Announce Type: new  Abstract: Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.",
        "arxiv_id": "2511.16666",
        "ARXIVID": "2511.16666",
        "COMMENT": "Matches criterion 4 as it focuses on controllable image generation with a novel representation and training strategy, relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.15923": {
        "authors": [
            "Meilong Xu",
            "Di Fu",
            "Jiaxing Zhang",
            "Gong Yu",
            "Jiayu Zheng",
            "Xiaoling Hu",
            "Dongdi Zhao",
            "Feiyang Li",
            "Chao Chen",
            "Yong Cao"
        ],
        "title": "RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification",
        "abstract": "arXiv:2511.15923v1 Announce Type: new  Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \\textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.",
        "arxiv_id": "2511.15923",
        "ARXIVID": "2511.15923",
        "COMMENT": "Matches criterion 6 as it proposes a novel method for video classification using Vision Language Models (VLMs) with a rationale-bootstrapped fine-tuning approach.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16672": {
        "authors": [
            "Omkat Thawakar",
            "Shravan Venkatraman",
            "Ritesh Thawkar",
            "Abdelrahman Shaker",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Fahad Khan"
        ],
        "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
        "abstract": "arXiv:2511.16672v1 Announce Type: new  Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
        "arxiv_id": "2511.16672",
        "ARXIVID": "2511.16672",
        "COMMENT": "Matches criterion 2 as it introduces a self-evolving framework for improving multimodal reasoning in large multimodal models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.16454": {
        "authors": [
            "Doriand Petit",
            "Steve Bourgeois",
            "Vincent Gay-Bellile",
            "Florian Chabot",
            "Lo\\\"ic Barthe"
        ],
        "title": "LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs",
        "abstract": "arXiv:2511.16454v1 Announce Type: new  Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.",
        "arxiv_id": "2511.16454",
        "ARXIVID": "2511.16454",
        "COMMENT": "Matches criterion 5 as it proposes a method to improve 3D scene understanding in Vision-Language Models using multi-view 2D images.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.15831": {
        "authors": [
            "Wei Zhang",
            "Yeying Jin",
            "Xin Li",
            "Yan Zhang",
            "Xiaofeng Cong",
            "Cong Wang",
            "Fengcai Qiao",
            "zhichao Lian"
        ],
        "title": "UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment",
        "abstract": "arXiv:2511.15831v1 Announce Type: new  Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.",
        "arxiv_id": "2511.15831",
        "ARXIVID": "2511.15831",
        "COMMENT": "Matches criterion 5 as it integrates multimodal inputs (image and text) with a Multimodal Large Language Model (MLLM) for virtual try-on tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16077": {
        "authors": [
            "Zishan Xu",
            "Yifu Guo",
            "Yuquan Lu",
            "Fengyu Yang",
            "Junxin Li"
        ],
        "title": "VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning",
        "abstract": "arXiv:2511.16077v1 Announce Type: new  Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \\textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.",
        "arxiv_id": "2511.16077",
        "ARXIVID": "2511.16077",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks with a novel reinforcement learning-based framework.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16527": {
        "authors": [
            "Kwun Ho Ngan",
            "Saman Sadeghi Afgeh",
            "Joe Townsend",
            "Artur d'Avila Garcez"
        ],
        "title": "Contrastive vision-language learning with paraphrasing and negation",
        "abstract": "arXiv:2511.16527v1 Announce Type: new  Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.",
        "arxiv_id": "2511.16527",
        "ARXIVID": "2511.16527",
        "COMMENT": "Matches criterion 2 as it explores improvements in vision-language models with a focus on contrastive learning and semantic transformations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16624": {
        "authors": [
            "SAM 3D Team",
            "Xingyu Chen",
            "Fu-Jen Chu",
            "Pierre Gleize",
            "Kevin J Liang",
            "Alexander Sax",
            "Hao Tang",
            "Weiyao Wang",
            "Michelle Guo",
            "Thibaut Hardin",
            "Xiang Li",
            "Aohan Lin",
            "Jiawei Liu",
            "Ziqi Ma",
            "Anushka Sagar",
            "Bowen Song",
            "Xiaodong Wang",
            "Jianing Yang",
            "Bowen Zhang",
            "Piotr Doll\\'ar",
            "Georgia Gkioxari",
            "Matt Feiszli",
            "Jitendra Malik"
        ],
        "title": "SAM 3D: 3Dfy Anything in Images",
        "abstract": "arXiv:2511.16624v1 Announce Type: new  Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
        "arxiv_id": "2511.16624",
        "ARXIVID": "2511.16624",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model for 3D object reconstruction and its applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.16435": {
        "authors": [
            "Jin Wang",
            "Bingfeng Zhang",
            "Jian Pang",
            "Mengyu Liu",
            "Honglong Chen",
            "Weifeng Liu"
        ],
        "title": "Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation",
        "abstract": "arXiv:2511.16435v1 Announce Type: new  Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.",
        "arxiv_id": "2511.16435",
        "ARXIVID": "2511.16435",
        "COMMENT": "Matches criterion 2 and 5 as it explores the integration of visual and language modalities using Large Language Models (LLMs) for few-shot segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16140": {
        "authors": [
            "Chenyu Zhao",
            "Xianwei Zheng",
            "Zimin Xia",
            "Linwei Yue",
            "Nan Xue"
        ],
        "title": "Real-Time 3D Object Detection with Inference-Aligned Learning",
        "abstract": "arXiv:2511.16140v1 Announce Type: new  Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.",
        "arxiv_id": "2511.16140",
        "ARXIVID": "2511.16140",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for real-time 3D object detection with inference-aligned learning, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16418": {
        "authors": [
            "Hai Lan",
            "Zongyan Li",
            "Jianmin Hu",
            "Jialing Yang",
            "Houde Dai"
        ],
        "title": "End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss",
        "abstract": "arXiv:2511.16418v1 Announce Type: new  Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.",
        "arxiv_id": "2511.16418",
        "ARXIVID": "2511.16418",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for motion capture using rigid body markers and geodesic loss, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16440": {
        "authors": [
            "Diogo J. Paulo",
            "Jo\\~ao Martins",
            "Hugo Proen\\c{c}a",
            "Jo\\~ao C. Neves"
        ],
        "title": "StreetView-Waste: A Multi-Task Dataset for Urban Waste Management",
        "abstract": "arXiv:2511.16440v1 Announce Type: new  Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.",
        "arxiv_id": "2511.16440",
        "ARXIVID": "2511.16440",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for urban waste management, focusing on real-world perception systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16524": {
        "authors": [
            "Rahul Kumar",
            "Vipul Baghel",
            "Sudhanshu Singh",
            "Bikash Kumar Badatya",
            "Shivam Yadav",
            "Babji Srinivasan",
            "Ravi Hegde"
        ],
        "title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization",
        "abstract": "arXiv:2511.16524v1 Announce Type: new  Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.",
        "arxiv_id": "2511.16524",
        "ARXIVID": "2511.16524",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark for video-based action recognition in boxing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.16163": {
        "authors": [
            "Zhi Luo",
            "Zenghui Yuan",
            "Wenqi Wei",
            "Daizong Liu",
            "Pan Zhou"
        ],
        "title": "An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs",
        "abstract": "arXiv:2511.16163v1 Announce Type: new  Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.",
        "arxiv_id": "2511.16163",
        "ARXIVID": "2511.16163",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language Models (VLMs) and introduces a novel attack method to induce verbose outputs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.15943": {
        "authors": [
            "Zihan Li",
            "Yiqing Wang",
            "Sina Farsiu",
            "Paul Kinahan"
        ],
        "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
        "abstract": "arXiv:2511.15943v1 Announce Type: new  Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \\href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.",
        "arxiv_id": "2511.15943",
        "ARXIVID": "2511.15943",
        "COMMENT": "Matches criterion 2 as it proposes a novel multimodal learning framework for medical imaging, which aligns with vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.15055": {
        "authors": [
            "Jian-Ting Guo",
            "Yu-Cheng Chen",
            "Ping-Chun Hsieh",
            "Kuo-Hao Ho",
            "Po-Wei Huang",
            "Ti-Rong Wu",
            "I-Chen Wu"
        ],
        "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
        "abstract": "arXiv:2511.15055v1 Announce Type: new  Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.",
        "arxiv_id": "2511.15055",
        "ARXIVID": "2511.15055",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for learning human-like RL agents, which is relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.16378": {
        "authors": [
            "Pan Yang",
            "Cheng Deng",
            "Jing Yang",
            "Han Zhao",
            "Yun Liu",
            "Yuling Chen",
            "Xiaoli Ruan",
            "Yanping Chen"
        ],
        "title": "CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement",
        "abstract": "arXiv:2511.16378v1 Announce Type: new  Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.",
        "arxiv_id": "2511.16378",
        "ARXIVID": "2511.16378",
        "COMMENT": "Matches criterion 2 as it focuses on compositional zero-shot learning using CLIP, which is a vision-language model.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2511.16317": {
        "authors": [
            "Zeqiang Lai",
            "Yunfei Zhao",
            "Zibo Zhao",
            "Xin Yang",
            "Xin Huang",
            "Jingwei Huang",
            "Xiangyu Yue",
            "Chunchao Guo"
        ],
        "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
        "abstract": "arXiv:2511.16317v1 Announce Type: new  Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
        "arxiv_id": "2511.16317",
        "ARXIVID": "2511.16317",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and computer vision, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.16143": {
        "authors": [
            "Quanqing Ma",
            "Jiaen Chen",
            "Peng Wang",
            "Yao Zheng",
            "Qingzhan Zhao",
            "Yuchen Zheng"
        ],
        "title": "A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection",
        "abstract": "arXiv:2511.16143v1 Announce Type: new  Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.",
        "arxiv_id": "2511.16143",
        "ARXIVID": "2511.16143",
        "COMMENT": "Does not closely match any specific criteria but is generally related to computer vision and spatial reasoning in remote sensing, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.15191": {
        "authors": [
            "Zhiyi Duan",
            "Zixing Shi",
            "Hongyu Yuan",
            "Qi Wang"
        ],
        "title": "HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization",
        "abstract": "arXiv:2511.15191v1 Announce Type: new  Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.",
        "arxiv_id": "2511.15191",
        "ARXIVID": "2511.15191",
        "COMMENT": "Does not match any specific criteria but is relevant to explainable AI and knowledge tracing, which may align with your friend's broader interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.15984": {
        "authors": [
            "Xinyu Nan",
            "Lingtao Mao",
            "Huangyu Dai",
            "Zexin Zheng",
            "Xinyu Sun",
            "Zihan Liang",
            "Ben Chen",
            "Yuqing Ding",
            "Chenyi Lei",
            "Wenwu Ou",
            "Han Li"
        ],
        "title": "UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition",
        "abstract": "arXiv:2511.15984v1 Announce Type: new  Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.",
        "arxiv_id": "2511.15984",
        "ARXIVID": "2511.15984",
        "COMMENT": "Does not match any specific criteria but is relevant to hierarchical object recognition and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16322": {
        "authors": [
            "Ching-Heng Cheng",
            "Chih-Chung Hsu"
        ],
        "title": "ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery",
        "abstract": "arXiv:2511.16322v1 Announce Type: new  Abstract: Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.",
        "arxiv_id": "2511.16322",
        "ARXIVID": "2511.16322",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and deep learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.15875": {
        "authors": [
            "Lukas Arzoumanidis",
            "Julius Knechtel",
            "Jan-Henrik Haunert",
            "Youness Dehbi"
        ],
        "title": "Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation",
        "abstract": "arXiv:2511.15875v1 Announce Type: new  Abstract: The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.",
        "arxiv_id": "2511.15875",
        "ARXIVID": "2511.15875",
        "COMMENT": "Does not closely match any specific criterion but is related to synthetic data generation and segmentation, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.14780": {
        "authors": [
            "Keith Moore",
            "Jun W. Kim",
            "David Lyu",
            "Jeffrey Heo",
            "Ehsan Adeli"
        ],
        "title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents",
        "abstract": "arXiv:2511.14780v1 Announce Type: new  Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.",
        "arxiv_id": "2511.14780",
        "ARXIVID": "2511.14780",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to multi-agent reasoning and belief formation, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16321": {
        "authors": [
            "Ching-Heng Cheng",
            "Jen-Wei Lee",
            "Chia-Ming Lee",
            "Chih-Chung Hsu"
        ],
        "title": "WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement",
        "abstract": "arXiv:2511.16321v1 Announce Type: new  Abstract: Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.",
        "arxiv_id": "2511.16321",
        "ARXIVID": "2511.16321",
        "COMMENT": "Does not match any specific criterion but focuses on underwater image enhancement, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.15192": {
        "authors": [
            "Haodong Li",
            "Jingqi Zhang",
            "Xiao Cheng",
            "Peihua Mai",
            "Haoyu Wang",
            "Yan Pang"
        ],
        "title": "As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files",
        "abstract": "arXiv:2511.15192v2 Announce Type: new  Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.   We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen\" (training data) and ``unseen\" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.",
        "arxiv_id": "2511.15192",
        "ARXIVID": "2511.15192",
        "COMMENT": "Does not match any specific criterion but is related to copyright detection in LLMs, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16555": {
        "authors": [
            "Junpeng Jing",
            "Weixun Luo",
            "Ye Mao",
            "Krystian Mikolajczyk"
        ],
        "title": "Lite Any Stereo: Efficient Zero-Shot Stereo Matching",
        "abstract": "arXiv:2511.16555v1 Announce Type: new  Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.",
        "arxiv_id": "2511.16555",
        "ARXIVID": "2511.16555",
        "COMMENT": "Does not match any specific criterion but focuses on stereo depth estimation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16020": {
        "authors": [
            "Dingkun Zhou",
            "Patrick P. K. Chan",
            "Hengxu Wu",
            "Shikang Zheng",
            "Ruiqi Huang",
            "Yuanjie Zhao"
        ],
        "title": "Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion",
        "abstract": "arXiv:2511.16020v1 Announce Type: new  Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.",
        "arxiv_id": "2511.16020",
        "ARXIVID": "2511.16020",
        "COMMENT": "Does not match any specific criterion but is related to adversarial attacks in computer vision, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
{
    "2602.11073": {
        "authors": [
            "Junfei Wu",
            "Jian Guan",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang",
            "Wei Wu",
            "Tienie Tan"
        ],
        "title": "Chatting with Images for Introspective Visual Thinking",
        "abstract": "arXiv:2602.11073v1 Announce Type: new  Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.",
        "arxiv_id": "2602.11073",
        "ARXIVID": "2602.11073",
        "COMMENT": "Matches criterion 5 as it discusses techniques combining image understanding and large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.11024": {
        "authors": [
            "Rishikesh Bhyri",
            "Brian R Quaranto",
            "Philip J Seger",
            "Kaity Tung",
            "Brendan Fox",
            "Gene Yang",
            "Steven D. Schwaitzberg",
            "Junsong Yuan",
            "Nan Xi",
            "Peter C W Kim"
        ],
        "title": "Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting",
        "abstract": "arXiv:2602.11024v1 Announce Type: new  Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.",
        "arxiv_id": "2602.11024",
        "ARXIVID": "2602.11024",
        "COMMENT": "Matches criterion 1 as it introduces a novel spatial reasoning framework for dense surgical instrument counting.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.10159": {
        "authors": [
            "Tao Yu",
            "Yujia Yang",
            "Haopeng Jin",
            "Junhao Gong",
            "Xinlong Chen",
            "Yuxuan Zhou",
            "Shanbin Zhang",
            "Jiabing Yang",
            "Xinming Wang",
            "Hongzhu Yi",
            "Ping Nie",
            "Kai Zou",
            "Zhang Zhang",
            "Yan Huang",
            "Liang Wang",
            "Yeshani",
            "Ruiwen Tao",
            "Jin Ma",
            "Haijin Liang",
            "Jinwen Luo"
        ],
        "title": "Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization",
        "abstract": "arXiv:2602.10159v1 Announce Type: new  Abstract: Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \\textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \\textbf{1,440 samples} spanning \\textbf{20 diverse categories} and \\textbf{four duration groups}, sourced from \\textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \\textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \\textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.",
        "arxiv_id": "2602.10159",
        "ARXIVID": "2602.10159",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark and framework for video retrieval and moment localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.11007": {
        "authors": [
            "Lei Yao",
            "Yi Wang",
            "Yawen Cui",
            "Moyun Liu",
            "Lap-Pui Chau"
        ],
        "title": "LaSSM: Efficient Semantic-Spatial Query Decoding via Local Aggregation and State Space Models for 3D Instance Segmentation",
        "abstract": "arXiv:2602.11007v1 Announce Type: new  Abstract: Query-based 3D scene instance segmentation from point clouds has attained notable performance. However, existing methods suffer from the query initialization dilemma due to the sparse nature of point clouds and rely on computationally intensive attention mechanisms in query decoders. We accordingly introduce LaSSM, prioritizing simplicity and efficiency while maintaining competitive performance. Specifically, we propose a hierarchical semantic-spatial query initializer to derive the query set from superpoints by considering both semantic cues and spatial distribution, achieving comprehensive scene coverage and accelerated convergence. We further present a coordinate-guided state space model (SSM) decoder that progressively refines queries. The novel decoder features a local aggregation scheme that restricts the model to focus on geometrically coherent regions and a spatial dual-path SSM block to capture underlying dependencies within the query set by integrating associated coordinates information. Our design enables efficient instance prediction, avoiding the incorporation of noisy information and reducing redundant computation. LaSSM ranks first place on the latest ScanNet++ V2 leaderboard, outperforming the previous best method by 2.5% mAP with only 1/3 FLOPs, demonstrating its superiority in challenging large-scale scene instance segmentation. LaSSM also achieves competitive performance on ScanNet, ScanNet200, S3DIS and ScanNet++ V1 benchmarks with less computational cost. Extensive ablation studies and qualitative results validate the effectiveness of our design. The code and weights are available at https://github.com/RayYoh/LaSSM.",
        "arxiv_id": "2602.11007",
        "ARXIVID": "2602.11007",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (LaSSM) for 3D instance segmentation, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10880": {
        "authors": [
            "Minggui He",
            "Mingchen Dai",
            "Jian Zhang",
            "Yilun Liu",
            "Shimin Tao",
            "Pufan Zeng",
            "Osamu Yoshie",
            "Yuya Ieiri"
        ],
        "title": "Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation",
        "abstract": "arXiv:2602.10880v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper",
        "arxiv_id": "2602.10880",
        "ARXIVID": "2602.10880",
        "COMMENT": "Matches criterion 5 as it focuses on integrating vision tasks (chart-to-code generation) with large language models using structural representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10799": {
        "authors": [
            "Zihui Zhou",
            "Yong Feng",
            "Yanying Chen",
            "Guofan Duan",
            "Zhenxi Song",
            "Mingliang Zhou",
            "Weijia Jia"
        ],
        "title": "RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation",
        "abstract": "arXiv:2602.10799v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.",
        "arxiv_id": "2602.10799",
        "ARXIVID": "2602.10799",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) in the context of remote sensing, addressing hallucination issues.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.11005": {
        "authors": [
            "Vasileios Arampatzakis",
            "George Pavlidis",
            "Nikolaos Mitianoudis",
            "Nikos Papamarkos"
        ],
        "title": "Interpretable Vision Transformers in Monocular Depth Estimation via SVDA",
        "abstract": "arXiv:2602.11005v1 Announce Type: new  Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.",
        "arxiv_id": "2602.11005",
        "ARXIVID": "2602.11005",
        "COMMENT": "Matches criterion 1 as it introduces a novel method (SVDA) for spatial reasoning in monocular depth estimation, which is relevant for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.10425": {
        "authors": [
            "Yilin Yang",
            "Zhenghui Guo",
            "Yuke Wang",
            "Omprakash Gnawali",
            "Sheng Di",
            "Chengming Zhang"
        ],
        "title": "HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images",
        "abstract": "arXiv:2602.10425v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.",
        "arxiv_id": "2602.10425",
        "ARXIVID": "2602.10425",
        "COMMENT": "Matches criterion 2 as it focuses on hallucination mitigation in large vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2602.10639": {
        "authors": [
            "Yuxin Cao",
            "Wei Song",
            "Shangzhi Xu",
            "Jingling Xue",
            "Jin Song Dong"
        ],
        "title": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
        "abstract": "arXiv:2602.10639v1 Announce Type: new  Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.",
        "arxiv_id": "2602.10639",
        "ARXIVID": "2602.10639",
        "COMMENT": "Matches criterion 6 as it introduces a framework (VideoSTF) for evaluating and stress-testing video large language models, focusing on output repetition.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2602.10994": {
        "authors": [
            "Vasileios Arampatzakis",
            "George Pavlidis",
            "Nikolaos Mitianoudis",
            "Nikos Papamarkos"
        ],
        "title": "Interpretable Vision Transformers in Image Classification via SVDA",
        "abstract": "arXiv:2602.10994v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.",
        "arxiv_id": "2602.10994",
        "ARXIVID": "2602.10994",
        "COMMENT": "Matches criterion 4 as it focuses on interpretability and attention mechanisms in vision transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.10549": {
        "authors": [
            "Shengyang Sun",
            "Jiashen Hua",
            "Junyi Feng",
            "Xiaojin Gong"
        ],
        "title": "Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance",
        "abstract": "arXiv:2602.10549v1 Announce Type: new  Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.",
        "arxiv_id": "2602.10549",
        "ARXIVID": "2602.10549",
        "COMMENT": "Matches criterion 6 as it focuses on multimodal video anomaly detection, which is a video understanding task, and introduces text-guided enhancements.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.10825": {
        "authors": [
            "Yuexiao Ma",
            "Xuzhe Zheng",
            "Jing Xu",
            "Xiwei Xu",
            "Feng Ling",
            "Xiawu Zheng",
            "Huafeng Kuang",
            "Huixia Li",
            "Xing Wang",
            "Xuefeng Xiao",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "title": "Flow caching for autoregressive video generation",
        "abstract": "arXiv:2602.10825v1 Announce Type: new  Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.",
        "arxiv_id": "2602.10825",
        "ARXIVID": "2602.10825",
        "COMMENT": "Matches criterion 6 as it focuses on video generation, which is a video understanding task, and introduces a novel caching framework for efficiency.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.10583": {
        "authors": [
            "Bo Xue",
            "Yunchong Song",
            "Fanghao Shao",
            "Xuekai Zhu",
            "Lin Chen",
            "Luoyi Fu",
            "Xinbing Wang",
            "Zhouhan Lin"
        ],
        "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
        "abstract": "arXiv:2602.10583v1 Announce Type: new  Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.",
        "arxiv_id": "2602.10583",
        "ARXIVID": "2602.10583",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and language models, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.10635": {
        "authors": [
            "Keane Ong",
            "Sabri Boughorbel",
            "Luwei Xiao",
            "Chanakya Ekbote",
            "Wei Dai",
            "Ao Qu",
            "Jingyao Wu",
            "Rui Mao",
            "Ehsan Hoque",
            "Erik Cambria",
            "Gianmarco Mengaldo",
            "Paul Pu Liang"
        ],
        "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
        "abstract": "arXiv:2602.10635v1 Announce Type: new  Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.",
        "arxiv_id": "2602.10635",
        "ARXIVID": "2602.10635",
        "COMMENT": "Does not match any specific criterion but is generally relevant to embodied AI and social behavior modeling, which might be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.10367": {
        "authors": [
            "Zhiling Yan",
            "Dingjie Song",
            "Zhe Fang",
            "Yisheng Ji",
            "Xiang Li",
            "Quanzheng Li",
            "Lichao Sun"
        ],
        "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
        "abstract": "arXiv:2602.10367v1 Announce Type: new  Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.",
        "arxiv_id": "2602.10367",
        "ARXIVID": "2602.10367",
        "COMMENT": "Does not match any specific criterion but is generally relevant to benchmarks and evaluation in AI, which might be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.10625": {
        "authors": [
            "Nanxu Gong",
            "Haotian Li",
            "Sixun Dong",
            "Jianxun Lian",
            "Yanjie Fu",
            "Xing Xie"
        ],
        "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks",
        "abstract": "arXiv:2602.10625v1 Announce Type: new  Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.",
        "arxiv_id": "2602.10625",
        "ARXIVID": "2602.10625",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in LLMs, which is tangentially related to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10516": {
        "authors": [
            "Zhongju Wang",
            "Zhenhong Sun",
            "Beier Wang",
            "Yifu Wang",
            "Daoyi Dong",
            "Huadong Mo",
            "Hongdong Li"
        ],
        "title": "3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars",
        "abstract": "arXiv:2602.10516v1 Announce Type: new  Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.",
        "arxiv_id": "2602.10516",
        "ARXIVID": "2602.10516",
        "COMMENT": "Does not match any specific criteria but discusses 3D talking avatars, which is tangentially related to embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10964": {
        "authors": [
            "F. Carichon",
            "R. Rampa",
            "G. Farnadi"
        ],
        "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation",
        "abstract": "arXiv:2602.10964v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.",
        "arxiv_id": "2602.10964",
        "ARXIVID": "2602.10964",
        "COMMENT": "Does not match any specific criteria but discusses cultural adaptation in LLMs, which is tangentially related to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.10586": {
        "authors": [
            "Bosen Lin",
            "Feng Gao",
            "Yanwei Yu",
            "Junyu Dong",
            "Qian Du"
        ],
        "title": "Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning",
        "abstract": "arXiv:2602.10586v1 Announce Type: new  Abstract: Underwater Image Enhancement (UIE) is an ill-posed problem where natural clean references are not available, and the degradation levels vary significantly across semantic regions. Existing UIE methods treat images with a single global model and ignore the inconsistent degradation of different scene components. This oversight leads to significant color distortions and loss of fine details in heterogeneous underwater scenes, especially where degradation varies significantly across different image regions. Therefore, we propose SUCode (Semantic-aware Underwater Codebook Network), which achieves adaptive UIE from semantic-aware discrete codebook representation. Compared with one-shot codebook-based methods, SUCode exploits semantic-aware, pixel-level codebook representation tailored to heterogeneous underwater degradation. A three-stage training paradigm is employed to represent raw underwater image features to avoid pseudo ground-truth contamination. Gated Channel Attention Module (GCAM) and Frequency-Aware Feature Fusion (FAFF) jointly integrate channel and frequency cues for faithful color restoration and texture recovery. Extensive experiments on multiple benchmarks demonstrate that SUCode achieves state-of-the-art performance, outperforming recent UIE methods on both reference and no-reference metrics. The code will be made public available at https://github.com/oucailab/SUCode.",
        "arxiv_id": "2602.10586",
        "ARXIVID": "2602.10586",
        "COMMENT": "Does not match any specific criteria but focuses on underwater image enhancement, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2602.10744": {
        "authors": [
            "Kian Majlessi",
            "Amir Masoud Soltani",
            "Mohammad Ebrahim Mahdavi",
            "Aurelien Gourrier",
            "Peyman Adibi"
        ],
        "title": "Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning",
        "abstract": "arXiv:2602.10744v1 Announce Type: new  Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.",
        "arxiv_id": "2602.10744",
        "ARXIVID": "2602.10744",
        "COMMENT": "Does not match any specific criteria but focuses on image quality assessment, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
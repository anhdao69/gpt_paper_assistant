{
    "2602.23152": {
        "authors": [
            "Jingxuan Wei",
            "Siyuan Li",
            "Yuhang Xu",
            "Zheng Sun",
            "Junjie Jiang",
            "Hexuan Jin",
            "Caijun Jia",
            "Honghao He",
            "Xinglong Xu",
            "Xi bai",
            "Chang Yu",
            "Yumou Liu",
            "Junnan Zhu",
            "Xuanhe Zhou",
            "Jintao Chen",
            "Xiaobin Hu",
            "Shancheng Pang",
            "Bihui Yu",
            "Ran He",
            "Zhen Lei",
            "Stan Z. Li",
            "Conghui He",
            "Shuicheng Yan",
            "Cheng Tan"
        ],
        "title": "The Trinity of Consistency as a Defining Principle for General World Models",
        "abstract": "arXiv:2602.23152v1 Announce Type: new  Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
        "arxiv_id": "2602.23152",
        "ARXIVID": "2602.23152",
        "COMMENT": "Matches criterion 5 as it discusses integration of video understanding and multimodal models with a focus on spatial, temporal, and modal consistency.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.22897": {
        "authors": [
            "Xiaoxi Li",
            "Wenxiang Jiao",
            "Jiarui Jin",
            "Shijian Wang",
            "Guanting Dong",
            "Jiajie Jin",
            "Hao Wang",
            "Yinuo Wang",
            "Ji-Rong Wen",
            "Yuan Lu",
            "Zhicheng Dou"
        ],
        "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
        "abstract": "arXiv:2602.22897v1 Announce Type: new  Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
        "arxiv_id": "2602.22897",
        "ARXIVID": "2602.22897",
        "COMMENT": "Matches criteria 2 and 5. The paper introduces OmniGAIA, a benchmark for omni-modal AI agents, and proposes a foundation model for vision, audio, and language integration, which aligns with visual and multimodal large language models and integration of image/video and LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2602.22963": {
        "authors": [
            "Zehao Li",
            "Hongwei Yu",
            "Hao Jiang",
            "Qiang Sheng",
            "Yilong Xu",
            "Baolong Bi",
            "Yang Li",
            "Zhenlong Yuan",
            "Yujun Cai",
            "Zhaoqi Wang"
        ],
        "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
        "abstract": "arXiv:2602.22963v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.",
        "arxiv_id": "2602.22963",
        "ARXIVID": "2602.22963",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel framework for video misinformation detection using multimodal large language models (MLLMs) and integrates video understanding with LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2602.22769": {
        "authors": [
            "Yujie Zhao",
            "Boqin Yuan",
            "Junbo Huang",
            "Haocheng Yuan",
            "Zhongming Yu",
            "Haozhou Xu",
            "Lanxiang Hu",
            "Abhilash Shankarampeta",
            "Zimeng Huang",
            "Wentao Ni",
            "Yuandong Tian",
            "Jishen Zhao"
        ],
        "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications",
        "abstract": "arXiv:2602.22769v1 Announce Type: new  Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.",
        "arxiv_id": "2602.22769",
        "ARXIVID": "2602.22769",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (AMA-Bench) for evaluating long-horizon memory in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.22839": {
        "authors": [
            "Hao Zheng",
            "Guozhao Mo",
            "Xinru Yan",
            "Qianhao Yuan",
            "Wenkai Zhang",
            "Xuanang Chen",
            "Yaojie Lu",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation",
        "abstract": "arXiv:2602.22839v1 Announce Type: new  Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent",
        "arxiv_id": "2602.22839",
        "ARXIVID": "2602.22839",
        "COMMENT": "Matches criterion 1 as it discusses spatial intelligence and embodied agents with a novel environment-grounded reflection mechanism.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.22638": {
        "authors": [
            "Zhiheng Song",
            "Jingshuai Zhang",
            "Chuan Qin",
            "Chao Wang",
            "Chao Chen",
            "Longfei Xu",
            "Kaikui Liu",
            "Xiangxiang Chu",
            "Hengshu Zhu"
        ],
        "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
        "abstract": "arXiv:2602.22638v1 Announce Type: new  Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .",
        "arxiv_id": "2602.22638",
        "ARXIVID": "2602.22638",
        "COMMENT": "Matches criterion 3. The paper introduces MobilityBench, a benchmark for evaluating route-planning agents, which is relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2602.22523": {
        "authors": [
            "Ryan Liu",
            "Dilip Arumugam",
            "Cedegao E. Zhang",
            "Sean Escola",
            "Xaq Pitkow",
            "Thomas L. Griffiths"
        ],
        "title": "Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents",
        "abstract": "arXiv:2602.22523v1 Announce Type: new  Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.",
        "arxiv_id": "2602.22523",
        "ARXIVID": "2602.22523",
        "COMMENT": "Matches criteria 2 as it discusses modular language agents and their design inspired by cognitive models and AI algorithms, which could be relevant to multimodal LLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.22808": {
        "authors": [
            "Shiqian Su",
            "Sen Xing",
            "Xuan Dong",
            "Muyan Zhong",
            "Bin Wang",
            "Xizhou Zhu",
            "Yuntao Chen",
            "Wenhai Wang",
            "Yue Deng",
            "Pengxiang Zhu",
            "Ziyuan Liu",
            "Tiantong Li",
            "Jiaheng Yu",
            "Zhe Chen",
            "Lidong Bing",
            "Jifeng Dai"
        ],
        "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
        "abstract": "arXiv:2602.22808v1 Announce Type: new  Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.",
        "arxiv_id": "2602.22808",
        "ARXIVID": "2602.22808",
        "COMMENT": "Matches criterion 3 as it proposes a robust agent framework (MiroFlow) for general deep research tasks, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.22953": {
        "authors": [
            "Elron Bandel",
            "Asaf Yehudai",
            "Lilach Eden",
            "Yehoshua Sagron",
            "Yotam Perlitz",
            "Elad Venezian",
            "Natalia Razinkov",
            "Natan Ergas",
            "Shlomit Shachor Ifergan",
            "Segev Shlomov",
            "Michal Jacovi",
            "Leshem Choshen",
            "Liat Ein-Dor",
            "Yoav Katz",
            "Michal Shmueli-Scheuer"
        ],
        "title": "General Agent Evaluation",
        "abstract": "arXiv:2602.22953v1 Announce Type: new  Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.",
        "arxiv_id": "2602.22953",
        "ARXIVID": "2602.22953",
        "COMMENT": "Matches criterion 3 as it introduces a new evaluation framework for general-purpose agents, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.23258": {
        "authors": [
            "Yutong Wang",
            "Siyuan Xiong",
            "Xuebo Liu",
            "Wenkang Zhou",
            "Liang Ding",
            "Miao Zhang",
            "Min Zhang"
        ],
        "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
        "abstract": "arXiv:2602.23258v1 Announce Type: new  Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.",
        "arxiv_id": "2602.23258",
        "ARXIVID": "2602.23258",
        "COMMENT": "Matches criterion 3. The paper introduces a novel method for optimizing information flow in multi-agent systems, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2602.22519": {
        "authors": [
            "Wael Hafez",
            "Chenan Wei",
            "Rodrigo Felipe",
            "Amir Nazeri",
            "Cameron Reid"
        ],
        "title": "A Mathematical Theory of Agency and Intelligence",
        "abstract": "arXiv:2602.22519v1 Announce Type: new  Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.",
        "arxiv_id": "2602.22519",
        "ARXIVID": "2602.22519",
        "COMMENT": "Does not match any specific criteria. The paper introduces a mathematical framework for agency and intelligence, which is conceptually interesting but not directly tied to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2602.22981": {
        "authors": [
            "Haohui Jia",
            "Zheng Chen",
            "Lingwei Zhu",
            "Xu Cao",
            "Yasuko Matsubara",
            "Takashi Matsubara",
            "Yasushi Sakurai"
        ],
        "title": "RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs",
        "abstract": "arXiv:2602.22981v1 Announce Type: new  Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.",
        "arxiv_id": "2602.22981",
        "ARXIVID": "2602.22981",
        "COMMENT": "Does not match any specific criteria but is generally relevant to machine learning and neuroscience applications, which are tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.22425": {
        "authors": [
            "Raghav Gupta",
            "Akanksha Jain",
            "Abraham Gonzalez",
            "Alexander Novikov",
            "Po-Sen Huang",
            "Matej Balog",
            "Marvin Eisenberger",
            "Sergey Shirobokov",
            "Ng\\^an V\\~u",
            "Martin Dixon",
            "Borivoje Nikoli\\'c",
            "Parthasarathy Ranganathan",
            "Sagar Karandikar"
        ],
        "title": "ArchAgent: Agentic AI-driven Computer Architecture Discovery",
        "abstract": "arXiv:2602.22425v1 Announce Type: new  Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.   Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.   In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar \"winning margin\" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.   Agentic flows also enable \"post-silicon hyperspecialization\" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.   Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of \"simulator escapes\", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.",
        "arxiv_id": "2602.22425",
        "ARXIVID": "2602.22425",
        "COMMENT": "Does not match any specific criterion but discusses agentic AI in computer architecture, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.23199": {
        "authors": [
            "Jiahao Zhao",
            "Feng Jiang",
            "Shaowei Qin",
            "Zhonghui Zhang",
            "Junhao Liu",
            "Guibing Guo",
            "Hamid Alinejad-Rokny",
            "Min Yang"
        ],
        "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation",
        "abstract": "arXiv:2602.23199v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.",
        "arxiv_id": "2602.23199",
        "ARXIVID": "2602.23199",
        "COMMENT": "Does not match any specific criterion but is related to large language models applied to biology, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.23271": {
        "authors": [
            "Haotian Zhai",
            "Elias Stengel-Eskin",
            "Pratik Patil",
            "Liu Leqi"
        ],
        "title": "Evaluating Stochasticity in Deep Research Agents",
        "abstract": "arXiv:2602.23271v1 Announce Type: new  Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.",
        "arxiv_id": "2602.23271",
        "ARXIVID": "2602.23271",
        "COMMENT": "Does not match any specific criteria. The paper evaluates stochasticity in deep research agents, which is tangentially related to multi-agent systems but not directly tied to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.23093": {
        "authors": [
            "Dhwanil M. Mori",
            "Neil F. Johnson"
        ],
        "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents",
        "abstract": "arXiv:2602.23093v1 Announce Type: new  Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.",
        "arxiv_id": "2602.23093",
        "ARXIVID": "2602.23093",
        "COMMENT": "Does not match any specific criteria. The paper discusses emergent tribal behavior in AI agents, which is interesting but not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.22441": {
        "authors": [
            "Yingqian Cui",
            "Zhenwei Dai",
            "Bing He",
            "Zhan Shi",
            "Hui Liu",
            "Rui Sun",
            "Zhiji Liu",
            "Yue Xing",
            "Jiliang Tang",
            "Benoit Dumoulin"
        ],
        "title": "How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?",
        "abstract": "arXiv:2602.22441v1 Announce Type: new  Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.",
        "arxiv_id": "2602.22441",
        "ARXIVID": "2602.22441",
        "COMMENT": "Does not match any specific criterion but discusses latent reasoning methods, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.23315": {
        "authors": [
            "Sha Hu"
        ],
        "title": "Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction",
        "abstract": "arXiv:2602.23315v1 Announce Type: new  Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a \"resampling\" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.",
        "arxiv_id": "2602.23315",
        "ARXIVID": "2602.23315",
        "COMMENT": "Does not match any specific criteria. The paper discusses a resampling-based method for reducing epistemic uncertainty, which is not directly tied to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}
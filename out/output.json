{
    "2512.02339": {
        "authors": [
            "Chenshuang Zhang",
            "Kang Zhang",
            "Joon Son Chung",
            "In So Kweon",
            "Junmo Kim",
            "Chengzhi Mao"
        ],
        "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
        "abstract": "arXiv:2512.02339v1 Announce Type: new  Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.",
        "arxiv_id": "2512.02339",
        "ARXIVID": "2512.02339",
        "COMMENT": "Matches criteria 6 as it explores video diffusion models for tracking visually similar objects, presenting novel insights into motion representations.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.03041": {
        "authors": [
            "Qinghe Wang",
            "Xiaoyu Shi",
            "Baolu Li",
            "Weikang Bian",
            "Quande Liu",
            "Huchuan Lu",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Xu Jia"
        ],
        "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
        "abstract": "arXiv:2512.03041v1 Announce Type: new  Abstract: Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
        "arxiv_id": "2512.03041",
        "ARXIVID": "2512.03041",
        "COMMENT": "Matches criteria 5 and 6 closely as it focuses on multi-shot video generation with novel methodologies for spatiotemporal grounding and narrative coherence.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02231": {
        "authors": [
            "Le Thien Phuc Nguyen",
            "Zhuoran Yu",
            "Samuel Low Yu Hang",
            "Subin An",
            "Jeongik Lee",
            "Yohan Ban",
            "SeungEun Chung",
            "Thanh-Huy Nguyen",
            "JuWan Maeng",
            "Soochahn Lee",
            "Yong Jae Lee"
        ],
        "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
        "abstract": "arXiv:2512.02231v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
        "arxiv_id": "2512.02231",
        "ARXIVID": "2512.02231",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for fine-grained audiovisual reasoning in multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.02737": {
        "authors": [
            "Tristan Amadei",
            "Enric Meinhardt-Llopis",
            "Benedicte Bascle",
            "Corentin Abgrall",
            "Gabriele Facciolo"
        ],
        "title": "Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone",
        "abstract": "arXiv:2512.02737v1 Announce Type: new  Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.",
        "arxiv_id": "2512.02737",
        "ARXIVID": "2512.02737",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for UAV geo-localization without paired data and a new dataset for evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2512.03004": {
        "authors": [
            "Xiaoxue Chen",
            "Ziyi Xiong",
            "Yuantao Chen",
            "Gen Li",
            "Nan Wang",
            "Hongcheng Luo",
            "Long Chen",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Hongyang Li",
            "Ya-Qin Zhang",
            "Hao Zhao"
        ],
        "title": "DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images",
        "abstract": "arXiv:2512.03004v1 Announce Type: new  Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \\textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.",
        "arxiv_id": "2512.03004",
        "ARXIVID": "2512.03004",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel pose-free dynamic scene reconstruction method for autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2512.02793": {
        "authors": [
            "Fan Wu",
            "Jiacheng Wei",
            "Ruibo Li",
            "Yi Xu",
            "Junyou Li",
            "Deheng Ye",
            "Guosheng Lin"
        ],
        "title": "IC-World: In-Context Generation for Shared World Modeling",
        "abstract": "arXiv:2512.02793v1 Announce Type: new  Abstract: Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.",
        "arxiv_id": "2512.02793",
        "ARXIVID": "2512.02793",
        "COMMENT": "Matches criteria 6 as it focuses on video-based shared world modeling with novel reinforcement learning techniques for geometry and motion consistency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.03034": {
        "authors": [
            "Youxin Pang",
            "Jiajun Liu",
            "Lingfeng Tan",
            "Yong Zhang",
            "Feng Gao",
            "Xiang Deng",
            "Zhuoliang Kang",
            "Xiaoming Wei",
            "Yebin Liu"
        ],
        "title": "MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation",
        "abstract": "arXiv:2512.03034v1 Announce Type: new  Abstract: We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.",
        "arxiv_id": "2512.03034",
        "ARXIVID": "2512.03034",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a multimodal framework for audio-visual dialogue understanding and generation, integrating vision and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02450": {
        "authors": [
            "Valentin Bieri",
            "Marie-Julie Rakotosaona",
            "Keisuke Tateno",
            "Francis Engelmann",
            "Leonidas Guibas"
        ],
        "title": "HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild",
        "abstract": "arXiv:2512.02450v1 Announce Type: new  Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.",
        "arxiv_id": "2512.02450",
        "ARXIVID": "2512.02450",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for full building-scale 3D layout estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.03013": {
        "authors": [
            "Sagi Polaczek",
            "Or Patashnik",
            "Ali Mahdavi-Amiri",
            "Daniel Cohen-Or"
        ],
        "title": "In-Context Sync-LoRA for Portrait Video Editing",
        "abstract": "arXiv:2512.03013v1 Announce Type: new  Abstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
        "arxiv_id": "2512.03013",
        "ARXIVID": "2512.03013",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on portrait video editing using an image-to-video diffusion model with temporal coherence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02482": {
        "authors": [
            "Vishwesh Nath",
            "Javier G. Tejero",
            "Ruilong Li",
            "Filippo Filicori",
            "Mahdi Azizian",
            "Sean D. Huver"
        ],
        "title": "G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline",
        "abstract": "arXiv:2512.02482v1 Announce Type: new  Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.",
        "arxiv_id": "2512.02482",
        "ARXIVID": "2512.02482",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a real-time surgical scene reconstruction framework with novel methods for deformable tissue modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02566": {
        "authors": [
            "Kun Yuan",
            "Min Woo Sun",
            "Zhen Chen",
            "Alejandro Lozano",
            "Xiangteng He",
            "Shi Li",
            "Nassir Navab",
            "Xiaoxiao Sun",
            "Nicolas Padoy",
            "Serena Yeung-Levy"
        ],
        "title": "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature",
        "abstract": "arXiv:2512.02566v1 Announce Type: new  Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.",
        "arxiv_id": "2512.02566",
        "ARXIVID": "2512.02566",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a novel hierarchical vision-language pretraining strategy for biomedical literature.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02835": {
        "authors": [
            "Yifan Li",
            "Yingda Yin",
            "Lingting Zhu",
            "Weikai Chen",
            "Shengju Qian",
            "Xin Wang",
            "Yanwei Fu"
        ],
        "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
        "abstract": "arXiv:2512.02835v1 Announce Type: new  Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
        "arxiv_id": "2512.02835",
        "ARXIVID": "2512.02835",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on reasoning-centric video object segmentation with novel methodologies and interpretable reasoning trajectories.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02794": {
        "authors": [
            "Fan Wu",
            "Cheng Chen",
            "Zhoujie Fu",
            "Jiacheng Wei",
            "Yi Xu",
            "Deheng Ye",
            "Guosheng Lin"
        ],
        "title": "PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation",
        "abstract": "arXiv:2512.02794v1 Announce Type: new  Abstract: Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.",
        "arxiv_id": "2512.02794",
        "ARXIVID": "2512.02794",
        "COMMENT": "Matches criteria 4 as it focuses on improving text-to-image generation with physical customization, which is a novel application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02697": {
        "authors": [
            "Zixuan Song",
            "Jing Zhang",
            "Di Wang",
            "Zidie Zhou",
            "Wenbin Liu",
            "Haonan Guo",
            "En Wang",
            "Bo Du"
        ],
        "title": "GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization",
        "abstract": "arXiv:2512.02697v1 Announce Type: new  Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.",
        "arxiv_id": "2512.02697",
        "ARXIVID": "2512.02697",
        "COMMENT": "Matches criteria 2 and 5 as it explores a foundation model integrating vision and language for geo-localization, and introduces a novel dataset for cross-modal tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2512.02650": {
        "authors": [
            "Junwon Lee",
            "Juhan Nam",
            "Jiyoung Lee"
        ],
        "title": "Hear What Matters! Text-conditioned Selective Video-to-Audio Generation",
        "abstract": "arXiv:2512.02650v1 Announce Type: new  Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.",
        "arxiv_id": "2512.02650",
        "ARXIVID": "2512.02650",
        "COMMENT": "Matches criteria 6 as it introduces a novel video-to-audio generation task and methodology.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.02492": {
        "authors": [
            "Jiahui Chen",
            "Weida Wang",
            "Runhua Shi",
            "Huan Yang",
            "Chaofan Ding",
            "Zihao Chen"
        ],
        "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
        "abstract": "arXiv:2512.02492v1 Announce Type: new  Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
        "arxiv_id": "2512.02492",
        "ARXIVID": "2512.02492",
        "COMMENT": "Matches criteria 6 as it focuses on video generation with novel methodologies for music-driven video synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2512.02681": {
        "authors": [
            "Zhongbao Yang",
            "Jiangxin Dong",
            "Yazhou Yao",
            "Jinhui Tang",
            "Jinshan Pan"
        ],
        "title": "PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution",
        "abstract": "arXiv:2512.02681v1 Announce Type: new  Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.",
        "arxiv_id": "2512.02681",
        "ARXIVID": "2512.02681",
        "COMMENT": "Does not match any specific criteria but is relevant to efficient diffusion-based image super-resolution, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.02899": {
        "authors": [
            "Zhuobai Dong",
            "Rui Zhao",
            "Songjie Wu",
            "Junchao Yi",
            "Linjie Li",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Alex Jinpeng Wang"
        ],
        "title": "Glance: Accelerating Diffusion Models with 1 Sample",
        "abstract": "arXiv:2512.02899v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
        "arxiv_id": "2512.02899",
        "ARXIVID": "2512.02899",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and computational efficiency in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.02283": {
        "authors": [
            "Bin Xu",
            "Ayan Banerjee",
            "Sandeep K. S. Gupta"
        ],
        "title": "Model Recovery at the Edge under Resource Constraints for Physical AI",
        "abstract": "arXiv:2512.02283v1 Announce Type: new  Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.",
        "arxiv_id": "2512.02283",
        "ARXIVID": "2512.02283",
        "COMMENT": "Does not match any specific criteria but is tangentially related to embodied AI through resource-constrained model recovery for mission-critical systems.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.02258": {
        "authors": [
            "Shuang Chen",
            "Tomas Krajnik",
            "Farshad Arvin",
            "Amir Atapour-Abarghouei"
        ],
        "title": "Exploring the Potentials of Spiking Neural Networks for Image Deraining",
        "abstract": "arXiv:2512.02258v1 Announce Type: new  Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.",
        "arxiv_id": "2512.02258",
        "ARXIVID": "2512.02258",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.02472": {
        "authors": [
            "Wenhao Yu",
            "Zhenwen Liang",
            "Chengsong Huang",
            "Kishan Panaganti",
            "Tianqing Fang",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
        "abstract": "arXiv:2512.02472v1 Announce Type: new  Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
        "arxiv_id": "2512.02472",
        "ARXIVID": "2512.02472",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in large language models and self-evolution techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.03005": {
        "authors": [
            "Dawei Li",
            "Abdullah Alnaibari",
            "Arslan Bisharat",
            "Manny Sandoval",
            "Deborah Hall",
            "Yasin Silva",
            "Huan Liu"
        ],
        "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?",
        "abstract": "arXiv:2512.03005v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.",
        "arxiv_id": "2512.03005",
        "ARXIVID": "2512.03005",
        "COMMENT": "Does not match any specific criteria but is tangentially related to LLMs and their applications in online communication.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02716": {
        "authors": [
            "Tianyi Zhang",
            "Xiangyuan Xue",
            "Lingyan Ruan",
            "Shiya Fu",
            "Feng Xia",
            "Simon D'Alfonso",
            "Vassilis Kostakos",
            "Hong Jia"
        ],
        "title": "Menta: A Small Language Model for On-Device Mental Health Prediction",
        "abstract": "arXiv:2512.02716v1 Announce Type: new  Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/",
        "arxiv_id": "2512.02716",
        "ARXIVID": "2512.02716",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02499": {
        "authors": [
            "Yongkai Liu",
            "Helena Feng",
            "Bin Jiang",
            "Yixin Wang",
            "Max Wintermark",
            "David S. Liebeskind",
            "Michael Moseley",
            "Maarten Lansberg",
            "Gregory Albers",
            "Jeremy Heit",
            "Greg Zaharchuk"
        ],
        "title": "COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes",
        "abstract": "arXiv:2512.02499v1 Announce Type: new  Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.",
        "arxiv_id": "2512.02499",
        "ARXIVID": "2512.02499",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02280": {
        "authors": [
            "Noorbakhsh Amiri Golilarz",
            "Sindhuja Penchala",
            "Shahram Rahimi"
        ],
        "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence",
        "abstract": "arXiv:2512.02280v1 Announce Type: new  Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.",
        "arxiv_id": "2512.02280",
        "ARXIVID": "2512.02280",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in AI and cognitive autonomy.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02469": {
        "authors": [
            "Fengli Ran",
            "Xiao Pu",
            "Bo Liu",
            "Xiuli Bi",
            "Bin Xiao"
        ],
        "title": "TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution",
        "abstract": "arXiv:2512.02469v1 Announce Type: new  Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.",
        "arxiv_id": "2512.02469",
        "ARXIVID": "2512.02469",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in machine learning and dataset optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02392": {
        "authors": [
            "Yuqing Shao",
            "Yuchen Yang",
            "Rui Yu",
            "Weilong Li",
            "Xu Guo",
            "Huaicheng Yan",
            "Wei Wang",
            "Xiao Sun"
        ],
        "title": "From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking",
        "abstract": "arXiv:2512.02392v1 Announce Type: new  Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.",
        "arxiv_id": "2512.02392",
        "ARXIVID": "2512.02392",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.02369": {
        "authors": [
            "Qingmei Li",
            "Yang Zhang",
            "Peifeng Zhang",
            "Haohuan Fu",
            "Juepeng Zheng"
        ],
        "title": "SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains",
        "abstract": "arXiv:2512.02369v1 Announce Type: new  Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \\textbf{S}tyle-\\textbf{A}daptive \\textbf{GE}neralization framework (\\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.",
        "arxiv_id": "2512.02369",
        "ARXIVID": "2512.02369",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}
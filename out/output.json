{
    "2511.18823": {
        "authors": [
            "Fufangchen Zhao",
            "Liao Zhang",
            "Daiqi Shi",
            "Yuanjun Gao",
            "Chen Ye",
            "Yang Cai",
            "Jian Gao",
            "Danfeng Yan"
        ],
        "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models",
        "abstract": "arXiv:2511.18823v1 Announce Type: new  Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.",
        "arxiv_id": "2511.18823",
        "ARXIVID": "2511.18823",
        "COMMENT": "Matches criteria 2 and 6 closely as it introduces a novel video multimodal large language model (VMLLM) with fine-grained temporal perception and addresses video understanding tasks like fine-grained action understanding and rare event captioning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18450": {
        "authors": [
            "Rui Xu",
            "Dakuan Lu",
            "Zicheng Zhao",
            "Xiaoyu Tan",
            "Xintao Wang",
            "Siyu Yuan",
            "Jiangjie Chen",
            "Yinghui Xu"
        ],
        "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints",
        "abstract": "arXiv:2511.18450v1 Announce Type: new  Abstract: Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.",
        "arxiv_id": "2511.18450",
        "ARXIVID": "2511.18450",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a new benchmark (ORIGAMISPACE) for evaluating spatial reasoning in multimodal LLMs, specifically focusing on multi-step spatial reasoning and mathematical constraints.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.18817": {
        "authors": [
            "Siyuan Wei",
            "Chunjie Wang",
            "Xiao Liu",
            "Xiaosheng Yan",
            "Zhishan Zhou",
            "Rui Huang"
        ],
        "title": "Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring",
        "abstract": "arXiv:2511.18817v1 Announce Type: new  Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.",
        "arxiv_id": "2511.18817",
        "ARXIVID": "2511.18817",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a pipeline for generating 3D multimodal dialogue data and focuses on integrating 3D understanding with language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.19430": {
        "authors": [
            "Dingkang Liang",
            "Cheng Zhang",
            "Xiaopeng Xu",
            "Jianzhong Ju",
            "Zhenbo Luo",
            "Xiang Bai"
        ],
        "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
        "abstract": "arXiv:2511.19430v1 Announce Type: new  Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
        "arxiv_id": "2511.19430",
        "ARXIVID": "2511.19430",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark (ORS3D-60K) and a novel method (GRANT) for embodied AI task scheduling.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.18005": {
        "authors": [
            "Shengyuan Wang",
            "Zhiheng Zheng",
            "Yu Shang",
            "Lixuan He",
            "Yangcheng Yu",
            "Fan Hangyu",
            "Jie Feng",
            "Qingmin Liao",
            "Yong Li"
        ],
        "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
        "abstract": "arXiv:2511.18005v1 Announce Type: new  Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \\textbf{R}eality-\\textbf{A}ligned \\textbf{I}ntelligent \\textbf{S}ynthesis \\textbf{E}ngine that creates detailed, \\textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.",
        "arxiv_id": "2511.18005",
        "ARXIVID": "2511.18005",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework for city-scale 3D world generation, relevant for embodied agents and spatial intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.18116": {
        "authors": [
            "Yuheng Shao",
            "Lizhang Wang",
            "Changhao Li",
            "Peixian Chen",
            "Qinyuan Liu"
        ],
        "title": "PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures",
        "abstract": "arXiv:2511.18116v1 Announce Type: new  Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\\mathtt{PromptMoE}$.",
        "arxiv_id": "2511.18116",
        "ARXIVID": "2511.18116",
        "COMMENT": "Matches criteria 5 as it proposes a novel method for integrating image understanding tasks with large language models through a visually-guided mixture of experts approach.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.17843": {
        "authors": [
            "Chenyi Wang",
            "Zhaowei Li",
            "Ming F. Li",
            "Wujie Wen"
        ],
        "title": "JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception",
        "abstract": "arXiv:2511.17843v1 Announce Type: new  Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.",
        "arxiv_id": "2511.17843",
        "ARXIVID": "2511.17843",
        "COMMENT": "Matches criteria 3 as it introduces a novel framework for communication-efficient cooperative perception, addressing challenges in embodied/robotic AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.17952": {
        "authors": [
            "Liangyang Ouyang",
            "Yifei Huang",
            "Mingfang Zhang",
            "Caixin Kang",
            "Ryosuke Furuta",
            "Yoichi Sato"
        ],
        "title": "Multi-speaker Attention Alignment for Multimodal Social Interaction",
        "abstract": "arXiv:2511.17952v1 Announce Type: new  Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.",
        "arxiv_id": "2511.17952",
        "ARXIVID": "2511.17952",
        "COMMENT": "Matches criteria 2 and 5 as it explores improvements in Multimodal Large Language Models (MLLMs) and addresses integration of image/video understanding with LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.19172": {
        "authors": [
            "Kehua Chen",
            "Tianlu Mao",
            "Zhuxin Ma",
            "Hao Jiang",
            "Zehao Li",
            "Zihan Liu",
            "Shuqi Gao",
            "Honglong Zhao",
            "Feng Dai",
            "Yucheng Zhang",
            "Zhaoqi Wang"
        ],
        "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
        "abstract": "arXiv:2511.19172v1 Announce Type: new  Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
        "arxiv_id": "2511.19172",
        "ARXIVID": "2511.19172",
        "COMMENT": "Matches criteria 4 as it focuses on a novel framework for large-scale scene reconstruction, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18786": {
        "authors": [
            "Junyang Chen",
            "Jiangxin Dong",
            "Long Sun",
            "Yixin Yang",
            "Jinshan Pan"
        ],
        "title": "STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution",
        "abstract": "arXiv:2511.18786v1 Announce Type: new  Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.",
        "arxiv_id": "2511.18786",
        "ARXIVID": "2511.18786",
        "COMMENT": "Matches criteria 6 as it focuses on video super-resolution, a video understanding task, and introduces a novel framework (STCDiT) for high-quality video restoration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18742": {
        "authors": [
            "Zhenghan Fang",
            "Jian Zheng",
            "Qiaozi Gao",
            "Xiaofeng Gao",
            "Jeremias Sulam"
        ],
        "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion",
        "abstract": "arXiv:2511.18742v1 Announce Type: new  Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.",
        "arxiv_id": "2511.18742",
        "ARXIVID": "2511.18742",
        "COMMENT": "Matches criteria 5 as it introduces a reward-guided text-to-image generation framework with proximal diffusion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18378": {
        "authors": [
            "Shijian Wang",
            "Runhao Fu",
            "Siyi Zhao",
            "Qingqin Zhan",
            "Xingjian Wang",
            "Jiarui Jin",
            "Yuan Lu",
            "Hanqian Wu",
            "Cunjian Chen"
        ],
        "title": "Synthetic Curriculum Reinforces Compositional Text-to-Image Generation",
        "abstract": "arXiv:2511.18378v1 Announce Type: new  Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.",
        "arxiv_id": "2511.18378",
        "ARXIVID": "2511.18378",
        "COMMENT": "Matches criteria 5 as it focuses on compositional text-to-image generation with reinforcement learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18127": {
        "authors": [
            "Ruicong Liu",
            "Yifei Huang",
            "Liangyang Ouyang",
            "Caixin Kang",
            "Yoichi Sato"
        ],
        "title": "SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation",
        "abstract": "arXiv:2511.18127v1 Announce Type: new  Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.",
        "arxiv_id": "2511.18127",
        "ARXIVID": "2511.18127",
        "COMMENT": "Matches criteria 3 as it introduces a framework for language-guided 3D hand forecasting and embodied manipulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18780": {
        "authors": [
            "Ruize Ma",
            "Minghong Cai",
            "Yilei Jiang",
            "Jiaming Han",
            "Yi Feng",
            "Yingshui Tan",
            "Xiaoyong Zhu",
            "Bo Zhang",
            "Bo Zheng",
            "Xiangyu Yue"
        ],
        "title": "ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection",
        "abstract": "arXiv:2511.18780v1 Announce Type: new  Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.",
        "arxiv_id": "2511.18780",
        "ARXIVID": "2511.18780",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on multimodal video generation and safety in text-and-image-to-video generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.18075": {
        "authors": [
            "Jianhang Yao",
            "Yongbin Zheng",
            "Siqi Lu",
            "Wanying Xu",
            "Peng Sun"
        ],
        "title": "VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection",
        "abstract": "arXiv:2511.18075v1 Announce Type: new  Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\\textbf{VK-Det}$, a $\\textbf{V}$isual $\\textbf{K}$nowledge-guided open-vocabulary object $\\textbf{Det}$ection framework $\\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\\mathrm{mAP}^{N}$ on DIOR and 23.3 $\\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.",
        "arxiv_id": "2511.18075",
        "ARXIVID": "2511.18075",
        "COMMENT": "Matches criteria 4 as it focuses on leveraging visual knowledge for open-vocabulary object detection, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.18825": {
        "authors": [
            "Xiele Wu",
            "Zicheng Zhang",
            "Mingtao Chen",
            "Yixian Liu",
            "Yiming Liu",
            "Shushi Wang",
            "Zhichao Hu",
            "Yuhong Liu",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "Q-Save: Towards Scoring and Attribution for Generated Video Evaluation",
        "abstract": "arXiv:2511.18825v1 Announce Type: new  Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.",
        "arxiv_id": "2511.18825",
        "ARXIVID": "2511.18825",
        "COMMENT": "Matches criteria 6 as it introduces a benchmark and model for evaluating AI-generated video quality, focusing on video understanding and quality assessment.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.18715": {
        "authors": [
            "Shaoyin Ma",
            "Jie Song",
            "Huiqiong Wang",
            "Li Sun",
            "Mingli Song"
        ],
        "title": "HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions",
        "abstract": "arXiv:2511.18715v1 Announce Type: new  Abstract: Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.",
        "arxiv_id": "2511.18715",
        "ARXIVID": "2511.18715",
        "COMMENT": "Matches criteria 2 as it explores a framework (HuggingR$^4$) for selecting multimodal models, which aligns with the interest in multimodal LLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.17793": {
        "authors": [
            "Shweta Mahajan",
            "Hoang Le",
            "Hyojin Park",
            "Farzad Farhadzadeh",
            "Munawar Hayat",
            "Fatih Porikli"
        ],
        "title": "Attention Guided Alignment in Efficient Vision-Language Models",
        "abstract": "arXiv:2511.17793v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability \"look\" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.",
        "arxiv_id": "2511.17793",
        "ARXIVID": "2511.17793",
        "COMMENT": "This paper matches criterion 2 as it focuses on improving vision-language models (VLMs) with attention-guided alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.17962": {
        "authors": [
            "Ziheng Jia",
            "Linhan Cao",
            "Jinliang Han",
            "Zicheng Zhang",
            "Jiaying Qian",
            "Jiarui Wang",
            "Zijian Chen",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "title": "VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment",
        "abstract": "arXiv:2511.17962v1 Announce Type: new  Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.   However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.",
        "arxiv_id": "2511.17962",
        "ARXIVID": "2511.17962",
        "COMMENT": "This paper matches criterion 2 as it explores a vision-encoder-centered pre-training pipeline for large multi-modal models (LMMs).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.17805": {
        "authors": [
            "Chengan Che",
            "Chao Wang",
            "Xinyue Chen",
            "Sophia Tsoka",
            "Luis C. Garcia-Peraza-Herrera"
        ],
        "title": "A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking",
        "abstract": "arXiv:2511.17805v1 Announce Type: new  Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.",
        "arxiv_id": "2511.17805",
        "ARXIVID": "2511.17805",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding with a novel self-supervised learning framework for procedural video representation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.19380": {
        "authors": [
            "Maroun Ayli",
            "Youssef Bakouny",
            "Tushar Sharma",
            "Nader Jalloul",
            "Hani Seifeddine",
            "Rima Kilany"
        ],
        "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval",
        "abstract": "arXiv:2511.19380v1 Announce Type: new  Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.",
        "arxiv_id": "2511.19380",
        "ARXIVID": "2511.19380",
        "COMMENT": "Matches criteria 1 as it introduces a novel graph-based representation for spatial reasoning in UI screenshots, which aligns with spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.18822": {
        "authors": [
            "Zhennan Chen",
            "Junwei Zhu",
            "Xu Chen",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Hanzhen Zhao",
            "Chengjie Wang",
            "Jian Yang",
            "Ying Tai"
        ],
        "title": "DiP: Taming Diffusion Models in Pixel Space",
        "abstract": "arXiv:2511.18822v1 Announce Type: new  Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\\times$256.",
        "arxiv_id": "2511.18822",
        "ARXIVID": "2511.18822",
        "COMMENT": "Matches criteria 4 as it proposes a novel pixel-space diffusion framework for efficient image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.18262": {
        "authors": [
            "Tao Shen",
            "Xin Wan",
            "Taicai Chen",
            "Rui Zhang",
            "Junwen Pan",
            "Dawei Lu",
            "Fanding Lei",
            "Zhilin Lu",
            "Yunfei Yang",
            "Chen Cheng",
            "Qi She",
            "Chang Liu",
            "Zhenbang Sun"
        ],
        "title": "MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation",
        "abstract": "arXiv:2511.18262v1 Announce Type: new  Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.",
        "arxiv_id": "2511.18262",
        "ARXIVID": "2511.18262",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a unified multimodal framework combining semantic reasoning and high-fidelity visual synthesis.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.17943": {
        "authors": [
            "Zhiyu Xu",
            "Weilong Yan",
            "Yufei Shi",
            "Xin Meng",
            "Tao He",
            "Huiping Zhuang",
            "Ming Li",
            "Hehe Fan"
        ],
        "title": "SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System",
        "abstract": "arXiv:2511.17943v1 Announce Type: new  Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.",
        "arxiv_id": "2511.17943",
        "ARXIVID": "2511.17943",
        "COMMENT": "Matches criteria 6 as it focuses on scientific video understanding and education, leveraging multimodal large language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.18011": {
        "authors": [
            "Jun Zhang",
            "Jie Feng",
            "Long Chen",
            "Junhui Wang",
            "Zhicheng Liu",
            "Depeng Jin",
            "Yong Li"
        ],
        "title": "RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios",
        "abstract": "arXiv:2511.18011v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.",
        "arxiv_id": "2511.18011",
        "ARXIVID": "2511.18011",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces RoadBench, a benchmark for fine-grained spatial understanding and reasoning in urban road scenarios.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.19155": {
        "authors": [
            "Xihe Qiu",
            "Gengchen Ma",
            "Haoyu Wang",
            "Chen Zhan",
            "Xiaoyu Tan",
            "Shuo Li"
        ],
        "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction",
        "abstract": "arXiv:2511.19155v1 Announce Type: new  Abstract: Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.",
        "arxiv_id": "2511.19155",
        "ARXIVID": "2511.19155",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a vision-language model for EEG-based sleep stage classification with multi-level feature alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.18851": {
        "authors": [
            "Yilin Wen",
            "Kechuan Dong",
            "Yusuke Sugano"
        ],
        "title": "Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization",
        "abstract": "arXiv:2511.18851v1 Announce Type: new  Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.",
        "arxiv_id": "2511.18851",
        "ARXIVID": "2511.18851",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for long-term test-time adaptation in 3D human pose estimation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.17727": {
        "authors": [
            "Victor Li",
            "Naveenraj Kamalakannan",
            "Avinash Parnandi",
            "Heidi Schambra",
            "Carlos Fernandez-Granda"
        ],
        "title": "The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation",
        "abstract": "arXiv:2511.17727v1 Announce Type: new  Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.",
        "arxiv_id": "2511.17727",
        "ARXIVID": "2511.17727",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores the application of vision-language models in stroke rehabilitation, highlighting their limitations and potential.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.18346": {
        "authors": [
            "Wenshuo Gao",
            "Junyi Fan",
            "Jiangyue Zeng",
            "Shuai Yang"
        ],
        "title": "FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement",
        "abstract": "arXiv:2511.18346v1 Announce Type: new  Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.",
        "arxiv_id": "2511.18346",
        "ARXIVID": "2511.18346",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video relighting and background replacement with novel methodologies for temporal coherence and lighting realism.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.19065": {
        "authors": [
            "Jin-Young Kim",
            "Hyojun Go",
            "Lea Bogensperger",
            "Julius Erbach",
            "Nikolai Kalischek",
            "Federico Tombari",
            "Konrad Schindler",
            "Dominik Narnhofer"
        ],
        "title": "Understanding, Accelerating, and Improving MeanFlow Training",
        "abstract": "arXiv:2511.19065v1 Announce Type: new  Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.",
        "arxiv_id": "2511.19065",
        "ARXIVID": "2511.19065",
        "COMMENT": "Does not match any specific criteria but is relevant to your friend's general interest in generative modeling and clever statistical tricks, as it improves training dynamics for MeanFlow generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18699": {
        "authors": [
            "Jiarui Xue",
            "Dongjian Yang",
            "Ye Sun",
            "Gang Liu"
        ],
        "title": "Dendritic Convolution for Noise Image Recognition",
        "abstract": "arXiv:2511.18699v1 Announce Type: new  Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.",
        "arxiv_id": "2511.18699",
        "ARXIVID": "2511.18699",
        "COMMENT": "Does not match any specific criteria but introduces a biologically inspired convolution for noise image recognition, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19126": {
        "authors": [
            "Beilin Chu",
            "Weike You",
            "Mengtao Li",
            "Tingting Zheng",
            "Kehan Zhao",
            "Xuan Xu",
            "Zhigao Lu",
            "Jia Song",
            "Moxuan Xu",
            "Linna Zhou"
        ],
        "title": "When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP",
        "abstract": "arXiv:2511.19126v1 Announce Type: new  Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.",
        "arxiv_id": "2511.19126",
        "ARXIVID": "2511.19126",
        "COMMENT": "Does not match any specific criteria but is tangentially related to AI-generated image detection, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19274": {
        "authors": [
            "Mingyang Chen",
            "Jiawei Du",
            "Bo Huang",
            "Yi Wang",
            "Xiaobo Zhang",
            "Wei Wang"
        ],
        "title": "Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection",
        "abstract": "arXiv:2511.19274v1 Announce Type: new  Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.",
        "arxiv_id": "2511.19274",
        "ARXIVID": "2511.19274",
        "COMMENT": "Does not match any specific criteria but is tangentially related to data selection and diffusion models, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19169": {
        "authors": [
            "Bingchen Li",
            "Xin Li",
            "Jiaqi Xu",
            "Jiaming Guo",
            "Wenbo Li",
            "Renjing Pei",
            "Zhibo Chen"
        ],
        "title": "Test-Time Preference Optimization for Image Restoration",
        "abstract": "arXiv:2511.19169v1 Announce Type: new  Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.",
        "arxiv_id": "2511.19169",
        "ARXIVID": "2511.19169",
        "COMMENT": "Does not match any specific criteria but is tangentially related to image restoration, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.17890": {
        "authors": [
            "Wenyuan Li",
            "Guang Li",
            "Keisuke Maeda",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Decoupled Audio-Visual Dataset Distillation",
        "abstract": "arXiv:2511.17890v1 Announce Type: new  Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.",
        "arxiv_id": "2511.17890",
        "ARXIVID": "2511.17890",
        "COMMENT": "Does not match any specific criteria but is tangentially related to multimodal learning through audio-visual dataset distillation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.18640": {
        "authors": [
            "Akhil Kondepudi",
            "Akshay Rao",
            "Chenhui Zhao",
            "Yiwei Lyu",
            "Samir Harake",
            "Soumyanil Banerjee",
            "Rushikesh Joshi",
            "Anna-Katharina Meissner",
            "Renly Hou",
            "Cheng Jiang",
            "Asadur Chowdury",
            "Ashok Srinivasan",
            "Brian Athey",
            "Vikas Gulani",
            "Aditya Pandey",
            "Honglak Lee",
            "Todd Hollon"
        ],
        "title": "Health system learning achieves generalist neuroimaging models",
        "abstract": "arXiv:2511.18640v1 Announce Type: new  Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.",
        "arxiv_id": "2511.18640",
        "ARXIVID": "2511.18640",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on generalist neuroimaging models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.17979": {
        "authors": [
            "Bo Yin",
            "Xiaobin Hu",
            "Xingyu Zhou",
            "Peng-Tao Jiang",
            "Yue Liao",
            "Junwei Zhu",
            "Jiangning Zhang",
            "Ying Tai",
            "Chengjie Wang",
            "Shuicheng Yan"
        ],
        "title": "FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning",
        "abstract": "arXiv:2511.17979v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.",
        "arxiv_id": "2511.17979",
        "ARXIVID": "2511.17979",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on fine-tuning diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.19145": {
        "authors": [
            "Dongha Lee",
            "Jinhee Park",
            "Minjun Kim",
            "Junseok Kwon"
        ],
        "title": "ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation",
        "abstract": "arXiv:2511.19145v1 Announce Type: new  Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.",
        "arxiv_id": "2511.19145",
        "ARXIVID": "2511.19145",
        "COMMENT": "Does not match any specific criteria but proposes a method for faster convergence in low-rank adaptation, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.17914": {
        "authors": [
            "Chenyang Jiang",
            "Hang Zhao",
            "Xinyu Zhang",
            "Zhengcen Li",
            "Qiben Shan",
            "Shaocong Wu",
            "Jingyong Su"
        ],
        "title": "Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation",
        "abstract": "arXiv:2511.17914v1 Announce Type: new  Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.",
        "arxiv_id": "2511.17914",
        "ARXIVID": "2511.17914",
        "COMMENT": "Does not match any specific criteria but focuses on dataset distillation for long-tailed distributions, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18317": {
        "authors": [
            "Dongcai Tan",
            "Shunkun Liang",
            "Bin Li",
            "Banglei Guan",
            "Ang Su",
            "Yuan Lin",
            "Dapeng Zhang",
            "Minggang Wan",
            "Zibin Liu",
            "Chenglong Wang",
            "Jiajian Zhu",
            "Zhang Li",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "title": "Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement",
        "abstract": "arXiv:2511.18317v1 Announce Type: new  Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.   Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation",
        "arxiv_id": "2511.18317",
        "ARXIVID": "2511.18317",
        "COMMENT": "Does not match any specific criteria but is related to 3D deformation measurement and stereo calibration, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18656": {
        "authors": [
            "Harrison Bagley",
            "Will Meakin",
            "Simon Lucey",
            "Yee Wei Law",
            "Tat-Jun Chin"
        ],
        "title": "Robust Physical Adversarial Patches Using Dynamically Optimized Clusters",
        "abstract": "arXiv:2511.18656v1 Announce Type: new  Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.",
        "arxiv_id": "2511.18656",
        "ARXIVID": "2511.18656",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on physical adversarial patches and scale-resilient structures.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19394": {
        "authors": [
            "Rachit Saluja",
            "Asli Cihangir",
            "Ruining Deng",
            "Johannes C. Paetzold",
            "Fengbei Liu",
            "Mert R. Sabuncu"
        ],
        "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation",
        "abstract": "arXiv:2511.19394v1 Announce Type: new  Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.   In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.",
        "arxiv_id": "2511.19394",
        "ARXIVID": "2511.19394",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its novel approach to biomedical lesion segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19021": {
        "authors": [
            "Qiyang Yu",
            "Yu Fang",
            "Tianrui Li",
            "Xuemei Cao",
            "Yan Chen",
            "Jianghao Li",
            "Fan Min"
        ],
        "title": "Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting",
        "abstract": "arXiv:2511.19021v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, {\\alpha} and \\b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.",
        "arxiv_id": "2511.19021",
        "ARXIVID": "2511.19021",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on improving Vision Transformers with dynamic granularity.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18163": {
        "authors": [
            "Pasquale De Marinis",
            "Uzay Kaymak",
            "Rogier Brussee",
            "Gennaro Vessio",
            "Giovanna Castellano"
        ],
        "title": "Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design",
        "abstract": "arXiv:2511.18163v1 Announce Type: new  Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.",
        "arxiv_id": "2511.18163",
        "ARXIVID": "2511.18163",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on interpretability in few-shot semantic segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18037": {
        "authors": [
            "Yunfan Lu",
            "Nico Messikommer",
            "Xiaogang Xu",
            "Liming Chen",
            "Yuhan Chen",
            "Nikola Zubic",
            "Davide Scaramuzza",
            "Hui Xiong"
        ],
        "title": "Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation",
        "abstract": "arXiv:2511.18037v1 Announce Type: new  Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.",
        "arxiv_id": "2511.18037",
        "ARXIVID": "2511.18037",
        "COMMENT": "Does not match any specific criterion but is relevant to hybrid sensor modeling and simulation in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19024": {
        "authors": [
            "Long Tang",
            "Guoquan Zhen",
            "Jie Hao",
            "Jianbo Zhang",
            "Huiyu Duan",
            "Liang Yuan",
            "Guangtao Zhai"
        ],
        "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling",
        "abstract": "arXiv:2511.19024v1 Announce Type: new  Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.",
        "arxiv_id": "2511.19024",
        "ARXIVID": "2511.19024",
        "COMMENT": "Does not match any specific criterion but is relevant to image quality assessment and computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18695": {
        "authors": [
            "Changcai Li",
            "Wenwei Lin",
            "Zuoxun Hou",
            "Gang Chen",
            "Wei Zhang",
            "Huihui Zhou",
            "Weishi Zheng"
        ],
        "title": "Exploring Surround-View Fisheye Camera 3D Object Detection",
        "abstract": "arXiv:2511.18695v1 Announce Type: new  Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.",
        "arxiv_id": "2511.18695",
        "ARXIVID": "2511.18695",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D object detection and computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.19062": {
        "authors": [
            "Qiyang Yu",
            "Yu Fang",
            "Tianrui Li",
            "Xuemei Cao",
            "Yan Chen",
            "Jianghao Li",
            "Fan Min",
            "Yi Zhang"
        ],
        "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation",
        "abstract": "arXiv:2511.19062v1 Announce Type: new  Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.",
        "arxiv_id": "2511.19062",
        "ARXIVID": "2511.19062",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.17930": {
        "authors": [
            "Yuan Qu",
            "Zhipeng Zhang",
            "Chaojun Xu",
            "Qiao Wan",
            "Mengying Xie",
            "Yuzeng Chen",
            "Zhenqi Liu",
            "Yanfei Zhong"
        ],
        "title": "UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection",
        "abstract": "arXiv:2511.17930v1 Announce Type: new  Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.",
        "arxiv_id": "2511.17930",
        "ARXIVID": "2511.17930",
        "COMMENT": "Does not closely match any specific criteria but is related to remote sensing and change detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18055": {
        "authors": [
            "Bowen Qu",
            "Shangkun Sun",
            "Xiaoyu Liang",
            "Wei Gao"
        ],
        "title": "IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment",
        "abstract": "arXiv:2511.18055v1 Announce Type: new  Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.",
        "arxiv_id": "2511.18055",
        "ARXIVID": "2511.18055",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to vision and human perception alignment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.18272": {
        "authors": [
            "Richard J. Young"
        ],
        "title": "Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation",
        "abstract": "arXiv:2511.18272v1 Announce Type: new  Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.",
        "arxiv_id": "2511.18272",
        "ARXIVID": "2511.18272",
        "COMMENT": "Does not match any specific criteria but discusses privacy-preserving mechanisms in vision-language models, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.17674": {
        "authors": [
            "Kien Nguyen",
            "Feng Liu",
            "Clinton Fookes",
            "Sridha Sridharan",
            "Xiaoming Liu",
            "Arun Ross"
        ],
        "title": "Person Recognition in Aerial Surveillance: A Decade Survey",
        "abstract": "arXiv:2511.17674v1 Announce Type: new  Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.",
        "arxiv_id": "2511.17674",
        "ARXIVID": "2511.17674",
        "COMMENT": "This paper is a survey on person recognition in aerial surveillance, which does not match any specific criteria but may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}